[SearchHeadLevel - Accelerated DataModels with All Time Searching Enabled]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4 
counttype = number of events
cron_schedule = 0 7 * * *
description = Chance the alert requires action? High. Having an accelerated data model running searches every 5 minutes over all time can cause serious issues. Search Head specific? Yes
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/apps/local | search disabled=0 `comment("Look for accelerated data models that have all time set, this can cripple an indexer cluster if the index is large enough. \
Since the datamodel REST API only returns globally shared data models *or* data models seen in the current application scope we use a map to run the search many times")`\
| fields title \
| map maxsearches=200 search="| rest /servicesNS/admin/$title$/data/models | search acceleration!=0 AND acceleration.earliest_time=0 | fields acceleration, disabled, eai:acl.app, eai:acl.owner, eai:acl.sharing, title, eai:userName, acceleration.earliest_time" \
| search `comment("Since the search was run many times we now have duplicates for any globally shared datamodel, remove the duplicates")` `splunkadmins_acc_datamodels`\
| dedup title, eai:acl.app
disabled = 1

[AllSplunkEnterpriseLevel - Email Sending Failures]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 3 * * * *
description = Chance the alert requires action? High. Ideally this action shouldn't be using email but this should fire when the email server is throwing errors
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find any failures to send emails due to either the size of the email or the email server not working or similar")`\
index=_internal `splunkenterprisehosts` "stderr from " python sendemail.py sourcetype=splunkd (`splunkadmins_splunkd_source`)\
| dedup message \
| rex "ssname=(?P<savedsearch>[^\"]+)"\
| rex "stderr from '[^']+':\s+(?P<error>.*)"\
| rex field=results_file ".*/dispatch/[^_]+__(?P<user>[^_]+)"\
| eval time=strftime(_time, "%+")\
| stats count, values(time) AS time by error, savedsearch, user\
| table time, count, error, savedsearch, user
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Servers throwing runScript errors]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 10 * * *
description = Chance the alert requires action? Low. Splunk Enterprise servers are throwing an error related to running a script, this may or may not be an issue...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("runScript errors are an indicator of a potential issue with an application")`\
index=_internal `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \
"ERROR ScriptRunner - stderr from '*python *runScript.py execute'" OR "ERROR ExecProcessor - message from \"python *ERROR*"\
`comment("Do not include INFO level messages from standard error/out")`\
NOT " INFO " `splunkadmins_runscript` \
| cluster showcount=true \
| fields host, _raw, cluster_count
disabled = 1

[AllSplunkEnterpriseLevel - Splunkd Crash Logs Have Appeared in Production]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 47 3 * * 1
description = Chance the alert requires action? High. Production crashes are usually a problem
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","title","severity"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("crash logs on the Splunk enterprise servers are usually an issue, this may require a support ticket")`\
index=_internal `splunkenterprisehosts` sourcetype=splunkd_crash_log\
| top source, host, sourcetype
disabled = 1

[AllSplunkEnterpriseLevel - ulimit on Splunk enterprise servers is below 8192]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 17 3 * * *
description = Chance the alert requires action? High. ulimit should be 8192 or above as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Any Splunk enterprise servers running less than 8192 file descriptors can result in a crash, therefore we watch the ulimit numbers on startup")` \
`comment("You could do | rest /services/server/sysinfo | table ulimit* or similar but that will not cover all Splunk enterprise servers that are in the _internal index...")`\
index=_internal "ulimit" "open files:" `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \
| rex "(?P<nooffiles>\d+) files" \
| where nooffiles<8192\
| fields _time, _raw, host
disabled = 1

[AllSplunkLevel - Application Installation Failures From Deployment Manager]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 0 9 * * *
description = Chance the alert requires action? Moderate. Applications have failed to install from the deployment server and this may require investigation
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Deployment clients can pull applications down but they may not install the application so we watch for this error to see if an application failed to install")` \
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) action=Install\
| dedup app, ip | search result!="ok" \
| top limit=100 ip app result \
| lookup dnslookup clientip as ip \
| table clienthost app ip
disabled = 1

[AllSplunkLevel - Time skew on Splunk Servers]
action.keyindicator.invert = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
alert.suppress = 1
alert.suppress.period = 3d
counttype = number of events
cron_schedule = 53 2,6,10,14,18,22 * * *
description = Chance the alert requires action? Moderate. A time skew should not exist, if we see this alert then something is not working in NTP...
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A time skew issue likely shows an issue on the endpoint forwarder rather than a Splunk server but it is useful to watch for")`\
index=_internal "A time skew of approximately" (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) sourcetype=splunkd `splunkadmins_timeskew`\
| rex "Peer:https?://(?P<hostname>[^:]+).*A time skew of approximately (?P<seconds>-?\d+)"\
| eval negativeSeconds=if(substr(seconds,0,1)="-","true", "false"), seconds=abs(seconds)\
| stats values(negativeSeconds) AS negativeSeconds, first(_raw) AS _raw, values(host) AS reportingHost, max(_time) AS lastSeen, min(_time) AS firstSeen, avg(seconds) AS avgSkew, max(seconds) AS maxSkew by hostname \
| eval avgSkew=if(negativeSeconds="true","-" . avgSkew,avgSkew), maxSkew=if(negativeSeconds="true","-" . maxSkew,maxSkew) \
| eval lastSeen = strftime(lastSeen, "%+"), firstSeen = strftime(firstSeen, "%+") \
| table hostname, reportingHost, _raw, lastSeen, firstSeen, avgSkew, maxSkew
disabled = 1

[DeploymentServer - Application Not Found On Deployment Server]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 14 * * * *
description = Chance the alert requires action? High. The application was not found on the deployment server
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This usually indicates a misconfigured serverclass.conf or a missing application from the deployment-apps directory")` \
index=_internal `deploymentserverhosts` "ERROR Serverclass - Failed to load app." sourcetype=splunkd (`splunkadmins_splunkd_source`) \
| bin _time span=20m \
| top Application, path, _time
disabled = 1

[DeploymentServer - Forwarder has changed properties on phone home]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 37 6 * * 3
description = Chance the alert requires action? Moderate. Only detect when a forwarder has switched IP's or something strange has happened, ignore multiple DNS names for the same IP
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This looks for unusual changes on the phone home to the deployment server, this alert can also be completely harmless")` \
index=_internal `deploymentserverhosts` "has changed some of its properties on the latest phone home.Old properties are" (`splunkadmins_splunkd_source`) sourcetype=splunkd `splunkadmins_changedprops`\
| rex "Client with Id '(?P<clientid>[^']+)" \
| sort clientid \
| eventstats count by clientid \
| where count>`splunkadmins_changedprops_count` \
| stats values(ip) AS "IP List", values(dns) AS "DNS names", values(hostname) AS "Hostname List", values(uts) AS uts by name \
| eval numberOfIPs=mvcount("IP List"), numberOfHostnames=mvcount("Hostname List") \
| search `comment("Having multiple DNS names for an IP address is almost normal here, however multiple IPs or hostnames might be a real issue. Ignoring multiple DNS names only")` numberOfIPs>1 OR numberOfHostnames>1
disabled = 1

[DeploymentServer - btool validation failures occurring on deployment server]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 28 11 * * *
description = Chance the alert requires action? Moderate. Email about any btool validation errors on the deployment server
dispatch.earliest_time = -1d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert detects when the deployment server is throwing some kind of warning about an application it is going to deploy. The exclusion list includes lines that are not really relevant as they appear later in the log entries")` \
index=_internal `deploymentserverhosts` "WARN  Application" sourcetype=splunkd (`splunkadmins_splunkd_source`)\
 NOT "There were the following errors in btool check:" \
`splunkadmins_btoolvalidation_ds` \
| dedup message | fields _time _raw host
disabled = 1

[ForwarderLevel - Bandwidth Throttling Occurring]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 37 02 * * *
description = Chance the alert requires action? High. Cases where the Splunk forwarder is delayed from sending the data to Splunk due to the maxKbps limit
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert detects universal or heavy forwarders that have hit the maxKbps setting in the limits.conf and might need to be investigated")` \
index=_internal "has reached maxKBps. As a result, data forwarding may be throttled" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_bandwidth`\
| bin _time span=1h\
| stats count as countPerHost by host, _time\
| where countPerHost > 1
disabled = 1

[ForwarderLevel - File Too Small to checkCRC occurring multiple times]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? Low. CRC checksum errors multiple times in may indicate a problem with the crc checksum on the particular file, it's also possible we are seeing a zero sized file or a rolled file...
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins 
request.ui_dispatch_view = search
search = `comment("An experimental alert to detect the seekcrc too small errors in the splunkd.log file occurring a bit too regularly")` \
index=_internal "File too small to check seekcrc, probably truncated" \
sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_toosmall_checkcrc`\
`comment("Older universal forwarders have a variety of logs that will never be more than zero sized, therefore this error is legitimate for them")`\
NOT (file="'/*/splunkforwarder/var/log/splunk/license_usage.log'" OR file="'/*/splunkforwarder/var/log/splunk/license_usage_summary.log'" OR file="'/*/splunkforwarder/var/log/splunk/mongod.log'" OR file="'/*/splunkforwarder/var/log/splunk/remote_searches.log'" OR file="'/*/splunkforwarder/var/log/splunk/scheduler.log'" OR file="'/*/splunkforwarder/var/log/splunk/searchhistory.log'" OR file="'/*/splunkforwarder/var/log/splunk/splunkd_ui_access.log'" OR file="'/*/splunkforwarder/var/log/splunk/crash-*'" OR file="'/*/splunkforwarder/var/log/splunk/btool.log'" OR file="'/*/splunkforwarder/var/log/splunk/license_audit.log'")\
`comment("Older windows based universal forwarders can also have these same zero sized log files, therefore this error is legitimate for them")`\
NOT (file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_usage.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_usage_summary.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\mongod.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\remote_searches.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\scheduler.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\searchhistory.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\splunkd_ui_access.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\crash-*'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\btool.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_audit.log'")\
`comment("Splunk enterprise instances running on non-official hostnames")`\
NOT (file="'/opt/splunk/var/log/splunk/license_usage.log'" OR file="'/opt/splunk/var/log/splunk/license_usage_summary.log'" OR file="'/opt/splunk/var/log/splunk/mongod.log'" OR file="'/opt/splunk/var/log/splunk/remote_searches.log'" OR file="'/opt/splunk/var/log/splunk/scheduler.log'" OR file="'/opt/splunk/var/log/splunk/searchhistory.log'" OR file="'/opt/splunk/var/log/splunk/splunkd_ui_access.log'" OR file="'/opt/splunk/var/log/splunk/crash-*'" OR file="'/opt/splunk/var/log/splunk/btool.log'" OR file="'/opt/splunk/var/log/splunk/license_audit.log'")\
`comment("Regex for filename now replaces the default field extraction due to Windows based filenames containing spaces..")`\
| rex "file=(?P<file>.+)\)\."\
| stats sum(linecount) as numberOfEntries by host, file\
| where numberOfEntries > 10
disabled = 1

[ForwarderLevel - Forwarders in restart loop]
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 60m
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? Moderate. Attempt to detect universal forwarders that are restarting too often
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If a forwarder restarts more than 5 times in 15 minutes there might be a problematic script that is restarting it too often")` \
index=_internal "Received shutdown signal." sourcetype=splunkd (`splunkadmins_splunkuf_source`)\
| stats count as restartCount by host \
| where restartCount > 5
disabled = 1

[ForwarderLevel - SSL Errors In Logs (Potential Universal Forwarder and License Issue)]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 53 22 * * *
description = Chance the alert requires action? Moderate. SSL errors from Windows forwarder sin the past have resulted in duplication and excessive license usage, this alert exists to detect this scenario. 
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Excessive SSL errors may relate to a bug in the universal forwarder, if the SSL errors relate to duplication this could cause a license usage issue")` \
index=_internal sourcetype=splunkd (`splunkadmins_splunkuf_source`) NOT (`splunkenterprisehosts`)\
"sock_error = 10054. SSL Error = error:00000000:lib(0):func(0):reason(0)"\
| top limit=500 host
disabled = 1

[ForwarderLevel - Splunk Forwarder Down]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 * * * *
description = Chance the alert requires action? Low. Splunk Forwarders Down (excluding timeshift servers and AWS cloud forwarders)
dispatch.earliest_time = -4h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
display.statistics.drilldown = row
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | metadata type=hosts index=_internal \
| search `comment("Find forwarders that have recently stopped talking to the indexers / appear to be down")`\
NOT (`splunkenterprisehosts`) `splunkadmins_forwarderdown`\
| eval age=now()-recentTime | eval status=if(age<1200,"UP","DOWN") \
| eval "Last Active On"=strftime(recentTime, "%+") \
| rename age as Age \
| eval Hour=round(Age/3600,0)\
| eval Minute=round((Age%3600)/60,0)\
| eval Age="-".Hour."h"." : ".Minute."m" \
| table host, status, "Last Active On", Age \
| search status=DOWN \
| lookup dnslookup clienthost AS host
disabled = 1

[ForwarderLevel - Splunk HTTP Listener Overwhelmed]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 53 3 * * *
description = Chance the alert requires action? High. HTTP listeners should not be overwhelmed with incoming connections
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find if the HTTP listener for HEC is overloaded / cannot cope with the incoming load of data")` \
index=_internal "HttpListener - Can't handle request for" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts`
disabled = 1

[ForwarderLevel - Splunk Heavy logging sources]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 4,34 * * * *
description = Chance the alert requires action? Low. Sources that are sending a large amount of log data...
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find splunk sources sending excessive amounts of logs in and then conditionally email the right team members")` \
index=_internal source=*license_usage.log type=Usage `licensemasterhost` sourcetype=splunkd `splunkadmins_heavylogging`\
| stats sum(b) as totalBytes by s, h, idx, st \
| eval totalMBInPast30Mins=round(totalBytes/1024/1024) \
| where totalMBInPast30Mins>500\
| table s, h, idx, st, totalMBInPast30Mins
disabled = 1

[ForwarderLevel - Splunk Universal Forwarders Exceeding the File Descriptor Cache]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 11 * * *
description = Chance the alert requires action? Low. These forwarders may need an increase in their file descriptor cache limits
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","title","severity"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The file descriptor cache been full is a potential indicator that we are monitoring directories with many files and this might cause the forwarder to use more CPU")`\
index=_internal "TailReader - File descriptor cache is full" (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) sourcetype=splunkd `splunkadmins_exceeding_filedescriptor`\
| stats values(message), count by host
disabled = 1

[ForwarderLevel - Splunk forwarders are having issues with sending data to indexers]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 28 * * * *
description = Chance the alert requires action? Low. A level of these alerts just mean the indexer is busy / not receiving data fast enough, many alerts indicate the indexer is having serious issues.
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A could not send data to output queue from the indexers or heavy forwarders often indicates a performance issue")` \
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) "Could not send data to output queue" (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_sending_data`\
| search `comment("Exclude shutdown times")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,0,0)`]\
| bin _time span=20m\
| stats count by host, _time\
| search (count>`splunkadmins_sending_data_nonhf_count` NOT `heavyforwarderhosts`) OR (count>`splunkadmins_sending_data_hf_count` `heavyforwarderhosts`)
disabled = 1

[ForwarderLevel - Splunk forwarders failing due to disk space issues]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 45 * * * *
description = Chance the alert requires action? High. A universal forwarder has run out of disk space
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect universal forwarders that do not have any disk space left and therefore cannot work as expected")` \
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) "No space left on device" \
| top host
disabled = 1

[ForwarderLevel - Splunk universal forwarders with ulimit issues]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 10 * * 1
description = Chance the alert requires action? High. Universal forwarder with ulimit issues
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
schedule_window = 50
search = `comment("Detect universal forwarders that have the ulimit set too low for the number of file descriptors (ulimit -n)")` \
index=_internal log_level=WARN sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) component=ulimit "Splunk may not work due to low file size limit" \
| dedup host \
| fields host _raw
disabled = 1

[ForwarderLevel - Unusual number of duplication alerts]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 07 22 * * *
description = Chance the alert requires action? Low. An unusual number of duplication alerts has appeared from these universal forwarders
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The number of warnings about duplication seems unusually high and may require investigation")` \
`comment("The duplication warnings will occur with indexer acknowledgement enabled and indexer shutdowns. Other circumstances likely require some kind of investigation, the issue may also appear if the forwarder is having trouble getting CPU time...")`\
index=_internal sourcetype=splunkd (`splunkadmins_splunkuf_source`) NOT (`splunkenterprisehosts`) "duplication" `splunkadmins_unusual_duplication`\
| search `comment("Exclude shutdown times")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,60,60)`]\
| stats count by host \
| where count > `splunkadmins_unusual_duplication_count`
disabled = 1

[ForwarderLevel - crcSalt or initCrcLength change may be required]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 53 2 * * *
description = Chance the alert requires action? Low. The forwarder is advising the crcSalt = <SOURCE> or an initCrcLength change may be required on these files therefore these should be investigated.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Look for issues relating to CRC salt on any files...the universal forwarder settings may need tweaking to ensure the file is read as expected, or it may be a rolled file")` \
index=_internal "You may wish to use larger initCrcLen for this sourcetype" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_crcsalt_initcrc`\
`comment("Attempt to exclude rolled files from the check by looking for the most common pattern (.1, .2, .10 or similar)")` \
`comment("This alert aims to find files where crcSalt = <SOURCE> might be required in the inputs.conf file or a tweak to the initCrcLen...")`\
`comment("Regex for filename now replaces the default field extraction due to Windows based filenames containing spaces..")`\
| rex "file=(?P<file>.+)\)\."\
| regex file!="\.\d+$" \
| top limit=500 file, host, message
disabled = 1

[ForwarderLevel - Splunk Universal Forwarders that are time shifting]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 0 * * *
description = Chance the alert requires action? Moderate. The clock has changed many times on this server and may indicate a timeshfiting test environment
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect universal forwarders that appear to be moving their clocks backwards/into the past or forwards, into the future. Timeshifting servers may need to be excluded from Splunk")` \
`comment("The string \"WARN TimeoutHeap - Either time adjusted forwards by, or event loop was descheduled for\" looks similar but tends to relate to a poorly performing server rather than a time shift...")`\
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) "Detected system time adjusted" OR "System time went " `splunkadmins_uf_timeshifting`\
| rex "by (?P<timePeriod>\d+)ms\.$"\
| rex "by (?P<timePeriodInSecs>[\d\.]+) seconds$"\
| eval timePeriod=if(isnotnull(timePeriodInSecs),timePeriodInSecs*1000,timePeriod)\
| where timePeriod > 100000 \
| dedup host\
| fields host, _raw
disabled = 1


[IndexerLevel - ClusterMaster Advising SearchOrRep Factor Not Met]
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 3h
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */10 * * * *
description = Chance the alert requires action? High. The cluster master shows that either not all data is searchable or rep/search factors are not met
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/cluster/searchhead/generation splunk_server=local | where is_searchable!=1 OR replication_factor_met!=1 OR search_factor_met!=1 | table is_searchable replication_factor_met, search_factor_met\
| search `comment("If the cluster master advises there is an issue, you probably want to check why")`
disabled = 1

[IndexerLevel - Future Dated Events that appeared in the last week]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 0 * * 2
description = Chance the alert requires action? High. Search for any data that has future based time-stamping, this likely shows a date parsing issue or a server sending logs with a date in the future
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Data should not appear from the future...this alert finds that data so it can be investigated")` \
index=* earliest=+5m latest=+10y `splunkadmins_future_dated`\
| eval ahead=abs(now() - _time)\
| eval indextime=_indextime\
| bin span=1d indextime \
| eval timeToLookBack=now()-(60*60*24*7)\
| stats avg(ahead) as averageahead, max(_time) AS maxTime, min(_time) as minTime, count, first(timeToLookBack) AS timeToLookBack by host, sourcetype, index, indextime\
| where indextime>timeToLookBack AND averageahead > 1000\
| eval averageahead =tostring(averageahead, "duration")\
| eval invesMaxTime=if(minTime=maxTime,maxTime+1,maxTime)\
| eval investigationQuery="index=" . index . " host=" . host . " sourcetype=\"" . sourcetype . "\" earliest=" . minTime . " latest=" . invesMaxTime . " _index_earliest=" . timeToLookBack . " |  eval indextime=strftime(_indextime, \"%+\")"\
| eval indextime=strftime(indextime, "%+"), maxTime = strftime(maxTime, "%+"), minTime = strftime(minTime, "%+")\
| table host, sourcetype, index, averageahead, indextime, minTime, maxTime, count, investigationQuery
disabled = 1

[IndexerLevel - Failures To Parse Timestamp Correctly (excluding breaking issues)]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 3 * * 5
description = Chance the alert requires action? Moderate. Failures to parse incoming log file timestamps, this excludes a timestamp failure due to the event been broken (there is a separate alert for breaking issues)
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Timestamp parsing has failed, and it doesn't appear to be related to the event been broken due to having too many lines, that is a separate alert that may trigger a timestamp parsing issue (excluded from this alert as that issue needs to be resolved first)")` \
`comment("Please note that you may see this particular warning on data that is sent to the nullQueue using a transforms.conf. Obviously you won't see this in the index but you will see the warning because the time parsing occurs before the transforms.conf occurs")`\
`comment("This alert now checks for at least 2 failures, and header entries can often trigger 2 entries in the log files about timestamp parsing failures...")`\
`comment("Finally one strange edge case is a newline inserted into the log file (by itself with no content before/afterward) can trigger the warning but nothing will get indexed, multiline_event_extra_waittime, time_before_close and EVENT_BREAKER can resolve this edge case")`\
index=_internal sourcetype=splunkd ("Failed to parse timestamp" "Defaulting to timestamp of previous event") OR "Breaking event because limit of " OR "outside of the acceptable time window" (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_failuretoparse_timestamp`\
| bin _time span=`splunkadmins_failuretoparse_timestamp_binperiod` \
| eval host=data_host, source=data_source, sourcetype=data_sourcetype\
| rex "source::(?P<source>[^|]+)\|host::(?P<host>[^|]+)\|(?P<sourcetype>[^|]+)" \
| eventstats count(eval(isnotnull(data_host))) AS hasBrokenEventOrTuncatedLine, count(eval(searchmatch("outside of the acceptable time window"))) AS outsideTimewindow by _time, host, source, sourcetype\
| where hasBrokenEventOrTuncatedLine=0 AND isnull(data_host) AND NOT searchmatch("outside of the acceptable time window")\
| search `comment("To investigate further we want the previous timestamp that Splunk used for the event in question, that way we can see what it looks like in raw format...")`\
| rex "Defaulting to timestamp of previous event \((?P<previousTimeStamp>[^)]+)"\
| eval previousTimeStamp=strptime(previousTimeStamp, "%a %b %d %H:%M:%S %Y")\
| stats count, min(_time) AS firstSeen, max(_time) AS mostRecent, first(previousTimeStamp) AS recentExample, sum(outsideTimewindow) AS outsideTimewindow by host, sourcetype, source\
| where count>`splunkadmins_failuretoparse_timestamp_count`\
| stats sum(count) AS count, min(firstSeen) AS firstSeen, max(mostRecent) AS mostRecent, first(recentExample) AS recentExample, values(source) AS sourceList, sum(outsideTimewindow) AS outsideTimewindow by host, sourcetype\
| search `comment("Allow exclusions based on count or similar...")` `splunkadmins_failuretoparse_timestamp2`\
| eval invesEnd=recentExample+1\
| eval invesDataSource=sourceList\
| eval invesDataSource=if(mvcount(invesDataSource)>1,mvjoin(invesDataSource,"\" OR source=\""),invesDataSource)\
| eval invesDataSource = "source=\"" + invesDataSource + "\""\
| eval invesDataSource = replace(invesDataSource, "\\\\", "\\\\\\\\")\
| eval investigationQuery="`comment(\"The investigation query may find zero data if the data was sent to the null queue by a transforms.conf as the time parsing occurs before the transforms occur. If this source/sourcetype has a null queue you may need to exclude it from this alert\")` `comment(\"Note that the host= can be inaccurate if host overrides are in use in transforms.conf, if this query finds no results remove host=...\")` index=* host=" . host . " sourcetype=\"" . sourcetype . "\" " . invesDataSource . " earliest=" . recentExample . " latest=" . invesEnd . " | eval indextime=strftime(_indextime, \"%+\")" \
| eval mostRecent=strftime(mostRecent, "%+"), firstSeen=strftime(firstSeen, "%+")\
| eval outsideAcceptableTimeWindow=if(outsideTimewindow!=0,"Timestamp parsing failed due to been outside the acceptable time window","No")\
| fields - recentExample, invesEnd, invesDataSource, outsideTimewindow\
| sort - count
disabled = 1

[IndexerLevel - IndexConfig Warnings from Splunk indexers]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 2 * * * 
description = Chance the alert requires action? High. IndexConfig warnings are usually a problem so should be investigated...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("IndexConfig warnings are generally a problem")` \
index=_internal "WARN  IndexConfig" (`splunkadmins_splunkd_source`) `indexerhosts` `splunkadmins_indexconfig_warn` | top message
disabled = 1

[IndexerLevel - Indexer Queues May Have Issues]
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 1h
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = */11 * * * *
description = Chance the alert requires action? Low. One or more indexer queues have been filled for a period of time and may require investigation.
dispatch.earliest_time = -11m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert is borrowed from the monitoring console. When the queues are filled there is an issue in the indexer cluster!")`\
index=_internal `indexerhosts` source=*metrics.log sourcetype=splunkd group=queue \
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, "none") | search ingest_pipe=*\
| eval name=case(name=="aggqueue","2 - Aggregation Queue",\
 name=="indexqueue", "4 - Indexing Queue",\
 name=="parsingqueue", "1 - Parsing Queue",\
 name=="typingqueue", "3 - Typing Queue",\
 name=="splunktcpin", "0 - TCP In Queue",\
 name=="tcpin_cooked_pqueue", "0 - TCP In Queue") \
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size) \
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size) \
| eval fill_perc=round((curr/max)*100,2) \
| eval combined = host . "_pipe_" . ingest_pipe\
| bin _time span=1m\
| stats Median(fill_perc) AS "fill_percentage" by combined, _time, name \
| where (fill_percentage>`splunkadmins_indexerqueue_fillperc_nonindexqueue` AND name!="4 - Indexing Queue") OR (fill_percentage>`splunkadmins_indexerqueue_fillperc_indexqueue` AND name="4 - Indexing Queue")
disabled = 1

[IndexerLevel - Indexer replication queue issues to some peers]
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 90m
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = */11 * * * *
description = Chance the alert requires action? Low. Indexer replication queue issues to some peers may prevent indexing of data and result in a large index queue
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If the replication queue is full, then depending on the replication factor this can stop / slow indexing. Note this alert has been set to find many results to remove false alarms...")` \
`comment("Unfortunately this setting is not tunable, at the time of writing (7.0.0) the queue size is 20. If the \"has room now\" appears shortly afterward this is not an issue.")`\
index=_internal `indexerhosts` "replication queue for " "full" sourcetype=splunkd (`splunkadmins_splunkd_source`)\
| rename peer AS guid \
| join guid [| rest /services/search/distributed/peers | fields guid peerName]\
| bin _time span=10m \
| stats count by peerName, _time \
| where count>`splunkadmins_indexer_replication_queue_count`
disabled = 1

[IndexerLevel - Rolling Hot Bucket Failure]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? High. Hot buckets are throwing errors while trying to roll
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If this alert fires, we are potentially out of disk in the hot section or something else has gone wrong")` \
index=_internal `indexerhosts` "Not rolling hot buckets on further errors to this target" sourcetype=splunkd (`splunkadmins_splunkd_source`)
disabled = 1

[IndexerLevel - Splunk Indexers Losing Contact With Master]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 11 * * * *
description = Chance the alert requires action? Moderate. One or more splunk indexers have lost contact with the splunk cluster master server. This may require additional investigation.
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.statistics.drilldown = row
display.visualizations.charting.chart = line
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Either the master is down or the indexers are having issues contacting the master")` \
index=_internal `splunkenterprisehosts` "master is down" OR (`indexerhosts` "Failed to register with cluster master reason") sourcetype=splunkd (`splunkadmins_splunkd_source`) log_level="WARN" \
NOT [`splunkadmins_shutdown_time(splunkadmins_clustermaster_oshost,30,60)`]\
| fields _time _raw host
disabled = 1

[IndexerLevel - Uneven Indexed Data Across The Indexers]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 56 1,5,9,13,17,21 * * *
description = Chance the alert requires action? Moderate. The data is not been spread across the indexers correctly during this last 4 hour block
dispatch.earliest_time = -4h@h
dispatch.latest_time = @h
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
display.visualizations.charting.chart.stackMode = stacked100
enableSched = 1
quantity = 10
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats summariesonly=t count WHERE index="*" by splunk_server _time span=10m\
| search `comment("If the balance of data between indexer cluster members becomes very unbalanced then the searches tend to spend more CPU on a particular indexer / search peer and this eventually creates issues")`\
| sort _time \
| eventstats sum(count) AS totalCountForTime by _time \
| eval perc=round((count/totalCountForTime)*100,2) \
| where perc>`splunkadmins_uneven_indexed_perc`
disabled = 1

[IndexerLevel - Weekly Broken Events Report]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 5 * * 4
description = Chance the alert requires action? Moderate. These events are been broken due to reaching the maximum number of lines limit...in Splunk 7 and above the Monitoring Console, Indexing -> Inputs -> Data Quality will help here...
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The event that came in was greater than the maximum number of lines that were configured, therefore it was broken into multiple events...")` \
`comment("If running Splunk 7 or newer than refer to the monitoring console Indexing -> Inputs -> Data Quality")`\
index=_internal "AggregatorMiningProcessor - Breaking event because limit of" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_weekly_brokenevents`\
| rex "Breaking event because limit of (?P<curlimit>\d+)" \
| stats max(_time) AS mostRecent, min(_time) AS firstSeen, count by data_sourcetype, data_host, curlimit\
| eval longerThan=curlimit-1\
| eval invesLatest = if(mostRecent==firstSeen,mostRecent+1,mostRecent)\
| rename data_sourcetype AS sourcetype, data_host AS host\
| eval investigationQuery="`comment(\"If no results are found prepend the earliest=/latest= with _index_ (eg _index_earliest=...) and expand the timeframe searched over, as the parsed timestamps from the data does not have to exactly match the time the warnings appeared...\")` index=* host=" . host . " sourcetype=\"" . sourcetype . "\" linecount>" . longerThan . " earliest=" . firstSeen . " latest=" . invesLatest\
| fields - firstSeen, longerThan, invesLatest\
| eval mostRecent=strftime(mostRecent, "%+")\
| sort - count
disabled = 1

[IndexerLevel - Weekly Truncated Logs Report]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 5 * * 2
description = Chance the alert requires action? Moderate. These events are been truncated due to hitting the truncation limit, in Splunk 7 and above the Monitoring Console, Indexing -> Inputs -> Data Quality will help here...
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The line was truncated due to length, the TRUNCATE setting may need tweaking (or it may be just bad data coming in)")`\
`comment("If running Splunk 7 or newer than refer to the Monitoring Console, Indexing -> Inputs -> Data Quality")`\
`comment("If you are in a (very) performance sensitive environment you might want to remove the rex/eval lines for the data_host field and let the admin update the inves query manually")`\
index=_internal "Truncating line because limit of" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`heavyforwarderhosts`) OR (`indexerhosts`) `splunkadmins_weekly_truncated`\
| rex "Truncating line because limit of (?P<curlimit>\d+) bytes.*with a line length >= (?P<approxlinelength>\S+)" \
| rex field=data_host "(?P<data_host>[^\.]+)"\
| eval data_host=data_host . "*"\
| stats min(_time) AS firstSeen, max(_time) AS lastSeen, count, avg(approxlinelength) AS avgApproxLineLength, max(approxlinelength) AS maxApproxLineLength, values(data_host) AS hosts by data_sourcetype, curlimit\
| rename data_sourcetype AS sourcetype\
| eval hostList=if(mvcount(hosts)>1,mvjoin(hosts," OR host="),hosts)\
| eval hostList="host=" . hostList\
| eval avgApproxLineLength = round(avgApproxLineLength)\
| eval invesLastSeen=if(firstSeen==lastSeen,lastSeen+1,lastSeen)\
| eval firstSeen=firstSeen-10\
| eval invesLastSeen=invesLastSeen+10\
| eval investigationQuery="`comment(\"Find examples where the truncation limit has been reached\")` `comment(\"The earliest/latest time is based on the warning messages in the Splunk logs, they may need customisation!\")` index=* sourcetype=" . sourcetype . " " . hostList . " earliest=" . firstSeen . " latest=" . invesLastSeen . " | where len(_raw)=" . curlimit\
| sort - count\
| eval lastSeen=strftime(lastSeen, "%+")\
| table sourcetype, curlimit, count, avgApproxLineLength, maxApproxLineLength, lastSeen, investigationQuery\
| where count>`splunkadmins_weekly_truncated_count`
disabled = 1

[IndexerLevel - Valid Timestamp Invalid Parsed Time]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 2 * * 3
description = Chance the alert requires action? Moderate. The timestamp was parsed but an error was thrown to advise that the timestamp does not appear to be correct
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The timestamp parsing did run but the timestamp found did not match previous events so the time parsing may need a review")`\
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) \
"outside of the acceptable time window. If this timestamp is correct, consider adjusting" \
OR "is too far away from the previous event's time" \
OR "is suspiciously far away from the previous event's time" `splunkadmins_valid_timestamp_invalidparsed`\
| rex "source::(?P<source>[^|]+)\|host::(?P<host>[^|]+)\|(?P<sourcetype>[^|]+)"\
| search `comment("The goal of this part of the search was to obtain the messages that are relating to this particular host/source/sourcetype, however since the message includes a time we cannot uses values(message) without getting a huge number of values, therefore we use cluster to obtain the unique values. Since we want the original start/end times we use labelonly=true")`\
| cluster labelonly=true \
| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, first(message) AS message by host, source, sourcetype, cluster_label\
| search `comment("While 'A possible timestamp match (...) is outside of the acceptable time window' and 'Time parsed (...) is too far away from the previous event's time', result in the current indexing time been used, the 'Accepted time (...) is suspiciously far away from the previous event's time' is accepted and therefore we need to expand the investigation query time to include this time range as well!")` \
| rex field=message "Accepted time \((?P<acceptedTime>[^\)]+)"\
| eval acceptedTime=strptime(acceptedTime, "%a %b %d %H:%M:%S %Y")\
| eval firstSeen=if(acceptedTime<firstSeen,acceptedTime,firstSeen)\
| search `comment("Now that we have the first message for each labelled cluster, we now take all relevant message per host/source/sourcetype")`\
| stats values(acceptedTime) AS acceptedTime, sum(count) AS count, min(firstSeen) AS firstSeen, max(lastSeen) AS lastSeen, values(message) AS message by host, source, sourcetype\
| eval invesEnd=if(lastSeen=firstSeen,round(lastSeen+1),round(lastSeen)), invesStart=floor(firstSeen)\
| eval invesDataSource = replace(source, "\\\\", "\\\\\\\\")\
| eval investigationQuery="`comment(\"Please note that this query may need to be narrowed down further before running it, this is an example only...\")` index=* host=" . host . " sourcetype=\"" . sourcetype . "\" source=\"" . invesDataSource . "\" earliest=" . invesStart . " latest=" . invesEnd . " | eval indextime=strftime(_indextime, \"%+\")"\
| eval firstSeen=strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")\
| table host, source, sourcetype count, firstSeen, lastSeen, message, investigationQuery\
| sort - count
disabled = 1

[SearchHeadLevel - Long Running Searches Found]
action.analyzeioc.param.verbose = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 38 0,4,8,12,14,16,20 * * *
description = Chance the alert requires action? Low. Extra long running searches have been found
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find any search running longer than a period of time, this is useful for tracking down poorly built search queries or dashboards")` \
index=_audit sourcetype=audittrail  \
`splunkenterprisehosts` \
`comment("Exclude accelerated searches")` \
savedsearch_name!=_ACCELERATE*\
total_run_time>300\
`splunkadmins_longrunning_searches`\
`comment("At this point we have a list of searches minus the various exclusions, we now filter out the real time searches as they will always run for a long period of time...")`\
| regex search_id!="rt.*" | table savedsearch_name, search_id, total_run_time, search_et, search_lt, api_et, api_lt, scan_count, _time, user, info, host | eval search_et=strftime(search_et, "%d/%m/%Y %H:%M"), search_lt=strftime(search_lt, "%d/%m/%Y %H:%M"), api_et=strftime(api_et, "%d/%m/%Y %H:%M"), api_lt=strftime(api_lt, "%d/%m/%Y %H:%M"), _time=strftime(_time, "%d/%m/%Y %H:%M")
disabled = 1

[SearchHeadLevel - Realtime Scheduled Searches are in use]
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 8 0,4,8,12,16,20 * * *
description = Chance the alert requires action? High. Realtime searches should not be scheduled. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("Find realtime scheduled searches, they should not be enabled")` `splunkadmins_realtime_scheduledsearches`\
| table title author, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.index_earliest, dispatch.index_latest, dispatch.latest_time, dispatchAs, eai:acl.app, eai:acl.owner, eai:acl.owner, updated, qualifiedSearch, is_scheduled, next_scheduled_time, alert_type, schedule_priority\
| search dispatch.earliest_time=rt* next_scheduled_time!=""
disabled = 1

[SearchHeadLevel - Scheduled Searches That Cannot Run]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 16 6,10,18 * * *
description = Chance the alert requires action? High. As found in the DMC console, moving it into an alert so we can get alerted to the problem rather than checking a dashboard/log about this. Can be fixed by the end user? Yes
dispatch.earliest_time = -8h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("These searches are scheduled but for some reason cannot run (eg. invalid search syntax)")`\
index=_internal `searchheadhosts` sourcetype=scheduler `splunkadmins_scheduledsearches_cannot_run`\
`comment("Additional rex due to someone using app= inside their saved search name...")`\
| rex "app=\"(?P<app>[^\"]+)\""\
| stats max(_time) AS mostRecentlySeen, values(success) AS success by message, savedsearch_name, app, log_level, user, status \
| stats count(eval(status="success")) AS successCount, count(eval(success==0)) AS reportSuccessCount, count(eval(searchmatch("log_level=WARN OR log_level=ERROR OR status=delegated_remote_error"))) AS warnerrorcount, max(mostRecentlySeen) AS mostRecentlySeen, values(status) AS status by savedsearch_name, app, user\
| where warnerrorcount>0\
| eval successCount=successCount+reportSuccessCount\
| append \
    [ search index=_internal `searchheadhosts` sourcetype=scheduler status=delegated_remote_error \
    | stats max(_time) AS mostRecentlySeen, first(message) AS message by savedsearch_name, app, log_level, user, status] \
| selfjoin overwrite=true keepsingle=true savedsearch_name, app, user \
| append \
    [ search `comment("macro failures in the search syntax result in the log only appearing in splunkd, and the absence of delegated_remote_completion in scheduler.log")` \
        index=_internal `searchheadhosts` ERROR "failed job" sourcetype=splunkd `splunkadmins_splunkd_source` saved_search=* \
    | search `comment("Exclude time periods where shutdowns were occurring")` AND NOT \
        [ `splunkadmins_shutdown_time(searchheadhosts,0,0)`] \
    | rex "saved_search=([^;]+);(?P<app>[^;]+);(?P<savedsearch_name>.*?) err=" \
    | rex "(?P<messagewithoutheader>saved_search=.*uri=)http(s)?://[^/]+(?P<messagewithoutheader2>.*)" \
    | eval messagewithoutheader=messagewithoutheader . messagewithoutheader2 \
    | stats max(_time) AS mostRecentlySeen, first(message) AS message by messagewithoutheader, savedsearch_name, app \
    | eval log_level="ERROR" \
    | fields - messagewithoutheader \
    | sort - mostRecentlySeen] \
| selfjoin overwrite=true keepsingle=true savedsearch_name, app \
| where successCount<1 \
| sort - warnerrorcount, savedsearch_name \
| rename message as Message, count as runCount \
| eval mostRecentlySeen = strftime(mostRecentlySeen, "%+") \
| fields - cluster_label, status, savedsearch_id, host, status, reportSuccessCount
disabled = 1

[SearchHeadLevel - Scheduled Searches without a configured earliest and latest time]
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 8h
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 8 0,4,8,12,16,20 * * *
description = Chance the alert requires action? High. A scheduled search without time limits could kill the Splunk indexers with CPU / IO issues depending on the criteria of the search. Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("Find scheduled searches where they are searching over all time, this is generally not good practice and can cause performance issues")`\
dispatch.earliest_time="" OR dispatch.earliest_time="0" next_scheduled_time!="" `splunkadmins_scheduledsearches_without_earliestlatest`\
| table title author, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.latest_time, eai:acl.app, eai:acl.owner, updated, qualifiedSearch, is_scheduled, is_visible, next_scheduled_time, splunk_server \
| regex qualifiedSearch="^\s*(search|tstats) " \
| rex field=qualifiedSearch "earliest=(?P<earliestTime>\S+)"\
| where isnull(earliestTime) \
| fields - earliestTime\
| rename eai:acl.owner AS owner, eai:appName AS Application
disabled = 1

[SearchHeadLevel - Scheduled searches not specifying an index]
action.analyzeioc.param.verbose = 0
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 14 6 * * 1-5
description = Chance the alert requires action? High. These searches are either using index=* or not specifying an index at all and relying on the default set of indexes. Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches\
| search `comment("Look over all scheduled searches and find those not specifying/narrowing down to an index, or using the index=* trick")`\
| table title, eai:acl.owner, description, eai:acl.app, qualifiedSearch, next_scheduled_time\
| search next_scheduled_time!="" `splunkadmins_scheduledsearches_without_index` \
| regex qualifiedSearch!=".*index\s*(!?)=\s*([^*]|\*\S+)" \
| regex qualifiedSearch="^\s*search "\
| regex qualifiedSearch!="^\s*search\s*\[\s*\|\s*inputlookup"\
| rex field=qualifiedSearch "^(?P<exampleQueryToDetermineIndexes>[^\|]+)"\
| regex exampleQueryToDetermineIndexes!="\`"\
| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . "| stats values(index) AS index | format | fields search | eval search=replace(search,\"\\)\",\"\"), search=replace(search,\"\\(\",\"\"), search=if(search==\"NOT \",\"No indexes found\",search)"\
| rename eai:acl.owner AS owner, eai:acl.app AS Application
disabled = 1

[SearchHeadLevel - Script failures in the last day]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 22 * * *
description = Chance the alert requires action? Moderate. Scripts are throwing errors which may indicate an issue
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Shell scripts running from Splunk are failing to run or throwing errors")` \
index=_internal sourcetype=splunkd command="runshellscript" log_level="ERROR" OR log_level="WARN" `splunkadmins_scriptfailures` | fields _time, host, _raw
disabled = 1

[SearchHeadLevel - Splunk Max Historic Search Limits Reached]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 11 * * *
description = Chance the alert requires action? Moderate. Splunk Max Historic Search Limits Reached
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Once the max historic search limit has been reached the search jobs will be queued, this can be an issue if the limit is set too low (or too high)")` \
index=_internal `splunkenterprisehosts` "The maximum number of historical concurrent system-wide searches has been reached" OR "The system is approaching the maximum number of historical searches that can be run concurrently" (`splunkadmins_splunkd_source`) \
| fields _time, host
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Scheduler skipped searches and the reason]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 2,6,10,14,18,22 * * *
description = Chance the alert requires action? Low. Provides the skipped searches and a list of reasons why they were skipped. To remove false alarms this alert now checks if any shutdown messages appear, this may require tweaking in your environment as it checks for *any* Splunk enterprise instance shutdown... 
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("For some reason this search had to be skipped, this might be due to over scheduling the search or an inefficient search or similar")` index=_internal sourcetype=scheduler status=skipped source=*scheduler.log `splunkenterprisehosts` \
| search `comment("Exclude unable to distribute to peer messages where we sent the shutdown signal to the peer")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,0,0)`]\
| search `comment("Skipped searches can be expected for up-to 10 minutes after an indexer(s) has been shutdown...")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,0,600)`]\
| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen by savedsearch_id, reason, app, concurrency_category, concurrency_context, concurrency_limit, search_type, user, host \
| eval firstSeen = strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")
disabled = 1

[SearchHeadLevel - Splunk Users Violating the Search Quota]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 3 * * *
description = Chance the alert requires action? Low. These users have reached the search quota but may not be aware of this issue.
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have reached the search quota and may need to be informed of this, or they may need to be added to the macro for this alert")`\
index=_internal `searchheadhosts` (`splunkadmins_splunkd_source`) "was previously reported to be hung but has completed" OR "The maximum number of concurrent " `splunkadmins_users_violating_searchquota`\
| rex "Queued job id\s+=\s+(rt_)?(?P<username2>[^_]+)"\
| eval username=coalesce(username, username2)\
| bin span=24h _time\
| top showperc=false limit=200 username, reason, host, _time, provenance\
| where count>10
disabled = 1

[SearchHeadLevel - Users exceeding the disk quota]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 21 */2 * * *
description = Chance the alert requires action? High. One or more users have reached the disk quota limit and may not be aware of this... Can be fixed by the end user? Yes. You may wish to use sendresults with the output of this command...Search Head specific? Yes
dispatch.earliest_time = -2h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have reached the maximum disk quota, they may be unaware so it is best to let them know about this issue...")` \
`comment("Note that the REST API call accesses the jobs list which can expire for ad-hoc jobs in 10 minutes, so this may find zero results. The status.csv inside the dispatch directory records the size per job but it is not indexed by Splunk so either this alert needs to run very often or it will sometimes run after the issue has occurred and send an empty top 10 jobs list...")`\
`comment("The introspection index version is called SearchHeadLevel - Users exceeding the disk quota introspection, it is not search head specific but it is also less accurate for disk space used")`\
index=_internal sourcetype=splunkd `splunkenterprisehosts` (`splunkadmins_splunkd_source`) "maximum disk usage quota" `splunkadmins_users_exceeding_diskquota`\
| stats max(_time) AS mostRecent by username, reason, host\
| eval mostRecent = strftime(mostRecent, "%+")\
| search `comment("We use this bizarre field naming so when we append the actual search results we don't have 20 columns of data to read, also it looks nicer in an email. However since we want this over multiple lines and we only want to run the map command once we use a temporary field which we later expand to a multi-line field. Furthermore mvexpand on the search field can result in multiple rows per search which is why a temporary field is used")`\
| eval renameToSearch="Why am I, " + username + ", receiving this? |" + reason + " (from) " + host + "|_|Last seen? |" + mostRecent + "|_|Your top 10 largest jobs are listed below"\
| fields - reason, mostRecent, host\
| search `comment("The below is the complex attempt to include the largest jobs by querying the REST API. If we use map without the appendpipe we lose the original reason why we are sending this email. The initial workaround of makeresults and eval commands did work but this seemed slightly cleaner. Although there would be other ways to do this...")`\
| append [ | makeresults | eval username="workaround for map errors", body="to pass appinspect" ]\
| appendpipe\
    [\
| map \
    [| rest /services/search/jobs \
    | search `comment("Attempt to show the customer the top 10 jobs using disk and the related search commands/search names, also if it relates to their scheduled searches or not...")` author=$username$ diskUsage>0 \
    | fields diskUsage, eai:acl.app, latestTime, label, provenance, runDuration, searchEarliestTime, searchLatestTime, title, updated, ttl \
    | rename title AS search, eai:acl.app AS app, label AS searchName \
    | sort - diskUsage \
    | eval diskUsage=round(diskUsage/1024/1024,2), searchEarliestTime=strftime(searchEarliestTime, "%+"), searchLatestTime=strftime(searchLatestTime, "%+") \
    | eval expiry=strftime(strptime(updated, "%Y-%m-%dT%H:%M:%S.%3N%z")+ttl, "%+")\
    | eval runDuration=substr(tostring(runDuration,"duration"),0,8) \
    | eval search=substr(search,0,300) \
    | fields - provenance, ttl, updated \
    | eval searchName=if(searchName=="","ad-hoc search",searchName)\
    | eval renameToSearch="X"\
    | table searchName, app, diskUsage, expiry, runDuration, searchEarliestTime, searchLatestTime, search, renameToSearch\
    | head 10 ] \
]\
| where username!="workaround for map errors"\
| makemv delim="|" renameToSearch\
| mvexpand renameToSearch\
| eval search=if(renameToSearch!="X",renameToSearch,search)\
| table username, searchName, app, diskUsage, expiry, runDuration, searchEarliestTime, searchLatestTime, search
disabled = 1

[AllSplunkLevel execprocessor errors]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 28 5 * * *
description = Chance the alert requires action? Low. This alert can be very noisy, this will return any execprocessor errors from any script on any Splunk server!
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("An attempt to find the execprocessor errors these are often scripts or application which are having some kind of issue...")`\
index=_internal "ERROR ExecProcessor" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) NOT "Ignoring: \"" `splunkadmins_execprocessor`\
| dedup message, host | fields host _raw
disabled = 1

[IndexerLevel - Time format has changed multiple log types in one sourcetype]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 2 * * 1
description = Chance the alert requires action? High. A changing time format is likely due to multiple log types using the same sourcetype or a date time parsing issue
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This search detects when the time format has changed within the files 1 or more times, the time format per sourcetype should be consistent")`\
index=_internal "DateParserVerbose - Accepted time format has changed" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_timeformat_change`\
| rex "source::(?P<source>[^|]+)\|host::(?P<host>[^|]+)\|(?P<sourcetype>[^|]+)" \
| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen by host, source, sourcetype, message\
| eval invesMaxTime=if(firstSeen=lastSeen,lastSeen+1,lastSeen)\
| eval invesDataSource = replace(source, "\\\\", "\\\\\\\\")\
| eval potentialInvestigationQuery="`comment(\"If no results are found, prepend the earliest=/latest= with _index_ (eg _index_earliest=...) and expand the timeframe searched over, as the parsed timestamps from the data does not have to exactly match the time the warnings appeared...\")` sourcetype=\"" . sourcetype . "\" source=\"" . invesDataSource . "\" host=" . host . " earliest=" . firstSeen . " latest=" . invesMaxTime . " | eval start=substr(_raw, 0, 30) | cluster field=start"\
| eval firstSeen=strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")\
| fields - invesMaxTime, invesDataSource\
| sort - count
disabled = 1

[IndexerLevel - Volume (Cold) Has Been Exceeded]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 54 1,5,9,13,17,21 * * *
description = Chance the alert requires action? High. The non-hot volume has been exceeded therefore we are deleting data before the time limit is hit...
dispatch.earliest_time = -5h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The cold volume is causing indexes to be trimmed, this may or may not be an issue...")` \
index=_internal `indexerhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) "Size exceeds max, will have to trim " volume!="hot Size exceeds max"\
| fields host, _raw
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Scheduler excessive delays in executing search]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 6 * * 2
description = Chance the alert requires action? Moderate. Long latency delays in scheduled searches may indicate an issue, however the scheduled time of the search is what determine the search window. Therefore this only shows when it has taken a long period of time to execute an actual search (there is another alert for skipped searches)
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("These searches were scheduled to run at a particular time but the actual run time was more than X seconds later, which might indicate a search head performance issue")` \
`comment("In the environment this search was created on restarts are not overly common so we assume any restart might relate to a scheduling delay to prevent false alarms from the alert. This may need further tuning or you may wish to remove the where clause in this if you want more alerting.")`\
index=_internal `splunkenterprisehosts` sourcetype=scheduler app=* scheduled_time=* source=*scheduler.log\
| search `comment("Exclude time periods where shutdowns were occurring")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,600,600)`]\
| eval time=strftime(_time,"%+") \
| eval delay_in_start = (dispatch_time - scheduled_time) \
| where delay_in_start>100\
| eval scheduled_time=strftime(scheduled_time,"%+") \
| eval dispatch_time=strftime(dispatch_time,"%+") \
| rename time AS endTime \
| table host,savedsearch_name,delay_in_start, scheduled_time, dispatch_time, endTime, run_time, status, user, app \
| sort -delay_in_start \
| dedup host,savedsearch_name,delay_in_start
disabled = 1

[SearchHeadLevel - Splunk login attempts from users that do not have any LDAP roles]
action.analyzeioc.param.verbose = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 8 * * *
description = Chance the alert requires action? High. These usernames have appeared in the logs but they have no mapped roles. Can be fixed by the end user? Yes
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have attempted to login but were unable to, it is likely they do not have any LDAP role yet and should be informed of this")` \
index=_internal `searchheadhosts` "Couldn't find matching groups for user" OR "but none are mapped to Splunk roles" OR "SSO failed - User does not exist" (`splunkadmins_splunkd_source`) `splunkadmins_loginattempts`\
| join user [search index=_internal `searchheadhosts` action=login status=failure reason=user-initiated OR reason=sso-failed] \
| dedup user \
| table _time, user, host
disabled = 1

[IndexerLevel - Buckets are been frozen due to index sizing]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 33 3,7,11,15,19,23 * * *
description = Chance the alert requires action? High. One or more indexes have hit the index size limit and are now been frozen due to this
dispatch.earliest_time = -5h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer is freezing buckets due to disk space pressure before the frozenTimePeriodInSecs limit has been reached, this could be a problem if it is not expected...")`\
`comment("_introspection defaults to size based so exclude it")` \
index=_internal `indexerhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) "BucketMover - will attempt to freeze" NOT "because frozenTimePeriodInSecs=" \
`splunkadmins_bucketfrozen`\
| rex field=bkt "(rb_|db_)(?P<newestDataInBucket>\d+)_(?P<oldestDataInBucket>\d+)"\
| eval newestDataInBucket=strftime(newestDataInBucket, "%+"), oldestDataInBucket = strftime(oldestDataInBucket, "%+") \
| table message, oldestDataInBucket, newestDataInBucket
disabled = 1

[IndexerLevel - Indexer Out Of Disk Space]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = 4,19,34,49 * * * *
description = Chance the alert requires action? High. The indexer has run out of disk space while attempting to write to the filesystem
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer has run out of disk space, this requires immediate investigation...")` \
index=_internal "event=onFileWritten err=\"disk out of space\"" OR "event=replicationData status=failed err=\"onFileWritten failed\"" `indexerhosts` (`splunkadmins_splunkd_source`) sourcetype=splunkd \
| top host
disabled = 1

[AllSplunkEnterpriseLevel - Core Dumps Disabled]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 27 7 * * *
description = Chance the alert requires action? Moderate. Core Dumps are disabled and this may make support cases more difficult as sometimes the core dump is required for troubleshooting purposes.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Core dumps are disabled, if a crash occurs the Splunk Support team might not be able to assist without the core dump")`\
`comment("https://answers.splunk.com/answers/223838/why-are-my-ulimits-settings-not-being-respected-on.html applies to core limits so if the server has been rebooted the init.d may need a ulimit -Hc/Sc setting for this as well...")`\
index=_internal "WARN  ulimit - Core file generation disabled" `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \
| stats max(_time) AS mostRecentlySeen by host\
| eval mostRecentlySeen = strftime(mostRecentlySeen, "%+")
disabled = 1

#TODO will be removed in a future version, likely after December 2018
[IndexerLevel - Unable to replicate thawed directories in a cluster]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 41 2,6,10,14,18,22 * * *
description = Chance the alert requires action? High. This doesn't work as documented/expected so warn about it if someone does thaw buckets to the thawed directory, this alert is no longer required after versions 6.5.7, 6.6.5 or 7.0.2.
dispatch.earliest_time = -4h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Using the thaweddb location in a clustered environment might not work as expected, warn about this issue")` \
index=_internal `indexerhosts` (`splunkadmins_splunkd_source`) sourcetype=splunkd "Failed to trigger replication" bucketType=*thaweddb* \
| rex field=bucketType "(?P<indexpath>.*)(/|\\\\)[^/\\\\]+$"\
| eval error="Issue resolved in 6.5.7, 6.6.5 and 7.0.2. As per https://answers.splunk.com/answers/153341/thawed-buckets-error-clusterslavebuckethandler-failed-to-trigger-replication.html (otherwise use the workaround linked)..."\
| top showcount=false showperc=false host, error, indexpath
disabled = 1

[ForwarderLevel - Splunk Insufficient Permissions to Read Files]
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 51 6 * * *
description = Chance the alert requires action? Low. An insufficient permissions to read files error was thrown...
dispatch.earliest_time = -1d@d
dispatch.latest_time = @d
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats count groupby host, source\
| append \
    [ search\
        `comment("This search looks for insufficient permissions errors, the problem here is that we might have insufficient permissions to read a file but we might later obtain the correct permissions and then read the file (as permissions changes can happen *after* the file creation...this is why there is both a tstats listing all files (only done because I cannot find a nicer way to do this, map is possibly more compute intensive), and then a search for files")`\
        index=_internal "Insufficient permissions to read file" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`)\
    | rex "\(hint: (?P<hint>[^\)]+)"\
    | stats min(_time) AS firstSeen, max(_time) AS mostRecent, values(hint) AS hint by file, host \
    | rex field=file "'(?P<source>[^']+)'" \
    | eval insufficientpermissions="true" \
    | fields firstSeen, mostRecent, source, host, insufficientpermissions, hint] \
| search `comment("Ignore any files requested by the macro, i.e. source!= or host!= or similar...")` `splunkadmins_permissions`\
| stats sum(count) AS count, min(firstSeen) AS firstSeen, max(mostRecent) AS mostRecent, values(insufficientpermissions) AS insufficientpermissions, values(hint) AS hint by host, source \
| search `comment("If we have an insufficient permissions error, did we see no data from our tstats command?")` insufficientpermissions="true" NOT count=* `splunkadmins_insufficient_permissions`\
    `comment("At this point if we see an insufficient permissions line, and we cannot see a result from the tstats showing indexed data from that file, then we have an issue, if not there is no issue with permisisons!")` \
    `comment("Insufficient permissions to read file + hint: No such file or directory when the file exists on a Splunk enterprise instance might require TAILING_SKIP_READ_CHECK = 1 in the splunk-launch.conf refer to splunk support for more info")` \
| eval invesSource=replace(source, "\\\\", "\\\\\\\\") \
| addinfo \
| eval investigationQuery="index=_internal \"Insufficient permissions to read file\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) earliest=" . info_min_time . " latest=" . info_max_time . " host=" . host . " file=\"'" . invesSource . "'\""\
| eval firstSeen=strftime(firstSeen, "%+"), mostRecent=strftime(mostRecent, "%+") \
| fields host, source, firstSeen, mostRecent, hint, investigationQuery
disabled = 1

[AllSplunkLevel - TCP Output Processor has paused the data flow]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 44 * * * *
description = Chance the alert requires action? Low. A potential indicator of poor index performance or an overloaded forwarder
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A paused TCP output processor is a potential indicator of an index performance issue, you may wish to ignore the shorter pause times such as 10 seconds if this is creating too many alerts...")`\
`comment("On the indexer side you may see 'WARN TcpInputProc - Stopping all listening ports. Queues blocked for more than...' OR 'WARN TcpInputProc - Started listening on tcp ports. Queues unblocked', if there are indexer performance issues...")`\
index=_internal "The TCP output processor has paused the data flow. Forwarding to output group" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_tcpoutput_paused`\
| search `comment("Exclude shutdown times")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,60,60)`]\
| rex "has been blocked for (?P<timeperiod>\d+)"\
| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, first(message) AS message, max(timeperiod) AS maxInSeconds, avg(timeperiod) AS avgTimePeriod by host\
| eval firstSeen=strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+"), avgTimePeriod=round(avgTimePeriod)
disabled = 1

[IndexerLevel - These Indexes Are Approaching The warmDBCount limit]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 48 6 * * 1
description = Chance the alert requires action? Moderate. Buckets are either now rolling or will roll to cold due to the bucket count limit in warm been reached, this may need ajustment
dispatch.earliest_time = -90d@d
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | dbinspect index=* state=warm\
| search `comment("Once the warmdb bucket count is reached then the buckets are moved to cold, this may be an issue if incorrectly configured, this alert warns in advance if we get close to the limit")`\
`comment("This might be a bug in 6.5.2 but the buckets are printed twice by dbinspect in some cases...")` `splunkadmins_warmdbcount`\
| dedup bucketId, splunk_server\
| stats count AS theCount by index, splunk_server\
| stats avg(theCount) AS averageCount, max(theCount) AS maxCount, min(theCount) AS minCount, values(splunk_server) by index\
| eval averageCount = round(averageCount)\
| join index [| rest /services/data/indexes \
              | dedup title \
              | rename title AS index \
              | table index, maxWarmDBCount]\
| eval percUsed = (100/maxWarmDBCount)*averageCount\
| where percUsed > `splunkadmins_warmdbcount_perc`
disabled = 1

[ForwarderLevel - SplunkStream Errors]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 9 11 * * *
description = Chance the alert requires action? Moderate. Errors from the Splunk stream forwarders will normally require an action. Note that this search assumes your search heads are the ones hosting the stream application...you may need to customise this.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = index=_internal source="*streamfwd.log" ERROR OR FATAL `splunkadmins_streamerrors`\
| search `comment("Exclude time periods where shutdowns were occurring")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,0,0)`]\
| cluster showcount=true\
| fields host, _raw
disabled = 1

[SearchHeadLevel - LDAP users have been disabled or left the company cleanup required]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 33 11 * * *
description = Chance the alert requires action? High. These users have been disabled or left the company but their users files are on the filesystem and this is therefore triggering warning or errors in the Splunk logs, please cleanup the old user files for these users.\
A separate alert should exist for orphaned searches...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If we see a failed to get LDAP user 'username' from any configured servers then that is a sign the user is no longer in the company. However if there is also a message of Couldn't find matching groups for that same user it is more likely that they exist but just do not have access to Splunk")`\
`comment("If you see this alert fire, than you probably need to cleanup the (for example) /opt/splunk/etc/users/... directory on each search head due to a user leaving/becoming disabled in LDAP. Alternatively they have a savedsearch/dashboard that you can find in the .meta files on the search head(s)")`\
index=_internal `searchheadhosts` "Failed to get LDAP user=\"" OR "Couldn't find matching groups for user=" OR "HTTPAuthManager - SSO failed - User does not exist" sourcetype=splunkd (`splunkadmins_splunkd_source`)\
| dedup message \
| rex "SSO failed - User does not exist: (?P<user>\S+)"\
| stats count, values(message) AS messages, values(component), AS components values(log_level), max(_time) AS lastSeen by user, host\
| search `comment("count=1 eliminates users who are failing to login...if a user is active in LDAP but fails to login we should not not get a 'Couldn't find matching groups for user' line in the logs")`\
`comment("If we are using a single sign on system and a user without any groups attempts sign on we should see the SSO failed - User does not exist: <username> message")`\
| where user!="undefined" AND user!="nobody" AND like(messages,"Failed to get LDAP user%") AND NOT like(messages,"SSO failed - User does not exist%")\
| table user, messages, lastSeen, host\
| eval lastSeen=strftime(lastSeen, "%+")
disabled = 1

[AllSplunkLevel - DeploymentServer Application Installation Error]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 11 * * * *
description = Chance the alert requires action? High. The deployment server sent out a new application but for some reason it has failed to install
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Failed to install app can appear *if* the application is installed to a particular targetRepositoryLocation (for example /opt/splunk/etc/deployment-apps or similar) and stateOnClient=noop is not added to the application within serverclass.conf")`\
index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) "ERROR DeployedServerclass - name=* Failed to install" OR "DeployedApplication - Installing app="\
| eventstats count(eval(log_level="ERROR")) AS errorCount, count(eval(log_level="INFO")) AS successCount by host, app \
| where errorCount>0 AND successCount<1
disabled = 1

[AllSplunkLevel - Unable To Distribute to Peer]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 9,24,39,54 * * * *
description = Chance the alert requires action? Low. A Splunk instance is advising that it cannot distribute to a peer node (indexer, another search head in the cluster or similar)
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Unable to distribute to peer messages often indicate downtime or serious performance issues")`\
index=_internal "Unable to distribute to peer named" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` `splunkadmins_unable_distribute_to_peer`\
| rex "(?P<message>Unable to distribute to peer named (?P<peer>[^: ]+))"\
| bin _time span=1m\
| join type=outer peer\
    [ rest /services/search/distributed/peers \
    | fields peerName, title\
    | rex field=title "(?P<title>[^:]+)"\
    | rename title AS peer ]\
| eval targetHost=if(isnotnull(peerName),peerName,peer)\
| search `comment("Exclude unable to distribute to peer messages where we sent the shutdown signal to the peer")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,0,0)`]\
| stats count, values(message) AS message, values(host) AS reportingHostList by _time, targetHost \
| eval reportingHostList=mvindex(reportingHostList,0,9)\
| sort - _time\
| where count>1
disabled = 1

[SearchHeadLevel - Alerts that have not fired an action in X days]
action.keyindicator.invert = 0
alert.track = 0
description = Report only? Yes. This report can be run to determine which alerts have not sent an alert based on the time period / amount of internal logs available...Search Head specific? Yes
dispatch.earliest_time = -30d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to find alerts that are scheduled but not firing any actions, the alerts may need further review or may no longer be required. The app regex is in here because of some creative alert naming, X:app=Y is a real alert name in my environment!")`\
index=_internal source="*scheduler.log" sourcetype=scheduler `searchheadhosts` alert_actions!="" \
| rex ", app=\"(?P<app>[^\"]+)\","\
| stats count by savedsearch_name, app \
| append \
    [| rest splunk_server=local /servicesNS/-/-/saved/searches \
    | search actions!="summary_index" actions!="" next_scheduled_time!="" search!="| noop" \
    | table eai:acl.app, title \
    | eval fromRESTQuery=""\
    | rename title as savedsearch_name, eai:acl.app as app ]\
| eventstats count(eval(isnotnull(fromRESTQuery))) AS restCount, count by savedsearch_name, app\
| where restCount=1 AND count=1\
| table savedsearch_name, app

[SearchHeadLevel - Data Model Acceleration Completion Status]
action.keyindicator.invert = 0
alert.track = 0
description = Report only? Yes. The % complete of the data model which is stored on the indexer level but run from the search head level...refer to the data model dashboards for more detailed information. Search Head specific? Yes
dispatch.earliest_time = @d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/admin/summarization by_tstats=t splunk_server=local count=0 \
| search `comment("Found on https://answers.splunk.com/answers/555005/how-to-check-the-percent-of-the-dm-acceleration-co.html ")`\
| eval datamodel=replace('summary.id',"DM_".'eai:acl.app'."_","") \
| join type=left datamodel \
  [| rest /services/data/models splunk_server=local count=0 \
  | table title acceleration.cron_schedule eai:digest \
  | rename title as datamodel \
  | rename acceleration.cron_schedule AS cron] \
| table datamodel eai:acl.app summary.access_time summary.is_inprogress summary.size summary.latest_time summary.complete summary.buckets_size summary.buckets cron summary.last_error summary.time_range summary.id summary.mod_time eai:digest summary.earliest_time summary.last_sid summary.access_count \
| rename summary.id AS summary_id, summary.time_range AS retention, summary.earliest_time as earliest, summary.latest_time as latest, eai:digest as digest \
| rename summary.* AS *, eai:acl.* AS * \
| sort datamodel

[SearchHeadLevel - User - Dashboards searching all indexes]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 30 5 * * 1-5
description = Chance the alert requires action? High. All dashboard panels that do not have an index= setting or use index=* are highlighted by this alert. Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/data/ui/views \
| search `comment("A dashboard searching all indexes is an issue just like a scheduled search querying all indexes or using the index=* trick")`\
eai:data=*query* `splunkadmins_dashboards_allindexes`\
| regex eai:data="<search.*" \
| rex field=eai:data "(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\/query>" max_match=200 \
| mvexpand theSearch \
| rex field=theSearch "<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)" \
| search `comment("If we are seeing post process search then we don't want to check if it has index= because that is likely only in the base query. These are also various exclusions for legitimate searches that will not involve scanning all indexes, such as rest or a savedsearch or similar")` searchInfo!="*base*"\
| rename eai:appName AS application, eai:acl.sharing AS sharing, eai:acl.owner AS owner, label AS name\
| table theQuery, application, owner, sharing, name, splunk_server, title\
| regex theQuery!="index\s*=(?!\s*\*)" \
| regex theQuery!="^(\()?\s*(\`|\$[^|]+\$|eventtype=|<!\[CDATA\[\s*\|\s*((acl)?inputlookup|rest) |\|)"\
| rex field=theQuery "^(?P<exampleQueryToDetermineIndexes>[^\|]+)"\
| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . "| stats values(index) AS index | format | fields search | eval search=replace(search,\"\\)\",\"\"), search=replace(search,\"\\(\",\"\"), search=if(search==\"NOT \",\"No indexes found\",search)"
disabled = 1

[SearchHeadLevel - Scheduled Searches Configured with incorrect sharing]
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 5 * * *
description = Chance the alert requires action? High. These searches are triggering scripts or alerts which will provide a results link to Splunk. But the sharing is not app or global and therefore the link is unusable to anyone who is not the owner...Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("The problem with alerts that are configured privately is that no one beyond the author can use the results link and non-admins cannot even see the alert in Splunk!")`\
`comment("Therefore we find anything that emails that is not shared correctly *and* anything that uses a script as often the script will include a results link.")`\
`comment("The idea here is to let the end user know so they can share it appropriately, the noop search is excluded to remove scheduled views from this list")`\
is_scheduled=1 disabled=0 eai:acl.sharing!="global" eai:acl.sharing!="app" search!="| noop" actions!="" `splunkadmins_scheduled_incorrectsharing`\
| eval numberOfEmailed = mvcount(split('action.email.to',"@"))-1\
| table title, eai:acl.app, author, eai:acl.sharing, actions, action.email.to, numberOfEmailed\
| where numberOfEmailed>1 OR isnull(numberOfEmailed)\
| sort author
disabled = 1

[SearchHeadLevel - Realtime Search Queries in dashboards]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 11 4 * * 1
description = Chance the alert requires action? High. Just a summary of all dashboards that use realtime searching...Search Head specific? Yes
dispatch.earliest_time = -48h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest/servicesNS/-/-/data/ui/views | regex eai:data="<(earliest|latest)(Time)?>rt"\
| search `comment("Shows realtime search usage within dashboards")` eai:data=*query* `splunkadmins_realtime_dashboard`\
| regex eai:data="<search.*"\
| rex field=eai:data "(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\/query>" max_match=200 \
| mvexpand theSearch \
| rex field=theSearch "<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)" \
| search searchInfo!="*base*" `comment("Exclude queries which have a base, in general they will not have a earliest/latesttime so this gets confusing")`\
`comment("It might be possible to use mvzip / mvexpand or mvindex to match the correct earliesttime/latesttime with each search query but it proved extremely difficult. So just keeping this as-is as the dashboard needs to be reviewed if it has too many realtime searches anyway")`\
| rex field=eai:data "<earliest(Time)?>(?P<earliesttime>[^<]+)" max_match=200 \
| rex field=eai:data "<latest(Time)?>(?P<latesttime>[^<]+)" max_match=200 \
| table title, eai:appName, searchInfo, theQuery,eai:acl.owner, eai:acl.sharing, label, earliesttime, latesttime, splunk_server
disabled = 1

[AllSplunkEnterpriseLevel - Transparent Huge Pages is enabled and should not be]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 14 2 * * *
description = Chance the alert requires action? High. Transparent huge pages should never be enabled on a Splunk enterprise server as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect when transparent huge pages is enabled on a Linux server, it should be disabled.")`\
`comment("Redhat Linux has an issue where the transparent huge pages setting changes after Splunk starts if the server was rebooted, check /sys/kernel/mm/transparent_hugepage to confirm...")`\
`comment("| rest /services/server/sysinfo is an alternative if you want the current search head + indexers, but this will ignore other search heads...")`\
index=_internal "Linux transparent hugepage support, enabled=" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` enabled!="never"\
| eval error="This configuration of transparent hugepages is known to cause serious runtime problems with Splunk. Typical symptoms include generally reduced performance and catastrophic breakdown in system resp\
onsiveness under high memory pressure. Please fix by setting the values for transparent huge pages to \"madvise\" or preferably \"never\" via sysctl, kernel boot parameters, or other method recommended by your Linux distribution."\
| table _time, host, _raw, error
disabled = 1

[IndexerLevel - Old data appearing in Splunk indexes]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 33 7 * * 0
description = Chance the alert requires action? Moderate. A slightly more complex alert that attempts to find recently indexed data that is been indexed with older timestamps, an attempt to find invalid date parsing for Splunk inputs
dispatch.earliest_time = -10y 
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats max(_time) AS mostRecentlySeen, max(_indextime) AS mostRecentlyIndexed, min(_time) AS earliestSeen, min(_indextime) AS earliestIndexTime , count \
    where _index_earliest=`splunkadmins_olddata_lookback`, earliest=`splunkadmins_olddata_earliest`, latest=`splunkadmins_olddata_latest` \
    groupby source, sourcetype, index, host\
| search `comment("Find data that appears to be logged in the past, this may indicate poor timestamp parsing (or we're just ingesting really old data")` `splunkadmins_olddata`\
| eval invesDataSource = replace(source, "\\\\", "\\\\\\\\"), invesLatestTime=mostRecentlySeen+1, invesLatestIndexTime=mostRecentlyIndexed+1\
| eval investigationQuery="`comment(\"Narrow down to the older part of the timeline after this query runs to see the potential issue...\")` index=" . index . " source=\"" . invesDataSource . "\" sourcetype=\"" . sourcetype . "\" host=" . host . " earliest=" . earliestSeen . " latest=" . invesLatestTime . " _index_earliest=" . earliestIndexTime . " _index_latest=" . invesLatestIndexTime . " | eval indextime=strftime(_indextime, \"%+\")" \
| eval mostRecentlySeen=strftime(mostRecentlySeen, "%+"), mostRecentlyIndexed=strftime(mostRecentlyIndexed, "%+")\
| sort index, host, sourcetype\
| table index, source, sourcetype, host, mostRecentlySeen, mostRecentlyIndexed, count, investigationQuery
disabled = 1

[AllSplunkLevel - Splunk forwarders that are not talking to the deployment server]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 8 * * *
description = Chance the alert requires action? Moderate. All forwarders should talk to the deployment server unless they have a special reason for an exclusion...
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats count where index=_internal groupby host \
| fields host \
| search `comment("This is an attempt to find any universal forwarders that send data into the indexers but do not phone home to the expected deployment server")` `splunkadmins_forwarders_nottalking_ds`\
| table host \
| search NOT [search index=_internal `deploymentserverhosts` source="*splunkd_access.log" sourcetype=splunkd_access \
             | rex field=uri "/services/broker/phonehome/connection_[^_]+_[89][0-9]{3}_[^_]+(_[0-9][^_]+)?_(?P<hostname>[^_]+)_" \
             | eval host=hostname \
             | dedup host\
             | table host]\
| search `comment("A small number of hosts come in as IP addresses instead of DNS names, ignore these ones if they phone home as expected")` \
NOT [search index=_internal `deploymentserverhosts` source="*splunkd_access.log" sourcetype=splunkd_access \
             | rex field=uri "/services/broker/phonehome/connection_(?P<ipaddr>[^_]+)_[89][0-9]{3}_[^_]+(_[0-9][^_]+)?_[^_]+_" \
             | dedup ipaddr\
             | rename ipaddr AS host\
             | table host]              \
| search NOT (`splunkenterprisehosts`) \
| lookup dnslookup clienthost AS host \
| search clientip!=''
disabled = 1

[AllSplunkEnterpriseLevel - sendmodalert errors]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
alert_condition = search savedsearch_name=*
counttype = custom
cron_schedule = */15 * * * *
description = Chance the alert requires action? Low. sendmodalert errors from Splunk might advise of a failure in an alert action
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("sendmodalert or sendalert errors and warnings may be an issue relating to the creation of alerts via a script")`\
index=_internal `splunkenterprisehosts` "ERROR sendmodalert - action=" OR "WARN sendmodalert - action" OR "Error in 'sendalert' command" sourcetype=splunkd (`splunkadmins_splunkd_source`)\
`comment("If you need more context on the above errors add this snippet into the above search (remove the \\):\
OR \"sendmodalert - Invoking modular alert action\"\
")`\
`splunkadmins_sendmodalert_errors`\
| rex field=results_file "[/\\\]dispatch[/\\\](?P<sid>[^/]+)"\
| eval sid=if(isnull(sid),"NOMATCH",sid)\
| join sid type=outer [search index=_internal source="*scheduler.log" sourcetype=scheduler `splunkenterprisehosts` | table sid, savedsearch_name, app, user]\
| cluster showcount=true\
| table host, savedsearch_name, app, user, _raw, _time, cluster_count\
| eval mostRecent = strftime(mostRecent, "%+")\
| sort - _time
disabled = 1

[IndexerLevel - Indexer not accepting TCP Connections]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 3,18,33,48 * * * *
description = Chance the alert requires action? Low. The indexer is either overloaded or down and not accepting TCP connections...
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer not accepting TCP connections is either a serious performance issue or downtime")` \
index=_internal TcpOutputFd "connection refused" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \
| rex "Connect to (?P<clientip>[^:]+)" \
| top clientip \
| lookup dnslookup clientip \
| where count>10
disabled = 1

[IndexerLevel - Buckets rolling more frequently than expected]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 8 8 * * *
description = Chance the alert requires action? Moderate. Indexer level issues - Buckets are moving out of the warm state quicker than expected and this may (or may not) be an issue, this could indicate that hot is undersized or there are too many buckets in the warm area.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Buckets are moving from warm to cold very quickly and this could be an issue related to the sizing not been valid for the indexes...")` \
`indexerhosts` index=_internal "Will chill bucket" (`splunkadmins_splunkd_source`) sourcetype=splunkd "/db/db"  \
| rex "=/.*?(?P<indexname>[^/]+)(/[^/]+){2} " \
| stats count by indexname \
| sort - count \
| where (count>`splunkadmins_bucketrolling_count`)
disabled = 1

[What Access Do I Have?]
action.keyindicator.invert = 0
alert.track = 0
description = Report only? Yes. Determine the access of the currently logged in user. Search Head specific? Yes. Please open in search and re-execute to make this work...
dispatch.earliest_time = @d
dispatch.latest_time = now
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = Global
request.ui_dispatch_view = search
search = | rest /services/authentication/users splunk_server=local\
| search `comment("REST query is limited to the current search head this is running on, note this version requires the ability to dispatch REST to indexers capability, if not available use the 'What Access Do I Have Without REST' report")`\
    [| rest /services/authentication/current-context/context splunk_server=local\
    | head 1 \
    | fields username \
    | rename username AS title] \
| table title roles | rename title as user | mvexpand roles\
| join type=left roles \
    [rest /services/authorization/roles splunk_server=local\
    | table title srchIndexesAllowed srchIndexesDefault | rename title as roles]\
| makemv srchIndexesAllowed tokenizer=(\S+) | makemv srchIndexesDefault tokenizer=(\S+)\
| fillnull value=" "\
| mvexpand srchIndexesAllowed | mvexpand srchIndexesDefault\
| join type=left max=999 srchIndexesAllowed \
    [ rest /services/data/indexes \
    | table title \
    | eval srchIndexesAllowed = if(match(title, "^_"), "_*", "*") \
    | rename title as IndexesAllowed]\
| join type=left max=999 srchIndexesDefault \
    [rest /services/data/indexes \
    | table title \
    | eval srchIndexesDefault = if(match(title, "^_"), "_*", "*") \
    | rename title as IndexesDefault]\
| stats values(*) as * by user\
| foreach srch* [eval <<FIELD>> = mvappend(<<FIELD>>, <<MATCHSTR>>) | eval <<FIELD>> = mvfilter(match(<<FIELD>>, "^[^*]+$"))]\
| fields - Indexes*

[ForwarderLevel - Read operation timed out expecting ACK]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 47 */2 * * *
description = Chance the alert requires action? Low. Acknowledgement from the indexers should ideally never timeout, the time out may cause duplication issues
dispatch.earliest_time = -2h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This read operation timed out expecting ACK will likely result in the forwarder re-sending at least some data to the indexer. This can be caused by lack of CPU on the forwarder and potentially other issues...")`\
index=_internal "Read operation timed out expecting ACK from" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`)\
| rex "from (?P<indexer>\S+)"\
| stats count, max(_time) AS mostRecent by host, indexer\
| eval mostRecent=strftime(mostRecent, "%+")\
| search `comment("Allow exclusions such as ignoring a count per host or similar...")` `splunkadmins_readop_expectingack`
disabled = 1

[AllSplunkEnterpriseLevel - Replication Failures]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? Moderate. Replication failures often show a search head that is having issues after an indexer restart, the search head might require a restart to resolve this or investigation.
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Replication status failure on a search head, the search head may require a restart or investigation...")`\
index=_internal "because replication was unsuccessful. replicationStatus Failed failure info: failed_because" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` `splunkadmins_repfailures`\
| stats count, max(_time) AS mostRecent by host, message\
| eval mostRecent=strftime(mostRecent, "%+") 
disabled = 1

[AllSplunkEnterpriseLevel - Low disk space]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */5 * * * *
description = Chance the alert requires action? Moderate. Low disk space on one or more partitions of the Splunk enterprise servers...
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Use introspection data to monitor Splunk mount points, if you want to monitor non-Splunk directories use nmon or another monitoring system")`\
index=_introspection host=* component=Partitions `splunkadmins_lowdisk`\
| eval available='data.available', capacity='data.capacity', mount_point='data.mount_point'\
| eval percfree = round((available/capacity)*100,2)\
| stats min(percfree) AS percfree, min(available) AS minMBAvailable by mount_point, host\
| search `comment("Below 10% (default only, can be changed in the macro) is an issue unless it's an indexer, as 10% of the indexer is actually a very large amount of data...")`\
(percfree<`splunkadmins_lowdisk_perc` NOT (`indexerhosts`)) OR (minMBAvailable<`splunkadmins_lowdisk_mb` (`indexerhosts`))
disabled = 1

[AllSplunkEnterpriseLevel - KVStore Process Terminated]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? High. Ideally you shouldn't see this error...
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This ideally should never happen during normal runtime...")`\
index=_internal `searchheadhosts` "KV Store process terminated" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_kvstore_terminated`\
| fields _time, host, _raw
disabled = 1

[AllSplunkEnterpriseLevel - File integrity check failure]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 33 9 * * *
description = Chance the alert requires action? Moderate. File integrity check failure would generally mean a change has been made to parts of Splunk that will be wiped out next upgrade
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("One or more files did not pass the startup hash-check against the Splunk provided manifest, you can tune the limits.conf to control how the warning is logged or not logged")`\
index=_internal `splunkenterprisehosts` "An installed * did not pass hash-checking due to" (`splunkadmins_splunkd_source`) sourcetype=splunkd `splunkadmins_fileintegritycheck`\
| stats count, latest(_time) AS lastSeen by message, host\
| eval lastSeen=strftime(lastSeen, "%+")
disabled = 1

[AllSplunkEnterpriseLevel - WARN iniFile Configuration Issues]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 4 * * 1
description = Chance the alert requires action? Low. Detect configuration errors in the files that the indexer cluster or enterprise servers are throwing warnings about
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find potentially invalid configuration within the Splunk applications on search heads/indexers and warn about this...")` \
index=_internal WARN IniFile `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_warninifile`\
| cluster showcount=true\
| fields _time, host, cluster_count, _raw
disabled = 1

[SearchHeadLevel - Long filenames may be causing issues]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 56 5 * * *
description = Chance the alert requires action? Moderate. There are one or more dashboards or alerts with a filename long enough to cause errors in the archive processor, the exact implications are unknown but the alert/dashboard may need to be removed.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect the issue where someone has created a file longer than 100 characters and the cluster is having issues with replication. The 100 character issue was confirmed in Splunk 6.5.2 during a support case")`\
index=_internal `searchheadhosts` ("ArchiveFile - Failed to write archive header for" "Pathname too long") OR ("ERROR Archiver - Unable to add entry") (`splunkadmins_splunkd_source`) sourcetype=splunkd \
| cluster showcount=true\
| fields _time, _raw, cluster_count
disabled = 1

[IndexerLevel - Large multiline events using SHOULD_LINEMERGE setting]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 42 7 * * *
description = Chance the alert requires action? Moderate. This alert advises that a multi-line event is appearing in Splunk that is large enough that the default SHOULD_LINEMERGE = true setting may cause blocking in the indexer aggregation queue, it's much more efficient to configure the SHOULD_LINEMERGE = false and LINE_BREAKER = ... if possible, note the TRUNCATE settings will likely need to be much larger to deal with the LINE_BREAKER change.\
Please update the props.conf for this sourcetype to LINE_BREAKER if applicable (and the TRUNCATE setting).
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats max(linecount) AS maxLineCount, min(_time) AS firstSeen, max(_time) AS mostRecent, values(host) AS hosts, count AS occurrenceCount where index=*, linecount>250 groupby sourcetype\
| search `comment("This search detects sourcetypes with greater than 250 lines which have SHOULD_LINEMERGE set to true, this might cause blocking in the indexer aggregation queue if there are a large number")`\
`comment("of events with hundreds of lines or very large events such as >5000 lines of data. This alert is designed to give hints about where SHOULD_LINEMERGE=false / LINE_BREAKER=... might be more appropriate")`\
`comment("Note that the REST API will return every instance of sourcetype, it's not quite as accurate a btool so this can generate false alarms if there are multiple props.conf definitions of a sourcetype")`\
`splunkadmins_multiline_linemerge`\
| join [| rest `splunkindexerhostsvalue` /servicesNS/-/-/configs/conf-props\
| fields title SHOULD_LINEMERGE\
| search SHOULD_LINEMERGE = 1\
| dedup title | rename title AS sourcetype]\
| where maxLineCount > 260 AND occurrenceCount>30\
| eval hostList=if(mvcount(hosts)>1,mvjoin(hosts," OR host="),hosts)\
| eval hostList="host=" . hostList\
| eval investigationQuery="index=* sourcetype=" . sourcetype . " " . hostList . " linecount>250 earliest=" . firstSeen . " latest=" . mostRecent\
| sort - occurrenceCount, maxLineCount\
| table sourcetype, maxLineCount, occurrenceCount, investigationQuery
disabled = 1

[IndexerLevel - Data parsing error]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */10 * * * *
description = Chance the alert requires action? High. This alert advises there is an error with the LINE_BREAKER or Aggregator, this generally relates to a misconfiguration that requires a fix...
dispatch.earliest_time = -10m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("LineBreakingProcessor ERROR's are usually related to misconfiguration/errors in the LINE_BREAKER= setup in props.conf and are therefore an issue. This version of the Aggregator error often relates to date time config files")`\
index=_internal "ERROR LineBreakingProcessor" OR "AggregatorMiningProcessor - Uncaught exception in Aggregator" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) | cluster | fields host, _raw
disabled = 1

[SearchHeadLevel - KVStore Or Conf Replication Issues Are Occurring]
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 12,22,32,42,52,02 * * * *
description = Chance the alert requires action? High. If the KVStore is out of sync or the search head is out of sync it will likely require a manual resync/clean to get it working as expected\
If it relates to a conf replication issue it is likely a problematic search head requiring a restart or it may require a force sync...(the logs will advise on this)\
To remove false alarms this alert now checks if any shutdown messages appear, this may require tweaking in your environment as it checks for *any* search head shutdown...
dispatch.earliest_time = -10m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect search head issues related to extended search head downtime, in particular ConfReplication issues or KV store replication issues")`\
`comment("KVStore - http://docs.splunk.com/Documentation/Splunk/latest/Admin/ResyncKVstore , ConfReplication - http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/HowconfrepoworksinSHC#Replication_synchronization_issues")`\
`comment("The search head cluster captain is disconnected can relate to a SH cluster restart *or* if outside a rolling restart this may require a restart of the problematic search head...")`\
`comment("In addition to this you could also look for "Error pushing configurations to captain" consecutiveErrors>1 , this would also hint at a potential issue although a small number of consecutive errors appears to be normal...")`\
index=_internal `searchheadhosts` "Local KV Store has replication issues" OR "ConfReplicationThread - Error pulling configurations from captain" OR "ConfReplicationThread - The search head cluster captain * is disconnected; skipping configuration replication" OR ("SHCMasterHTTPProxy - Low Level http request " "socket_error") sourcetype=splunkd (`splunkadmins_splunkd_source`) \
| search `comment("Exclude time periods where shutdowns were occurring")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,0,0)`]\
| cluster showcount=true \
| fields _time, host, _raw, message, cluster_count
disabled = 1

[SearchHeadLevel - SHCluster Artifact Replication Issues]
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 54 * * * *
description = In this scenario either something has changed or one or more search heads are not syncing the artifacts as expected, a restart of the SH cluster usually resolves this.
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("When this issue occurs it is likely related to some kind of issue post-restart of the indexer/search head cluster. Restarting the search head cluster appears to resolve the issue in 6.5.2")`\
index=_internal "ERROR SHCArtifactId - *This GUID does not match the member's current GUID" sourcetype=splunkd (`splunkadmins_splunkd_source`) `searchheadhosts` \
| eventstats max(_time) AS lasterror, min(_time) AS firsterror\
| cluster showcount=true \
| table host, cluster_count, _raw, lasterror, firsterror\
| eval lasterror = strftime(lasterror, "%+"), firsterror = strftime(firsterror, "%+")
disabled = 1

[LicenseMaster - Duplicated License Situation]
alert.suppress = 0
alert.track = 1
alert.severity = 5
counttype = number of events
cron_schedule = 27 * * * *
description = Chance the alert requires action? High. A duplicated licensing situation will normally require intervention to fix, this scenario can happen when a forwarder is not using the forwarder license or license master but is sending data into the indexers...
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This warning will appear in the messages console of the license master, considering you have 72 hours to fix the issue you might want to be alerted about this so it can be fixed promptly! The scenario is likely to occur when a heavy forwarder is sending data into the indexers without using the forwarder license or talking to the cluster master.")`\
index=_internal `licensemasterhost` "Duplicated License situation happen" (`splunkadmins_splunkd_source`) \
| cluster showcount=true \
| fields _time, host, source, _raw, cluster_count
disabled = 1

[DeploymentServer - Unsupported attribute within DS config]
alert.suppress = 0
alert.track = 1
alert.severity = 4
counttype = number of events
cron_schedule = 48 */4 * * *
description = Chance the alert requires action? High. A syntax error from manually editing the serverclass.conf will normally need to be fixed. Note that this alert is only useful if you are manually editing your serverclass.conf, if you only use the GUI then it is unlikely that this alert will ever be triggered.
dispatch.earliest_time = -4h@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A DS_DC_Common warning normally relates to a typo within the serverclass.conf, most likely due to manual editing, this will prevent the GUI from been used for the forwarder management configuration...")`\
index=_internal "WARN" "DS_DC_Common" `deploymentserverhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`)\
| stats first(_time) AS lastSeen by message, host\
| eval lastSeen=strftime(lastSeen, "%+")\
| table lastSeen, message, host
disabled = 1

[AllSplunkEnterpriseLevel - TCP or SSL Config Issue]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 53 * * * *
description = Chance the alert requires action? High. Since the TCP listener port may not be working as expected you likely wish to run the appropriate checks on the forwarder/indexer
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A TcpInputConfig or SSLCommon error likely indicates a misconfiguration of a heavy forwarder or indexer, this may prevent the listening port from working as expected")`\
index=_internal ERROR "TcpInputConfig" OR "SSLCommon" OR "Could not bind" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) \
| stats first(_time) AS mostRecent by host, source, sourcetype, message\
| table host, message, mostRecent\
| eval mostRecent=strftime(mostRecent, "%+")
disabled = 1

[IndexerLevel - Peer will not return results due to outdated generation]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? High. In general this error should not appear for a long period of time, so if it does there is likely an issue
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If the error Peer ... will not return any results for this search, because the search head is using an outdated generation then either the peer requires a restart or there is another issue here. Assuming the issue does not resolve itself quickly...")`\
index=_internal sourcetype=splunkd `splunkadmins_splunkd_source` `indexerhosts` "because the search head is using an outdated generation"\
| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, values(host) AS host by message\
| eval diff=lastSeen-firstSeen\
| where diff>60\
| eval firstSeen = strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+") \
| table host, message, firstSeen, lastSeen, count
disabled = 1

[SearchHeadLevel - Scheduled searches failing in cluster with 404 error]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 23 * * * *
description = Chance the alert requires action? High. If 404's are occurring within the search head cluster then it is possible there is a member out of sync...
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A 404 error appearing on some search head peers but not others might imply a synchronisation issue within the search head cluster has occurred, this might require correction potentially through a re-sync http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/HowconfrepoworksinSHC#Replication_synchronization_issues ")`\
index=_internal `searchheadhosts` "find saved search with name" sourcetype=splunkd `splunkadmins_splunkd_source`\
| rex field=err "/servicesNS/(?P<username>[^/]+)/(?P<appName>[^/]+)"\
| rex "'(?P<searchname>[^']+)'.$"\
| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, values(username) AS username, values(appName) AS appName, values(searchname) AS searchName by message, peer\
| eval firstSeen = strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")\
| table username, appName, searchName, firstSeen, lastSeen, count, peer, message
disabled = 1

[IndexerLevel - Too many events with the same timestamp]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 44 4 * * *
description = Find excessive numbers of events with the same timestamp so they can be reviewed to see if the data is valid or not
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Too many events with the same timestamp have been found. This may be a sign of poor quality data, or a problematic log file")`\
index=_internal "Too many events" (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_splunkd_source` `splunkadmins_toomany_sametimestamp`\
| cluster showcount=true \
| rex "Too many events \((?P<number>[0-9]+.)"\
| rename data_host AS host, data_sourcetype AS sourcetype, data_source AS source\
| eval invesDataSource = replace(source, "\\\\", "\\\\\\\\"), invesStartTime=floor(_time)\
| eval investigationQuery="`comment(\"You will need to set the time settings manually as the log does not provide the parsed time, only the indexed time the issue occurred at...\")` index=* host=" . host . " sourcetype=\"" . sourcetype . "\" source=\"" . invesDataSource . "\" _index_earliest=" . invesStartTime\
| table host, sourcetype, source, number, cluster_count, message, _time, investigationQuery
disabled = 1

[SearchHeadLevel - Detect MongoDB errors]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 53 * * * *
description = Chance the alert requires action? High. If there are errors in the mongo log files on a search head cluster (unrelated to restarts) then this might indicate a kvstore issue which needs attention
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The main goal of this alert errors which might not appear in splunkd.log but are critical to keeping the kvstore running on the search heads. Please check the mongod.log file for further information, the additional count field is simply determining that mongo is still logging...")`\
`comment("Attempt to find errors in the mongod log and make sure the errors do not relate to shutdown events in the search head cluster. Since this does will ignore any events when either cluster shutsdown it might not be sensitive enough for some use cases...")`\
index=_internal `searchheadhosts` `splunkadmins_mongo_source` (" E " OR " F ") `splunkadmins_mongodb_errors`\
| search `comment("Exclude time periods where shutdowns were occurring")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,60,60)`]\
| eventstats max(_time) AS mostRecent, min(_time) AS firstSeen by host\
| bin _time span=10m \
| stats values(_raw) AS logMessages, max(mostRecent) AS mostRecent, min(firstSeen) AS firstSeen by _time, host \
| search `comment("One final symptom that appears when mongodb is dead is the logging just stops, zero data, however this proved to be tricky in Splunk so the below query uses a few tricks to ensure the data will show zero values even if the server stops reporting. timechart was recommended by splunkanswers as it creates a timebucket with null values if no data is found...")`\
| append \
    [ tstats count where index=_internal `searchheadhosts` `splunkadmins_mongo_source` by host, _time span=5m \
    | timechart limit=0 span=5m sum(count) AS count by host \
    | fillnull \
    | untable _time, host, count \
    | stats max(_time) AS mostRecent, min(_time) AS firstSeen, last(count) AS lastCount by host \
    | where lastCount=0 \
    | eval logMessages="Zero log entries found at this time, mongod might not be running, please investigate" \
    | fields - lastCount] \
| eval mostRecent = strftime(mostRecent, "%+"), firstSeen=strftime(firstSeen, "%+")\
| fields _time, host, firstSeen, mostRecent, logMessages\
| search `comment("Just in case...")` `splunkadmins_mongodb_errors2`
disabled = 1

[IndexerLevel - Cold data location approaching size limits]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 13 7 * * *
description = Chance the alert requires action? High. One or more indexes are approaching the disk limits on their cold data, therefore the buckets will roll to frozen once this limit is reached...
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest `splunkindexerhostsvalue` /services/data/indexes/ \
| search\
    `comment("This search attempts to find indexes which are about to start rolling buckets to frozen due to disk space issues by checking how much percentage of the allocated cold section of disk is used. It does not take into account any volume sizing...")`\
    `comment("This is the more proactive form of IndexerLevel - Buckets are been frozen due to index sizing")` \
| join title splunk_server type=outer \
    [| rest `splunkindexerhostsvalue` /services/data/indexes-extended/] \
| stats values(bucket_dirs.cold.bucket_size) AS currentColdSizeMB, values(bucket_dirs.home.warm_bucket_size) AS currentWarmSizeMB, max(bucket_dirs.home.event_max_time) AS latestTime, min(bucket_dirs.cold.event_min_time) AS earliestTime, min(bucket_dirs.home.event_min_time) AS earliestTimeHot, values(coldPath.maxDataSizeMB) AS coldPathSizeLimitMB, values(currentDBSizeMB) AS hotSizeMB, values(maxTotalDataSizeMB) AS maxTotalDataSizeMB, values(frozenTimePeriodInSecs) AS frozenTimePeriodInSecs, values(maxDataSize) AS maxDataSize, values(homePath.maxDataSizeMB) AS hotPathMaxDataSizeMB by name, splunk_server \
| eval currentColdSizeMB=coalesce(currentColdSizeMB,currentWarmSizeMB), earliestTime=coalesce(earliestTime,earliestTimeHot)\
| eval "Days of data based on epoch values"=round((latestTime-earliestTime)/3600/24) \
| rename name AS index, splunk_server AS indexer\
| search  `comment("Things do get a little bit messy here, if the cold path size is unlimited, the remaining data is the maxTotalSizeMB minus what we have already used in the hot section (not a perfect calculation but close enough for our purposes")` \
| eval coldPathSizeLimitMB=if(coldPathSizeLimitMB=0,if(hotPathMaxDataSizeMB=0,maxTotalDataSizeMB,maxTotalDataSizeMB-hotSizeMB),coldPathSizeLimitMB) \
| eval percUsed=round((currentColdSizeMB/coldPathSizeLimitMB)*100,2) \
| eval frozenTimeInDays=frozenTimePeriodInSecs/60/60/24 \
| eval maxDataSize=case(maxDataSize="auto","750",maxDataSize="auto_high_volume","10240",true(),maxDataSize)\
| eval worstCaseBucketCountLeft=floor((coldPathSizeLimitMB-currentColdSizeMB)/maxDataSize)\
| where percUsed>`splunkadmins_colddata_percused` \
| search `splunkadmins_colddata`\
| table index, indexer, currentColdSizeMB, coldPathSizeLimitMB, percUsed, frozenTimeInDays, "Days of data based on epoch values", worstCaseBucketCountLeft
disabled = 1

[AllSplunkEnterpriseLevel - Unable to dispatch searches due to disk space]
alert.severity = 4
alert.suppress = 1
alert.suppress.period = 4h
alert.track = 1
counttype = number of events
cron_schedule = 32 * * * *
description = Chance the alert requires action? High. Unless the disk space issue clears itself some action will be required either now or to prevent future failures.
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect when the scheduler is unable to run search commands due to a lack of disk space on the filesystem")`\
index=_internal `splunkenterprisehosts` (sourcetype=splunkd `splunkadmins_splunkd_source` "Search not executed: Dispatch Command: The minimum free disk space") OR (sourcetype=scheduler "The minimum free disk space * reached")\
| stats count, min(_time) AS firstSeen, max(_time) AS mostRecent, max(_raw) AS lastExample by host, message\
| eval firstSeen=strftime(firstSeen, "%+"), mostRecent=strftime(mostRecent, "%+")\
| table host, count, firstSeen, mostRecent, lastExample
disabled = 1

[IndexerLevel - Unclean Shutdown - Fsck]
alert.suppress = 0
alert.track = 1
alert_condition = search host=*
counttype = custom
cron_schedule = 4,19,34,49 * * * *
description = Chance the alert requires action? Moderate. One or more indexes are mentioned as corrupt in the log files, this should auto-repair but it may cause errors in the search interface until the repair is complete
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to detect if an indexer crash resulted in corrupt buckets, if so alert the admin so they are aware...")`\
`comment("The indexer is likely going to print the line \"WARN  IndexerService - Indexer was started dirty: splunkd startup may take longer than usual; searches may not be accurate until background fsck completes.\", however we also want to know if buckets were corrupted. In a clustered environment the corrupt buckets should be added to the cluster master fixup list and repaired online, if a non-clustered environment refer to the Splunk fsck documentation. Note that I'm unable to find a log message to advise when the OnlineFsck completes in splunkd.log")`\
`comment("FYI fixup lines in the splunkd log file may look like \"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds.\"")`\
index=_internal sourcetype=splunkd `splunkadmins_splunkd_source` `indexerhosts` "At restart after an unclean shutdown found bucket path" OR ("finished moving hot to warm" caller=init_roll) OR "OnlineFsck - Scheduled repair fsck* kind='entire bucket'"\
| rex field=path "(?P<pathWithoutHot>.*)(/|\\\\)\S+"\
| join type=outer pathWithoutHot \
    [| rest /services/data/indexes `splunkindexerhostsvalue` \
    | fields homePath_expanded, title \
    | rename homePath_expanded AS pathWithoutHot, title AS idxFromREST]\
| eventstats max(_time) AS mostRecent by idx, host\
| bin _time span=5m\
| eval idx=coalesce(idxFromREST, idx)\
| stats count(eval(searchmatch("unclean shutdown"))) AS uncleanCount, count(eval(searchmatch("Scheduled repair fsck"))) AS scheduledRepairCount, count(eval(searchmatch("finished moving hot to warm"))) AS hotToWarmCount, max(mostRecent) AS mostRecent by idx, host, _time\
| where uncleanCount>0\
| append\
    [makeresults 1\
    | eval idx="#The message \"At restart after an unclean shutdown found bucket path...\" results in buckets being rolled/repaired. Users may see errors when running searches, such as \"Failed to read size=2 event(s) from rawdata in bucket=...Rawdata may be corrupt, see search.log. Results may be incomplete!\" (OR) \"idx=_internal Could not read event: cd=(n/a). Results may be incomplete ! (logging only the first such error; enable DEBUG to see the rest).\" This appears to resolve itself in Splunk 7+ when the fsck's complete. I have not determined how to find a completion time..."]\
| fields - _time, uncleanCount\
| sort idx\
| eval mostRecent=strftime(mostRecent, "%+")\
| addcoltotals labelfield=host label="Total Count"
disabled = 1

[AllSplunkEnterpriseLevel - Detect LDAP groups that no longer exist]
alert.severity = 2
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 57 6 * * *
description = Chance the alert requires action? High. An LDAP group is configured in Splunk that does not exist in LDAP, this is a minor issue but it can be fixed by removing it from authentication.conf
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find any LDAP groups that are reporting that they do not exist so can therefore be removed from the Splunk configuration")`\
`comment("This appears to occur only after restarts of the Splunk server, however it is useful to know about as the authentication.conf can be cleaned up once found")`\
index=_internal sourcetype=splunkd `splunkenterprisehosts` `splunkadmins_splunkd_source` "was not found on the LDAP server"\
| stats max(_time) AS mostRecentlySeen, min(_time) AS firstSeen by message, host\
| eval mostRecentlySeen=strftime(mostRecentlySeen, "%+"), firstSeen=strftime(firstSeen, "%+")\
| fields host, mostRecentlySeen, firstSeen, message
disabled = 1

[ClusterMasterLevel - Per index status]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = */5 * * * *
description = Chance the alert requires action? Moderate. Check if the is_searchable flag is set to false *or* detect when an index is not matching the search factor of at least 1 copy between the sites. Changed to 5 min intervals to pass certification you may want to run this more regularly. Cluster master specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins 
request.ui_dispatch_view = search
search = | rest /services/cluster/master/indexes `splunkadmins_clustermaster_host`\
| foreach searchable_copies_tracker.*.actual_copies_per_slot \
    [ eval expectedMatchesActual_<<MATCHSTR>>=if('searchable_copies_tracker.<<MATCHSTR>>.expected_total_per_slot'=='searchable_copies_tracker.<<MATCHSTR>>.actual_copies_per_slot',"true","false") ]\
| fields is_searchable, expectedMatchesActual_*, num_buckets, searchable*, title\
| eval failureCount=0\
| foreach expectedMatchesActual_*\
    [ eval failureCount=if('expectedMatchesActual_<<MATCHSTR>>'=="false",failureCount+1,failureCount) ]\
| where failureCount > `splunkadmins_clustermaster_failurecount` OR is_searchable=0
disabled = 1

[ClusterMasterLevel - Primary bucket count per peer]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. Graph the number of primary buckets for site0 on each peer. Note in an environment with a large number of peers/buckets this query will be very expensive (memory wise) and should be used with caution
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.timeRangePicker.show = 0
display.general.type = visualizations
display.page.search.tab = visualizations
display.statistics.show = 0
display.visualizations.trellis.splitBy = PeerName
request.ui_dispatch_app = SplunkAdmins 
request.ui_dispatch_view = search
search = | rest /services/cluster/master/buckets `splunkadmins_clustermaster_host` \
| search `comment("Note in larger environments this can cause an issue due to the number of peers returning data, so use with caution. The idea for this query comes from Splunk support & https://answers.splunk.com/answers/234717/how-to-get-list-of-buckets-which-are-having-issues.html , attempt to determine the count of primary buckets per peer for site0. This report is designed to provide 1 example of a useful REST endpoint")` standalone=0 frozen=0\
| rename primaries_by_site.site0 AS peerGUID\
| join type=outer peerGUID [ rest /services/cluster/master/peers splunk_server=local\
| fields active_* host* label title status site\
| eval PeerName= site + ":" + label + ":" + host_port_pair\
| rename title AS peerGUID\
| rename site AS peerSite\
| table peerGUID PeerName peerSite]\
| stats count by PeerName\
| chart sum(count) AS count by PeerName

#An overly complex alert due to the difficulty in expanding macros within Splunk programatically
[SearchHeadLevel - Scheduled searches not specifying an index macro version]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 37 6 * * 1-5
description = Chance the alert requires action? High. These searches are either using index=* or not specifying an index at all and relying on the default set of indexes. Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("Look over all scheduled searches and find those not specifying/narrowing down to an index, or using the index=* trick. This version is dealing with those using macros. Please ensure the SearchHeadLevel - Macro report is also enabled for this to work as expected. Attempted to use https://answers.splunk.com/answers/186698/how-can-i-expand-a-macro-definition-in-the-search.html but the intentionsparser does not work via a REST call within Splunk, only by an external call in Splunk 7...")` `splunkadmins_scheduledsearches_without_index_macro`\
| table title , description, eai:acl.app, eai:acl.owner, qualifiedSearch, next_scheduled_time \
| search next_scheduled_time!="" \
| regex qualifiedSearch!=".*index\s*(!?)=\s*([^*]|\*\S+)" \
| regex qualifiedSearch="^\s*search " \
| rex field=qualifiedSearch "^(?P<exampleQueryToDetermineIndexes>[^\|]+)"\
| regex exampleQueryToDetermineIndexes="`" \
| rename eai:acl.owner AS owner, eai:acl.app AS Application \
| fields title, owner, description, Application, qualifiedSearch, next_scheduled_time \
| rex field=qualifiedSearch max_match=5 "\`(?!\")(?!')(?P<macro>[^\`]+)\`" \
| mvexpand macro \
| dedup title, macro \
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)"\
| rex max_match=10 field=macro "(?P<commas2>,)"\
| rex max_match=1 field=macro "(?P<match>[^\(]+\()"\
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")`\
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null()))\
| eval argCount=if(isnull(argCount2),0,argCount2+1)\
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount)\
| rex field=macro "(?P<macro>^[^\( ]+)"\
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")") \
| lookup splunkadmins_macros title AS macroName \
| eval qualifiedSearch=replace(qualifiedSearch, macro, definition) \
| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . "| stats values(index) AS index | format | fields search | eval search=replace(search,\"\\)\",\"\"), search=replace(search,\"\\(\",\"\"), search=if(search==\"NOT \",\"No indexes found\",search)"\
| fields - argCount2, commas, argCount, eai:acl.app, macro, macroName, match \
| stats values(qualifiedSearch) AS qualifiedSearch, first(exampleQueryToDetermineIndexes) AS exampleQueryToDetermineIndexes by title, owner, description, Application, next_scheduled_time \
| nomv qualifiedSearch \
| regex qualifiedSearch!=".*index\s*(!?)=\s*([^*]|\*\S+)"
disabled = 1

#Enable scheduling on this report if you use SearchHeadLevel - Scheduled searches not specifying an index macro version or SearchHeadLevel - User - Dashboards searching all indexes macro version
[SearchHeadLevel - Macro report]
action.email.useNSSubject = 1
alert.track = 0
cron_schedule = 50 5 * * 1-5
description = Report only? Yes. This report is required to support SearchHeadLevel - Scheduled searches not specifying an index macro version AND SearchHeadLevel - User - Dashboards searching all indexes macro version. Search Head specific? Yes
dispatch.earliest_time = @d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
display.visualizations.show = 0
enableSched = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
schedule_window = 30
search = | rest "/servicesNS/-/-/configs/conf-macros?count=-1" splunk_server=local\
| fields title, eai:acl.app, definition \
| fields - _raw \
| dedup title \
| outputlookup splunkadmins_macros

[AllSplunkEnterpriseLevel - Non-existent roles are assigned to users]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 57 6 * * 1-5
description = Chance the alert requires action? High. This particular alert is harmless but can cause some very strange results if not resolved, the fix is documented within the alert
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to find where there are deleted roles assigned to users, this should only happen when the user was created with the Splunk authentication system. The fix is to open the user in the settings menu and find any user with the mentioned role, and then to save the user with no changes, this will wipe the non-existent roles from the user")`\
index=_internal sourcetype=splunkd `splunkenterprisehosts` `splunkadmins_splunkd_source` "AuthorizationManager - Unknown role"\
| stats max(_time) AS lastSeen, first(_raw) AS rawMessage by message\
| eval actionToTake="Find any users in the settings menu with the mentioned role and save them without changes to remove the role"\
| eval lastSeen = strftime(lastSeen, "%+")\
| table lastSeen, message, rawMessage, actionToTake
disabled = 1

[IndexerLevel - Index not defined]
alert.severity = 4
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 33 * * * *
description = Chance the alert requires action? High. Either the remote forwarder is sending to the wrong index name or the index has not been defined, either way the data will be rejected by the indexer
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect if data is been sent to the indexers to an index which is not yet configured")`\
index=_internal "Received event for unconfigured" sourcetype=splunkd `splunkadmins_splunkd_source` "IndexerService - Received event for unconfigured" `indexerhosts`\
| rex "index=(?P<index>[^ ]+).*source=\"source::(?P<source>[^\"]+)\" host=\"host::(?P<host>[^\"]+)"\
| stats min(_time) AS firstSeen, max(_time) AS lastSeen, values(source) AS sourceList, values(host) AS hostsSendingToThisIndex, first(_raw) AS message by index\
| eval sources=mvjoin(sources, ", "), firstSeen=strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")
disabled = 1

[SearchHeadLevel - Saved Searches with privileged owners and excessive write perms]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 56 5 * * *
description = This is a rudimentary way of detecting scheduled searches or reports that could be used by a non-privileged user to run the alert/report as a privileged user through search scheduling functionality. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches splunk_server=local \
| fields title, eai:acl.sharing, eai:acl.perms.read, eai:acl.perms.write, description, disabled, eai:acl.owner, dispatchAs, eai:acl.app \
| search `comment("Find alerts that the owner is set to a user (not nobody), and the sharing is non-private, and finally the owner has an admin or power role")` eai:acl.owner!="nobody" eai:acl.sharing!="user" dispatchAs=owner \
    [| rest /services/authentication/users `searchheadsplunkservers` \
    | search roles=admin OR roles=power \
    | fields title \
    | rename title AS eai:acl.owner] \
| eval writeCount=mvcount('eai:acl.perms.write') \
| eval writePerms=mvjoin('eai:acl.perms.write', ",") \
| search `comment("Exclude by macro")` `splunkadmins_privilegedowners` \
| search `comment("If only the admin or power role can write to the alert then it's no problem...")` NOT (writeCount=1 (eai:acl.perms.write="admin" OR eai:acl.perms.write="power")) \
| search `comment("power users have admin-like abilities, these users have a similar level of read access so less of a security concern...")` writePerms!="admin,power" \
| search `comment("If the alert is coming from an application that only admins can see to I'm not concerned as the user should not be able to access the app to edit the search...(in theory). We also ignore if there are no read permissions at all...")` \
    NOT ( \
    [| rest /services/apps/local splunk_server=local \
    | fields title, visible, eai:acl.perms.read \
    | search `comment("If we cannot access the application then I'm assuming making it visible does not matter...")` visible=1 \
    | eval readCount=mvcount('eai:acl.perms.read') \
    | search `comment("If the application can only be written to by admin or power users, then we can safely ignore the alerts within it...")` (readCount=1 (eai:acl.perms.read="admin" OR eai:acl.perms.read="power") ) \
    | fields title \
    | rename title AS eai:acl.app]) \
| where isnotnull('eai:acl.perms.write') \
| rename title AS "Alert Name", eai:acl.perms.read AS "read perms", eai:acl.perms.write AS "write perms", eai:acl.app AS app, eai:acl.owner AS owner \
| sort app, "Alert Name" \
| table app, "Alert Name", description, disabled, "read perms", "write perms", owner
disabled = 1

[IndexerLevel - Search Failures]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 48 6 * * *
description = Chance the alert requires action? Moderate. One or more search jobs are failing to run for some reason, this may require investigation. The only issue so far has been around search factory/unknown search command but this search is generic just in case a new issue appears...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to detect search failures of interest, such as Search Factory: Unknown search command 'base64' so they can be fixed before it becomes an issue for multiple users. The search is generic to attempt to detect any new errors...")`\
index=_internal `indexerhosts` sourcetype=splunkd_remote_searches " ERROR StreamedSearch " OR " WARN StreamedSearch " NOT " INFO  StreamedSearch - " NOT "Connection closed by peer" NOT "Search process did not exit cleanly" NOT "Connection reset by peer" NOT "Broken pipe" NOT "Failed to start the search process."\
| search `comment("Exclude the indexer shutdown times")` NOT [`splunkadmins_shutdown_list(indexerhosts,30,30)`]\
| rex "^.*sid=[^,]+, (?P<message>.*)" \
| search `comment("Allow exclusions via macro...")` `splunkadmins_searchfailures`\
| stats count, min(_time) AS firstSeen, max(_time) AS mostRecent, first(_raw) AS exampleMessage, values(host) AS hostList by message \
| eval firstSeen=strftime(firstSeen, "%+"), mostRecent=strftime(mostRecent, "%+")
disabled = 1

[SearchHeadLevel - User - Dashboards searching all indexes macro version]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 32 6 * * 1-5
description = Chance the alert requires action? High. All dashboard panels that are using a macro and do not have an index= setting or use index=* are highlighted by this alert. Can be fixed by the end user? Yes. Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/data/ui/views \
| search `comment("A dashboard searching all indexes is an issue just like a scheduled search querying all indexes or using the index=* trick. This version is dealing with those using macros. Please ensure the SearchHeadLevel - Macro report is also enabled for this to work as expected.")` \
    eai:data=*query* NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app) NOT (eai:appName=splunk_app_aws eai:acl.sharing=app) \
| regex eai:data="<search.*" \
| mvexpand theSearch \
| rex field=eai:data "(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\/query>" max_match=200 \
| mvexpand theSearch \
| rex field=theSearch "<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)" \
| search `comment("If we are seeing post process search then we don't want to check if it has index= because that is likely only in the base query. These are also various exclusions for legitimate searches that will not involve scanning all indexes, such as rest or a savedsearch or similar")` searchInfo!="*base*" \
| rename eai:appName AS application, eai:acl.sharing AS sharing, eai:acl.owner AS identity, label AS name \
| table theQuery, application, identity, sharing, name, splunk_server, title \
| regex theQuery!="index\s*=(?!\s*\*)" \
| regex theQuery!="^(\()?\s*(\`|\$[^|]+\$|eventtype=|rest |<!\[CDATA\[\s*\|\s*((acl)?inputlookup|rest) |\|)" \
| rex field=theQuery "^(?P<exampleQueryToDetermineIndexes>[^\|]+)" \
| regex exampleQueryToDetermineIndexes="\`"\
| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . "| stats values(index) AS index | format | fields search | eval search=replace(search,\"\\)\",\"\"), search=replace(search,\"\\(\",\"\"), search=if(search==\"NOT \",\"No indexes found\",search)" \
| rex field=theQuery max_match=5 "\`(?!\")(?!')(?P<macro>[^\`]+)\`"\
| eval theQueryBackup=theQuery\
| mvexpand macro \
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)"\
| rex max_match=10 field=macro "(?P<commas2>,)"\
| rex max_match=1 field=macro "(?P<match>[^\(]+\()"\
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")`\
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null()))\
| eval argCount=if(isnull(argCount2),0,argCount2+1)\
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount)\
| rex field=macro "(?P<macro>^[^\( ]+)"\
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")") \
| lookup splunkadmins_macros title AS macroName \
| eval theQuery=replace(theQuery, macro, definition)\
| stats values(theQuery) AS theQuery, first(exampleQueryToDetermineIndexes) AS exampleQueryToDetermineIndexes by theQueryBackup, application, identity, sharing, name, title, splunk_server\
| table theQuery, application, identity, sharing, name, splunk_server, title, exampleQueryToDetermineIndexes\
| nomv theQuery\
| regex theQuery!="index\s*=(?!\s*\*)"
disabled = 1

[SearchHeadLevel - Captain Switchover Occurring]
alert.suppress = 1
alert.suppress.period = 120m
alert.track = 1
counttype = number of events
cron_schedule = 37 * * * *
description = Chance the alert requires action? Moderate. If the captain has been changed then scheduled searches and alerts will be paused during the switchover, if this is not part of a restart then something is likely wrong...ironically this alert will not run if the issue is actually occurring so that's why the time range window is thee times the runtime of the alert in case it is missed once or twice...
dispatch.earliest_time = -180m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = index=_internal `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` "SHCRaftConsensus" NOT "failed appendEntriesRequest err" NOT "SHCRaftConsensus - NOT_LEADER" `splunkadmins_captain_switchover`\
| search `comment("Exclude the search head shutdown times")` NOT [`splunkadmins_shutdown_time(searchheadhosts,20,120)`] `comment("Exclude manual transfer")` NOT [`splunkadmins_transfer_captain_times(searchheadhosts,20,120)`]\
| cluster showcount=true\
| fields _time, cluster_count, _raw\
| sort - _time
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Servers with resource starvation]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 13 */2 * * *
description = Chance the alert requires action? Moderate. Detect when a Splunk enterprise host is reporting that it is seeing excessive response times while running operations
dispatch.earliest_time = -120m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to find entries in the splunkd logs that indiciate that Splunk is resource constrained and requires more CPU or similar")`\
index=_internal `splunkenterprisehosts`  sourcetype=splunkd `splunkadmins_splunkd_source` "Might indicate hardware or splunk limitations" OR "took longer than seems reasonable" NOT "Might indicate slow ldap server."\
| rex "^[\d-]+ [\d:\.]+( )+\+?\d+( )+[^ ]+( )+(?P<componentAndArea>([^ ]+( )+){3}).*\((?P<number>\d+) milliseconds"\
| stats count, avg(number) AS avgTimeInSeconds, max(number) AS maxTimeInSeconds, max(_time) AS mostRecent, min(_time) AS firstSeen by componentAndArea, host\
| search `comment("Allow custom exclusions")` `splunkadmins_resource_starvation`\
| sort - mostRecent\
| eval firstSeen=strftime(firstSeen, "%+"), mostRecent=strftime(mostRecent, "%+"), avgTimeInSeconds=round(avgTimeInSeconds/1000,2), maxTimeInSeconds=round(maxTimeInSeconds/1000,2)
disabled = 1

[IndexerLevel - S2SFileReceiver Error]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 36 */3 * * *
description = Chance the alert requires action? Moderate. One or more indexing peers are having issues with receiving file replications and may require investigation
dispatch.earliest_time = -3h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("An attempt to detect excessive numbers of S2SFileReceiver / TcpInputProc failures on the indexing tier, these may indicate an issue. We are only looking for errors about replication data")`\
`comment("An indexer peer that was constantly logging \"S2SFileReceiver...type=data_model already exists!\" / \"S2SFileReceiver...type=report_acceleration already exists!\" has been fixed by restart (once so far)")`\
index=_internal sourcetype=splunkd `indexerhosts` sourcetype=splunkd `splunkadmins_splunkd_source` "ERROR TcpInputProc - event=replicationData" OR "ERROR S2SFileReceiver - error alerting slave about" OR "ERROR S2SFileReceiver - error adding new summary replica to slave"\
| rex "(?P<error>ERROR [^-]+- [^=]+=)(?P<postEquals>[^ ]+) (?P<postEquals2>[^=]+=[^ ]+)"\
| rex field=err "(?P<type>type=.*)"\
| eval error=if(postEquals=="onFileAborted" OR postEquals=="replicationData",error . " " . postEquals . " " . postEquals2,error . " " . type)\
| stats count, max(_time) AS mostRecent by host, error\
| search `comment("Allow exclusion via macro")` `splunkadmins_s2sfilereceiver`\
| eval mostRecent=strftime(mostRecent, "%+")
disabled = 1

[SearchHeadLevel - Disabled modular inputs are running]
action.keyindicator.invert = 0
action.makestreams.param.verbose = 0
action.nbtstat.param.verbose = 0
action.notable.param.verbose = 0
action.nslookup.param.verbose = 0
action.ping.param.verbose = 0
action.risk.param.verbose = 0
action.threat_add.param.verbose = 0
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 39 5 * * *
description = Chance the alert requires action? Moderate. While the modular input is disabled it appears to be running according to the introspection logs. The solution is to remove the inputs.conf and inputs.conf.spec from the relevant app, or at least remove the non-used modular inputs. Note this alert is only relevant to the search head/cluster it is running on and this is *not* an issue as such from the Splunk support point of view, however the python scripts can impact server performance... Search Head specific? Yes
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
display.visualizations.type = mapping
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins 
request.ui_dispatch_view = search
search = `comment("Attempt to detect when a Splunk scripted_input appears to be running at OS level even though disabled=1 is set")`\
`comment("Splunk support have advised that the modular input scripts run by design, in fact disabled=1 means the inheriting input stanzas should be disabled, not that the modular input is disabled")`\
`comment("As per the updated documentation on https://docs.splunk.com/Documentation/AddOns/released/Overview/Distributedinstall best practice is now to remove the inputs.conf and inputs.conf.spec on SH clusters...")`\
index=_introspection `localsearchheadhosts` sourcetype=splunk_resource_usage \
| spath component \
| search component=PerProcess data.process!=splunkd \
| spath data.args \
| rex field=data.args "[/\\\\](?P<title>[^/\\\\\.]+)\.[^\.]+" \
| join title overwrite=false\
    [| rest splunk_server=local /servicesNS/-/-/configs/conf-inputs \
    | search title!="*://*" disabled=1 \
    | table eai:acl.app, disabled, interval, title] \
| rename eai:acl.app AS app \
| stats max(_time) AS mostRecent, min(_time) AS firstSeen, values(host) AS hostList, values(data.status) AS status by app, interval, title, data.args, data.process, data.process_type \
| eval mostRecent=strftime(mostRecent, "%+"), firstSeen=strftime(firstSeen, "%+") \
| table title, app, firstSeen, mostRecent, hostList, interval, data.args, data.process, data.process_type, status
disabled = 1

[ForwarderLevel - Forwarders connecting to a single endpoint for extended periods]
action.email.useNSSubject = 1
alert.track = 0
description = Experimental alert (report for now). This detects when a forwarder spent an extended period of time connecting to a single endpoint, this suggests that the EVENT_BREAKER (on a HF or a LINE_BREAKER on a HF) may assist in forcing the connection to switch between endpoints more regularly.\
Note this there are multiple variables that change this query depending on your environment setup.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect forwarders stuck connecting to a single indexer or heavy forwarder for an extended period of time...Assuming more than 60 seconds of continuous traffic is a problem...this may need to be customised for your environment")`\
`comment("Note that the number of metrics defaults to the top 10 measured every 30 seconds, so if this is customised you will need to change this alert")`\
index=_internal sourcetype=splunkd source=*metrics.log sourcetype=splunkd group=tcpin_connections Metrics `indexerhosts` OR `heavyforwarderhosts`\
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, "none")\
| streamstats time_window=60s count by hostname, host, ingest_pipe\
| where count>4\
| eval combined=hostname . " host:" . host . " pipe:" . ingest_pipe\
| timechart span=10m useother=false max(count) by combined

[ForwarderLevel - Forwarders connecting to a single endpoint for extended periods UF level]
action.email.useNSSubject = 1
alert.track = 0
description = Experimental alert (report for now). This detects when a forwarder spent an extended period of time connecting to a single endpoint, this suggests that the EVENT_BREAKER may assist in forcing the connection to switch between endpoints more regularly.\
Note this there are multiple variables that change this query depending on your environment setup.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect forwarders stuck connecting to a single indexer or heavy forwarder for an extended period of time...Assuming more than 60 seconds of continuous traffic is a problem...this may need to be customised for your environment")`\
`comment("Note that the number of metrics defaults to the top 10 measured every 30 seconds, so if this is customised you will need to change this alert")`\
index=_internal sourcetype=splunkd source=*metrics.log sourcetype=splunkd group=tcpin_connections Metrics `indexerhosts` OR `heavyforwarderhosts`\
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, "none")\
| streamstats time_window=60s count by name, host, ingest_pipe\
| where count>4\
| eval combined=name . " host:" . host . " pipe:" . ingest_pipe\
| timechart span=10m useother=false max(count) by combined

[SearchHeadLevel - Determine query scan density]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This query measures the approx density of a search to determine if it's considered rare or dense.\
This version provides an example query which can then be used to drill down into further details as required
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Determine the query scan density of queries per-index. Excludes replicated search jobs (rsa)")` index=_audit `searchheadhosts` action=search sourcetype=audittrail search_id!="rsa_*"\
| eval sname=if(isnull(savedsearch_name) OR savedsearch_name=="", search, savedsearch_name)\
| stats list(search_type) as search_type, list(api_et) as api_et, list(api_lt) as api_lt, list(apiStartTime) as apiStartTime, list(apiEndTime) as apiEndTime,list(search_et) as search_et, list(search_lt) as search_lt, list(info) as status, list(total_run_time) as total_run_time list(event_count) as event_count,list(considered_events) as considered_events, list(result_count) as result_count, list(scan_count) as scan_count, list(ttl) as ttl, list(is_realtime) as search_realtime_check, list(_time) as TimeAudited, list(sname) as sname by search_id\
| where isnotnull(scan_count) AND NOT event_count="N/A" AND LIKE(sname, "%index%=%")\
| search sname!="'typeahead prefix=*"\
| rex field=sname "index\s*(=|::)\s*(?P<indexname>[^ \t]+)"\
| eval indexname=replace(indexname, "'", ""), indexname=replace(indexname, "\"", "")\
| stats avg(event_count) AS avgEventCount, avg(scan_count) AS avgScanCount, avg(result_count) AS avgResultCount by indexname\
| eval scanDensity=(avgResultCount/avgScanCount)*100 

[IndexerLevel - Report on bucket corruption]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. Refer to IndexerLevel - Unclean Shutdown - Fsck for an alert for this issue, this just lists out all corrupt buckets.Bucket corruption is rare and in a clustered environment this should self-repair via the cluster master fixup list over time. If non-clustered refer to the documentation for splunk fsck in a non-clustered environment
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("For the alert version of this report refer to IndexerLevel - Unclean Shutdown - Fsck")`\
`comment("Attempt to find bucket corruption errors in the splunkd logs. this can also be found at search head level via the info.csv (not indexed by default). In a clustered environment a message such as \"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds\" may appear once the auto-repair has occurred")`\
index=_internal `indexerhosts` sourcetype=splunkd `splunkadmins_splunkd_source` IndexerService OR HotBucketRoller "corrupt" newly `comment("newly appears to show corruption, previously may be the term for when it is fixed...")`\
| stats values(Bucket) AS bucketList by idx \
| eval bucketCount=mvcount(bucketList)\
| addcoltotals
display.events.fields = ["host","source","sourcetype"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = line

[SearchHeadLevel - Indexer Peer Connection Failures]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 53 * * * *
description = Chance the alert requires action? Moderate. For some reason one or more peers are failing to respond to the search heads, which may impact search results. Any failure will be reporting an error either to the end user or to a scheduled search. Note this alert requires the search.log and the info.csv to be indexed
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect failures from the search.log advising that the peer was unable to send a response, for example This can be caused by the peer unexpectedly closing or resetting the connection. Search results might be incomplete!...This requires the search.log to be indexed refer to props.conf in this app")`\
`comment("info.csv often reports failures as well but sometimes these are not in search.log and vice-versa. Unable to distribute to peer/Connection failed/Unable to determine response all appear to be some kind of failure. For the append to work as expected index the info.csv file")`\
index=_internal sourcetype=splunk:searchlog source!="*rsa_scheduler_*" `searchheadhosts` ("error" "for peer") OR "Error connecting" OR "Got status" `comment("Ignoring \"HTTP error status message from\" OR \"HTTP client error\" as they tend to appear when one of the previous examples is there...")`\
| rex "for peer (?P<peer>[^\.]+)"\
| rex "ERROR\s+\S+\s+-\s+(sid:[^ ]+)?(?P<message>.*)"\
| bin _time span=1m\
| eval msgpeer = host + source + peer + _time\
| rex field=host "(?P<host>[^\.]+)"\
| stats dc(msgpeer) AS count, dc(eval(searchmatch("source=*scheduler_*"))) AS schedulerCount, values(host) AS reportingHost, values(message) AS message by peer, _time\
| eval errorFrom="search.log"\
| append\
    [ search index=_internal sourcetype=splunk:search:info source!="*rsa_scheduler_*" `searchheadhosts` "Connection failed" OR "Unable to determine response" OR "Unable to distribute to peer"\
    | rex ",\"(\[[^\]]+\]\[[^\]]+\]: )?\[(?P<peer>[^\.\]]+).*?\] (?P<message>[^\"]+)"\
    | rex "Unable to distribute to peer named .* (?P<message>because.*?)\","\
    | rex field=uri "(?P<IP>[^:]+)"\
    | lookup dnslookup clientip as IP OUTPUT clienthost AS peer\
    | rex field=peer "(?P<peer>[^\.]+)"\
    | bin _time span=1m\
    | eval msgpeer = host + source + peer + _time\
    | rex field=host "(?P<host>[^\.]+)"\
    | stats dc(msgpeer) AS count, dc(eval(searchmatch("source=*scheduler_*"))) AS schedulerCount, values(host) AS reportingHost, values(message) AS message by peer, _time\
    | eval errorFrom="info.csv"\
        ]\
| stats sum(count) AS count, sum(schedulerCount) AS schedulerCount, values(reportingHost) AS reportingHost, values(message) AS message, values(errorFrom) AS errorFrom by peer, _time\
| eval countAndSchedulerCount = count . " / " . schedulerCount\
| table _time, peer, reportingHost, countAndSchedulerCount, message, errorFrom\
| sort - _time
disabled = 1

[SearchHeadLevel - Detect searches hitting corrupt buckets]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This query checks for searches that have found a corrupt bucket in the environment, this does require the info.csv (not logged by default) to be indexed
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Note this requires info.csv to be indexed (not indexed by default)")`\
`comment("Attempt to find corrupt buckets appearing in the search heads dispatch/info.csv files, this will show that a user is seeing the \"data may be corrupt\" messages")`\
`comment("In a clustered environment this should auto-repair via the cluster master fixup list, messages such as \"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds.\" should appear in the splunkd logs. In a non-clustered environment refer to the Splunk fsck documentation")`\
index=_internal sourcetype=splunk:search:info "may be corrupt" OR "could be corrupted" `searchheadhosts`\
| rex ",\"(\[[^\]]+\]\[[^\]]+\]: )?\[(?P<peer>[^\]]+)"\
| rex field=path "'(?P<diskloc>[^\.]+)"\
| stats values(host) AS reportingHost, max(_time) AS mostRecent by bucket, diskloc, peer\
| sort - mostRecent\
| eval mostRecent=strftime(mostRecent, "%+")\
| table mostRecent, diskloc, peer, reportingHost, bucket

[SearchHeadLevel - Users exceeding the disk quota introspection]
action.email.useNSSubject = 1
alert.track = 0
cron_schedule = 11 6,10,14,18,22,2 * * *
description = Chance the alert requires action? High. One or more users have reached the disk quota limit and may not be aware of this... Can be fixed by the end user? Yes. This version requires sendresults and customisation to work as expected.\
Also refer to SearchHeadLevel - Users exceeding the disk quota introspection cleanup for the lookup cleaner. For testing you may wish to hardcode the email_to field and remove all lines after the line starting with table app...
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
schedule_window = 30
search = `comment("The listed users have reached the maximum disk quota, they may be unaware so it is best to let them know about this issue...")`\
`comment("Note that the REST API call accesses the jobs list which can expire for ad-hoc jobs in 10 minutes, the introspection index has data for a longer period of time however it's not as accurate as the rest version. This alert also outputs the emailed users to a lookup so they don't continue to receive this same email without some kind of throttle per-user")`\
`comment("The REST version is called SearchHeadLevel - Users exceeding the disk quota and is search head specific")`\
index=_internal sourcetype=splunkd `splunkenterprisehosts` (`splunkadmins_splunkd_source`) "maximum disk usage quota" `splunkadmins_users_exceeding_diskquota` NOT [| inputlookup splunkusersexceedingdiskquota.csv | fields username ]\
| stats max(_time) AS mostRecent by username, reason, host\
| rename username AS username_from_search\
| eval mostRecent = strftime(mostRecent, "%+")\
| search `comment("For each result we find we're going to run the map command to send an email to each individual user who has had the issue, if they have been emailed before the inputlookup will exclude them. Username renamed due to issues with username variable in a scheduled search")`\
| eval body="Why am I receiving this? <br />" + reason + "<br /><br /> This occurred on host " + host + "<br /><br />The issue was last noticed on " + mostRecent + "<br /><br />" + "Your top 20 searches are listed below" + "<br /><br />"\
| search `comment("The below is the attempt to include the largest jobs by querying the introspection index. If we use map without the appendpipe we lose parts of the original search we need. The initial workaround of makeresults and eval commands did work but this seemed slightly cleaner. Although there would be other ways to do this...")`\
| head 30\
| append [ | makeresults | eval username_from_search="workaround for map errors", body="to pass appinspect" ]\
| appendpipe\
    [| map\
        [ search `comment("The intropsection data provides a written_mb field which is not going to advise an accurate real-disk usage for a job, but it does provide an estimate and provides data for longer than a 10 minute time period for ad-hoc jobs...the | rest version is the alternative available which is more accurate but may return zero results if this is not run at least every 10 minutes, it also must run on the same search head cluster as the disk usage quota, unlike this introspection version. max(data.written_mb) was used instead of last() as sometimes the quota kicks in before the temporary files are removed.")`\
            index=_introspection "data.search_props.role"=head data.written_mb>1 sourcetype=splunk_resource_usage "data.search_props.user"=$username_from_search$\
        | stats max(data.written_mb) AS MBwritten, last(data.elapsed) AS approxDuration, max(_time) AS searchLatestTime, min(_time) AS searchEarliestTime by "data.search_props.app", "data.search_props.provenance", "data.search_props.type", "data.search_props.sid"\
        | stats count, sum(MBwritten) AS MBwritten, max(approxDuration) AS approxDuration, max(searchLatestTime) AS searchLatestTime, min(searchEarliestTime) AS searchEarliestTime by "data.search_props.app", "data.search_props.provenance", "data.search_props.type"\
        | eval searchLatestTime=strftime(searchLatestTime, "%+"), searchEarliestTime=strftime(searchEarliestTime, "%+")\
        | rename data.search_props.app AS app, data.search_props.provenance AS dashboardURLorSearchName, data.search_props.type AS type\
        | sort - MBwritten\
        | eval approxDuration=substr(tostring(approxDuration,"duration"),0,8)\
        | appendcols\
            [ search `comment("At this point you need to either lookup the username to email translation, here's an example using ldapsearch: ldapsearch search=\"(&(CN=$username_from_search$)(objectClass=organizationalPerson))\" attrs=mail | fields mail")` ]\
        | eval email_to=mail\
        | fields - mail\
        | search `comment('Remove search/comment and replace <EQ> with equals to use sendresults to make this fully automated. AppInspect badge does not allow dependencies. sendresults subject<EQ>"Splunk Disk Quota Exceeded" body<EQ>$body$ msgstyle<EQ>"table {font-family:Arial;font-size:12px;border: 1px solid black;padding:3px}th {background-color:#AAAAAA;color:#fff;border-left: solid 1px #e9e9e9} td {border:solid 1px #e9e9e9}" showemail<EQ>f')`\
        | head 20 ] maxsearches=30\
        ]\
| where username_from_search!="workaround for map errors"\
| table app, MBwritten, dashboardURLorSearchName, approxDuration, searchEarliestTime, searchLatestTime, username_from_search, type, count, email_to\
| fields username_from_search \
| rename username_from_search AS username\
| eval currtime=now()\
| fields currtime, username\
| where isnotnull(username)\
| outputlookup splunkusersexceedingdiskquota.csv append=true

[SearchHeadLevel - Users exceeding the disk quota introspection cleanup]
action.email.useNSSubject = 1
alert.track = 0
cron_schedule = 53 4,10,16,22 * * *
description = Relates to the alert SearchHeadLevel - Users exceeding the disk quota introspection\
This report cleans up the lookup file created by the disk quota alert so that users will re-receive the alert after a period of time. Hardcoded to 1 week for now
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
schedule_window = 30
search = | inputlookup splunkusersexceedingdiskquota.csv \
| where currtime > now() - (7*60*60*24) \
| outputlookup splunkusersexceedingdiskquota.csv

[IndexerLevel - Timestamp parsing issues combined alert]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 44 4 * * *
description = Chance the alert requires action? High. Find timestamp parsing issues and provide a report on the issues. Also refer to Mark Runal's blog for the original query or his app via https://splunkbase.splunk.com/app/1848/
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("As found on https://runals.blogspot.com/2014/05/splunk-dateparserverbose-logs-part-2.html with minor modifications. Further queries available in the app https://splunkbase.splunk.com/app/1848/")`\
index=_internal DateParserVerbose `heavyforwarderhosts` OR `indexerhosts` sourcetype=splunkd source=*splunkd.log\
| rex "source::(?<Source>[^\|]+)\|host::(?<Host>[^\|]+)\|(?<Sourcetype>[^\|]+)"\
| rex "(?<msgs_suppressed>\d+) similar messages suppressed."\
| eval Issue = case(like(_raw, "%too far away from the previous event's time%"), "Variability in date/event timestamp", like(_raw, "%suspiciously far away from the previous event's time%"), "Variability in date/event timestamp", like(_raw, "%outside of the acceptable time window%"), "Timestamp is too far outside acceptable time window", like(_raw, "%Failed to parse timestamp%"), "Reverting to last known good timestamp", like(_raw, "%Accepted time format has changed%"), "Attempting to learn new timestamp format", like(_raw, "%The same timestamp has been used%"), "More than 100k+ events have the same timestamp", 1=1, "fixme")\
| stats count sum(msgs_suppressed) as "Duplicate Messages Suppressed" by Sourcetype Issue Host Source\
| stats sum(count) as count dc(Host) as Hosts, dc(Source) as Sources, sum("Duplicate Messages Suppressed") as "Duplicate Messages Suppressed" by Sourcetype Issue\
| eval "Total Count"='Duplicate Messages Suppressed' + count\
| stats sum("Total Count") as "Total Count", list(Issue) as Issues, list(Hosts) as Hosts, list(Sources) as Sources, list("Duplicate Messages Suppressed") as "Duplicate Messages Suppressed" by Sourcetype\
| sort - "Total Count"
disabled = 1

[SearchHeadLevel - Audit log search example only]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This is just an example for querying the audit logs provided for reference only
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Query the audit logs for information about earliest/latest time and the search used. However the issue appears to be that apiStart/EndTime can be overriden by earliest/latest keywords *and* the auto-extracted search field isn't accurate and savedsearch_name of search<number> is actually a dashboards (which can be seen through introspection but not through audit searches!")`\
index=_audit `searchheadhosts` action=search info=granted search=* NOT "search='typeahead prefix"\
| rex "(?m)search='(?P<thesearch>[\S\s]+)',\s+autojoin="\
| table _time, ttl, user, apiStartTime, apiEndTime, earliest, latest, savedsearch_name, thesearch\
| sort - _time

[IndexerLevel - Buckets changes per day]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. Attempt to count the number of buckets added/removed within an indexer cluster in order to forecast potential capacity issues with the cluster master 
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("May not be 100% accurate, still under testing. deleteBucket frozen=false is usually excess bucket removal")`\
index=_internal "Creating hot bucket" OR ("CMSlave - deleteBucket" AND "frozen=false") OR "freeze succeeded" sourcetype=splunkd source=*splunkd.log `indexerhosts`\
`comment("Multiplying by replication factor as each hot bucket is duplicated")`\
`comment("To split by index\
| rex "(/[^/]+){2}/(?P<idx2>[^/]+)"\
| eval idx=if(isnull(idx),idx2,idx)\
")`\
| timechart span=1d count(eval(searchmatch("Creating hot bucket"))) AS created, count(eval(searchmatch("(\"CMSlave - deleteBucket\" AND \"frozen=false\") OR \"freeze succeeded\""))) AS frozenCount\
| eval change=(created*`splunkadmins_replicationfactor`)-frozenCount\
| fields - created, frozenCount

[What Access Do I Have Without REST?]
action.keyindicator.invert = 0
alert.track = 0
description = Report only? Yes. Determine the access of the currently logged in user assuming they cannot run REST queries against the indexers. Search Head specific? Yes. Please open in search and re-execute to make this work...
dispatch.earliest_time = @d
dispatch.latest_time = now
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = Global
request.ui_dispatch_view = search
search = | rest /services/authentication/users splunk_server=local\
| search `comment("REST query is limited to the current search head this is running on. If users have the dispatch REST to indexers capability then ise the 'What Access Do I Have' version' for more detail")`\
    [| rest /services/authentication/current-context/context splunk_server=local\
    | head 1 \
    | fields username \
    | rename username AS title] \
| table title roles | rename title as user | mvexpand roles\
| join type=left roles \
    [rest /services/authorization/roles splunk_server=local\
    | table title srchIndexesAllowed srchIndexesDefault | rename title as roles]\
| makemv srchIndexesAllowed tokenizer=(\S+) | makemv srchIndexesDefault tokenizer=(\S+)\
| eval indexes= [ | eventcount summarize=false index=* index=_* | stats values(index) AS indexes | eval theindexes="\"" . mvjoin(indexes, " ") . "\"" | return $theindexes ]\
| makemv indexes\
| stats values(roles) AS roles, values(indexes) AS indexes, values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user


[SearchHeadLevel - Users with auto-finalized searches]
action.keyindicator.invert = 0
alert.track = 0
description = Report only? Yes. Determine who has had a search auto-finalized due to time or disk quota. This does require the info.csv (not logged by default) to be indexed
dispatch.earliest_time = @d
dispatch.latest_time = now
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = Global
request.ui_dispatch_view = search
search = `comment("Find searches which have been auto-finalized and the search contents which was running when the search was auto-finalized. Very similar to Users Exceeding the disk quota however this covers both disk quota and srchMaxTime")`\
index=_internal `searchheadhosts` sourcetype=splunk:search:info auto-finalized\
| rex field=source "[/\\\]dispatch[/\\\](?P<sid>[^/\\\]+)"\
| rex "(?P<message>auto-finalized[^\"]+)"\
| append [ | makeresults | eval sid="workaround for map errors", message="to pass appinspect" ]\
| map\
    [ search index=_audit `searchheadhosts` "info=granted" "search_id='$sid$'"\
    | rex ", search='(?P<search>[\S+\s+]+?)', "\
    | eval search=substr(search,0,100)\
    | eval message=$message$ ] maxsearches=50\
| stats values(timestamp) AS time, values(message) AS message, values(search) AS search, values(apiStartTime) AS startTime, values(apiEndTime) AS endTime, values(savedsearch_name) AS savedsearch_name by user

[SearchHeadLevel - Search Queries Per Day Audit Logs]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. A query to list the number of searches per/day by type of search (dashboard/saved search/ad-hoc) 
dispatch.earliest_time = -2d@d
dispatch.latest_time = @d
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = line
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Count the number of non-system Splunk queries that use a search command, excludes rest/metrics/data model acceleration et cetera")`\
index=_audit `searchheadhosts` "info=granted" "search='search " search_id!="'SummaryDirector_*" search_id!="'rsa_*" user!=admin user!=splunk-system-user \
| bin _time span=1d \
| stats dc(user) AS activeUserCount, count AS totalSearchCount, count(eval(match(savedsearch_name,"^.+$") AND NOT (match(savedsearch_name,"^search")))) AS savedsearchCount, count(eval(match(savedsearch_name,"^search"))) AS searchesFromDashboards, count(eval(match(savedsearch_name,"^$"))) AS adhocSearches by _time

[SearchHeadLevel - Search Queries By Type Audit Logs]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. A pie graph to show statistics on the number of searches by type of search (index specified, index wildcard used) et cetera, "SearchHeadLevel - Search Queries By Type Audit Logs macro version" includes macro substitution but is otherwise the same report
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = visualizations
display.page.search.tab = visualizations
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = pie
display.visualizations.show = 0
display.visualizations.trellis.splitBy = _aggregation
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one")`\
    index=_audit "info=granted" `searchheadhosts` "search='" search_id!="'rsa_*"\
| rex ", search='(?P<search>[\S+\s+]+?)', "\
| rex field=search "^(?P<searchbeforepipe>[^|]+)" \
| rex mode=sed field=searchbeforepipe "s/search \(index=\* OR index=_\*\) index=/search index=/"\
| rex mode=sed field=searchbeforepipe "s/search index=\s*\S+\s+index=/search index=/"\
| eval indexNotSpecified = if(NOT match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"^\s*search "),"1","0")\
| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,"(?s)^\s*search\s.*(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"`")),"1","0")\
| stats count, count(eval(match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)"))) AS indexClause, count(eval(match(searchbeforepipe,"(index\s*(=|::)\s*\S*\*)|(index\s+IN\s*\([^\)]*\*)"))) AS indexWildcard, count(eval(match(searchbeforepipe,"\`[^\`]+\`"))) AS macroNoIndex, count(eval(match(search,"^\s*\|\s*summarize"))) AS summarize, count(eval(match(search,"(?i)^\s*\|\s*savedsearch"))) AS savedsearch, count(eval(match(search,"(?i)^\s*\|\s*(from\s*)?datamodel"))) AS datamodel, count(eval(match(search,"(?i)^\s*\|\s*loadjob"))) AS loadjob, count(eval(match(search,"(?i)^\s*\|\s*(multisearch|union)"))) AS multisearch, count(eval(match(search,"(?i)^\s*\|\s*(pivot)"))) AS pivot, count(eval(match(search,"(?i)^\s*\|\s*(metadata)"))) AS metadata, count(eval(indexNotSpecified==1)) AS indexNotSpecified, count(eval(macroWithIndexClause==1)) AS macroWithIndexClause, count(eval(match(search,"(?i)^\s*\|\s*(tstats)"))) AS tstats, count(eval(match(search,"(?i)^\s*\|\s*(rest)"))) AS rest, count(eval(match(search,"(?i)^\s*\|\s*(mcatalog|mstats)"))) AS metrics, count(eval(match(search,"(?i)^\s*\|\s*(from\s+)?inputlookup"))) AS inputlookup, count(eval(match(search_id,"^'ta_"))) AS typeahead\
| eval macroNoIndex = macroNoIndex-macroWithIndexClause, indexClause = indexClause - indexWildcard\
| eval unknown = count - (indexClause + macroNoIndex + summarize + savedsearch + datamodel + loadjob + multisearch + pivot + metadata + indexNotSpecified + tstats + rest + metrics + inputlookup + typeahead)\
| fields - macroWithIndexClause, count\
| transpose column_name="xaxis" header_field="perc" 

[SearchHeadLevel - Search Queries By Type Audit Logs macro version]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. A pie graph to show statistics on the number of searches by type of search (index specified, index wildcard used) et cetera this version attempts to substitute macros, "SearchHeadLevel - Search Queries By Type Audit Logs" does not include macro substitution but is otherwise the same report. Requires "SearchHeadLevel - Macro report". Also note that you need to remove the comment around the lookup command within the search...
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = visualizations
display.page.search.tab = visualizations
display.statistics.drilldown = none
display.statistics.show = 0
display.statistics.wrap = 0
display.visualizations.charting.chart = pie
display.visualizations.trellis.splitBy = _aggregation
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one")`\
    index=_audit `searchheadhosts` "info=granted" "search='" search_id!="'rsa_*"\
| rex ", search='(?P<search>[\S+\s+]+?)', " \
| rex field=search "^(?P<searchbeforepipe>[^|]+)" \
| rex field=searchbeforepipe max_match=5 "\`(?!\")(?!')(?P<macro>[^\`]+)\`" \
| fillnull macro value="removeme" \
| mvexpand macro \
| eval macro = if(macro=="removeme",null(),macro) \
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)" \
| rex max_match=10 field=macro "(?P<commas2>,)" \
| rex max_match=1 field=macro "(?P<match>[^\(]+\()" \
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")` \
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null())) \
| eval argCount=if(isnull(argCount2),0,argCount2+1) \
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount) \
| rex field=macro "(?P<macro>^[^\( ]+)" \
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")") \
| search `comment("Remove this comment from the line below once the required reports have been run, commented due to preventing AppInspect passing...\
| lookup splunkadmins_macros title AS macroName ")`\
| eval macroReplace=if(argCount==0,"`" . macro . "`","`" . macro . "\(.*?\)`")\
| eval hasMacro = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,"(?s)^\s*search\s.*(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"`")),"1","0")\
| eval search=if(isnotnull(definition),replace(search, macroReplace, mvindex(definition,0)),search)\
| eval searchbeforepipe=if(isnotnull(definition),replace(searchbeforepipe, macroReplace, mvindex(definition,0)),searchbeforepipe)\
| rex mode=sed field=searchbeforepipe "s/search \(index=\* OR index=_\*\) index=/search index=/" \
| rex mode=sed field=searchbeforepipe "s/search index=\s*\S+\s+index=/search index=/" \
| eval indexNotSpecified = if(NOT match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"^\s*search "),"1","0")\
| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,"(?s)^\s*search\s.*(index\s*(=|::))|(index\s*IN)") AND hasMacro=="1"),"1","0")\
| stats count, count(eval(match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)"))) AS indexClause, count(eval(match(searchbeforepipe,"(index\s*(=|::)\s*\S*\*)|(index\s+IN\s*\([^\)]*\*)"))) AS indexWildcard, count(eval(hasMacro=="1")) AS macroNoIndex, count(eval(match(search,"^\s*\|\s*summarize"))) AS summarize, count(eval(match(search,"(?i)^\s*\|\s*savedsearch"))) AS savedsearch, count(eval(match(search,"(?i)^\s*\|\s*(from\s*)?datamodel"))) AS datamodel, count(eval(match(search,"(?i)^\s*\|\s*loadjob"))) AS loadjob, count(eval(match(search,"(?i)^\s*\|\s*(multisearch|union)"))) AS multisearch, count(eval(match(search,"(?i)^\s*\|\s*(pivot)"))) AS pivot, count(eval(match(search,"(?i)^\s*\|\s*(metadata)"))) AS metadata, count(eval(indexNotSpecified==1)) AS indexNotSpecified, count(eval(macroWithIndexClause==1)) AS macroWithIndexClause, count(eval(match(search,"(?i)^\s*\|\s*(tstats)"))) AS tstats, count(eval(match(search,"(?i)^\s*\|\s*(rest)"))) AS rest, count(eval(match(search,"(?i)^\s*\|\s*(mcatalog|mstats)"))) AS metrics, count(eval(match(search,"(?i)^\s*\|\s*(from\s+)?inputlookup"))) AS inputlookup, count(eval(match(search_id,"^'ta_"))) AS typeahead\
| eval indexClause = indexClause-indexWildcard, macroNoIndex = macroNoIndex-macroWithIndexClause\
| eval total = indexClause + indexWildcard + macroNoIndex + summarize + savedsearch + datamodel + loadjob + multisearch + pivot + metadata + indexNotSpecified + tstats + rest + metrics + typeahead\
| eval unknown = count - total\
| fields - total, searchcommandcount, macroWithIndexClause, count\
| transpose column_name="xaxis" header_field="perc"

[SearchHeadLevel - Search Queries By Type Audit Logs macro version other]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report relates to the "SearchHeadLevel - Search Queries By Type Audit Logs" and equivalent macro version but exists to print out the entries that did not fit into any of the categories. Requires "SearchHeadLevel - Macro report". Also note that you need to remove the comment around the lookup command within the search...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = pie
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one")`\
    index=_audit `searchheadhosts` "info=granted" "search='" search_id!="'rsa_*"\
| rex ", search='(?P<search>[\S+\s+]+?)', " \
| rex field=search "^(?P<searchbeforepipe>[^|]+)" \
| rex field=searchbeforepipe max_match=5 "\`(?!\")(?!')(?P<macro>[^\`]+)\`"\
| fillnull macro value="removeme"\
| mvexpand macro\
| eval macro = if(macro=="removeme",null(),macro)\
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")`\
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)" \
| rex max_match=10 field=macro "(?P<commas2>,)"\
| rex max_match=1 field=macro "(?P<match>[^\(]+\()"\
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")` \
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null()))\
| eval argCount=if(isnull(argCount2),0,argCount2+1)\
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount)\
| rex field=macro "(?P<macro>^[^\( ]+)"\
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")")\
| search `comment("Remove this comment from the line below once the required reports have been run, commented due to preventing AppInspect passing...\
| lookup splunkadmins_macros title AS macroName ")`\
| eval macroReplace=if(argCount==0,"`" . macro . "`","`" . macro . "\(.*?\)`")\
| eval hasMacro = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,"(?s)^\s*search\s.*(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"`")),"1","0")\
| eval search=if(isnotnull(definition),replace(search, macroReplace, mvindex(definition,0)),search)\
| eval searchbeforepipe=if(isnotnull(definition),replace(searchbeforepipe, macroReplace, mvindex(definition,0)),searchbeforepipe)\
| rex mode=sed field=searchbeforepipe "s/search \(index=\* OR index=_\*\) index=/search index=/"\
| rex mode=sed field=searchbeforepipe "s/search index=\s*\S+\s+index=/search index=/"\
| eval indexNotSpecified = if(NOT match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)") AND match(searchbeforepipe,"^\s*search "),"1","0")\
| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,"(?s)^\s*search\s.*(index\s*(=|::))|(index\s*IN)") AND hasMacro=="1"),"1","0")\
| eval indexClause = if(match(searchbeforepipe,"(index\s*(=|::))|(index\s*IN)"),"1","0")\
| eval indexWildcard = if(match(searchbeforepipe,"(index\s*(=|::)\s*\S*\*)|(index\s+IN\s*\([^\)]*\*)"),"1","0")\
| eval macroNoIndex = if(hasMacro=="1","1","0")\
| eval summarize = if(match(search,"^\s*\|\s*summarize"),"1","0")\
| eval savedsearch = if(match(search,"(?i)^\s*\|\s*(from\s+)?savedsearch"),"1","0")\
| eval datamodel = if(match(search,"(?i)^\s*\|\s*(from\s+)?datamodel"),"1","0")\
| eval loadjob = if(match(search,"(?i)^\s*\|\s*loadjob"),"1","0")\
| eval multisearch = if(match(search,"(?i)^\s*\|\s*(multisearch|union)"),"1","0")\
| eval pivot = if(match(search,"(?i)^\s*\|\s*(pivot)"),"1","0")\
| eval metadata = if(match(search,"(?i)^\s*\|\s*(metadata)"),"1","0")\
| eval tstats = if(match(search,"(?i)^\s*\|\s*(tstats)"),"1","0")\
| eval rest = if(match(search,"(?i)^\s*\|\s*(rest)"),"1","0")\
| eval inputlookup = if(match(search,"(?i)^\s*\|\s*(from\s+)?inputlookup"),"1","0")\
| eval metrics = if(match(search,"(?i)^\s*\|\s*(mcatalog|mstats)"),"1","0")\
| eval typeahead = if(match(search_id,"^'ta_"),"1","0")\
| search indexClause=0 AND indexWildcard=0 AND macroNoIndex=0 AND summarize=0 AND savedsearch=0 AND datamodel=0 AND loadjob=0 AND pivot=0 AND multisearch=0 AND  metadata=0 AND indexNotSpecified=0 AND macroWithIndexClause=0 AND tstats=0 AND rest=0 AND inputlookup=0 AND metrics=0 AND typeahead=0\
| cluster  field=search t=0.01 showcount=true\
| table search, cluster_count

[SearchHeadLevel - Search Queries summary exact match]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report is an attempt to use the Splunk audit logs to generate summary statistics on what indexes were accessed and the period of time they were accessed over. There is a lot of complexity here as the audit logs make this task very challenging. This version relates to entries where index=<indexname> where used without wildcards, an additional report "SearchHeadLevel - Search Queries summary non-exact match" also exists to perform this same function without an index specified or when wildcards are used. This report requires "SearchHeadLevel - Index access list by user" and "SearchHeadLevel - Macro report". Also note that you need to remove the comment around the lookup command within the search... 
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = pie
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to extract out which indexes are accessed per search query by any search and compute statistics on them")`\
    index=_audit ("info=granted" "search='search " ("index" OR "`")) OR "info=completed" OR "info=canceled" `comment("Only include search strings based on search ..., that way we don't include REST commands or similar, but don't exlcude the completed/canceled events as they do not have a search field")`\
    `comment("audit_search_timeperiod")` search_id!="'SummaryDirector_*" search_id!="'rsa_*" \
| rex ", search='(?P<search>[\S+\s+]+?)', "\
| rex field=search "^(?P<search>[^|]+)"\
| rex field=search max_match=1 "\`(?!\")(?!')(?P<macro>[^\`]+)\`"\
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them, we could mvexpand but this brings more issues so we just replace the first macro we see...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)" \
| rex max_match=10 field=macro "(?P<commas2>,)" \
| rex max_match=1 field=macro "(?P<match>[^\(]+\()"\
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")` \
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null()))\
| eval argCount=if(isnull(argCount2),0,argCount2+1) \
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount)\
| rex field=macro "(?P<macro>^[^\( ]+)"\
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")")\
| search `comment("Remove this comment from the line below once the required reports have been run, commented due to preventing AppInspect passing...\
| lookup splunkadmins_macros title AS macroName ")`\
| eval macroReplace=if(argCount==0,"`" . macro . "`","`" . macro . "\(.*?\)`")\
| eval search=if(isnotnull(definition),replace(search, macroReplace, mvindex(definition,0)),search)\
| search `comment("The (index=* OR index=_*) index=<specific index> is a common use case for enterprise security, also some individuals like doing a similar trick so remove the index=*... as this is not a wildcard index search")`\
| rex mode=sed field=search "s/search \(index=\* OR index=_\*\)\s*[\(]*index=/search index=/"\
| rex mode=sed field=search "s/search index=\s*\S+\s+index=/search index=/" \
| search `comment("This eliminates excessive amounts of data going into the selfjoin command as it has a 100K limit, therefore we eliminate as much as possible before running it. Note that we're ignoring both the macros case and the savedsearches case in the below by looking for index= stanzas in the search query. We also eliminate some legitimate searches that hit zero buckets by doing the searched_buckets>0 clause. We also add searchinfo so we can exclude anything that wasn't run without the search time period (as we will miss the search part of the command)")` \
| eval keep=if(isnull(search),"true",if(match(search,"(index\s*(=|::))|(index\s*IN)"),"true","false")) \
| eval keep2=if(isnull(search),"true",if(match(search,"(index\s*(=|::)\s*\S*\*)|(index\s+IN\s*\S*\*)"),"false","true")) \
| where keep="true" AND keep2="true" AND if(isnotnull(searched_buckets),if(searched_buckets>0,true(),false()),true()) \
| addinfo \
| where if(isnotnull(exec_time),exec_time>info_min_time,true())\
| selfjoin overwrite=false search_id \
| fields timestamp, user, total_run_time, event_count, scan_count, search, search_et, search_lt, savedsearch_name, search_id \
| search `comment("We now deal with cases where search earliest/latest times were not specified, assume all time is about 1 year in the past and latest time was the search run time")`\
| eval search_lt=if(search_lt=="N/A",strptime(timestamp,"%m-%d-%Y %H:%M:%S.%3N"),search_lt), search_et=if(search_et=="N/A",now()-(365*24*60*60),search_et) \
| search `comment("Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements")` \
| rex field=search "(NOT\s+index\s*(=|::)\s*[^ ]+)|(NOT\s+\([^\)]+\))|(index\s*(=|::)\s*(?P<indexregex>[^ \)\(\r\n]+))" max_match=50 \
| rex field=search "(NOT\s+index\s+IN\s*\([^\)]+)|(index\s+IN\s*\((?P<indexin>[^\)]+))" max_match=50 \
| makemv delim="," indexin \
| eval indexes=mvappend(indexregex,indexin) \
| mvexpand indexes \
| eval indexes=replace(lower(indexes), "\"", "") \
| eval indexes=replace(indexes, "'", "") \
| eval period=search_lt-search_et\
| fields - indexin, indexregex \
| search `comment("Commands like multikv result in giant event count numbers compared to scan count, lower the lispy back down to normal to prevent the stats from been broken. lispy efficiency as per Martin Muller's conf presentations")`\
| eval lispy_efficiency = if(event_count>scan_count,scan_count,event_count) / scan_count\
| search `comment("TODO breakdown the counts into time periods, perhaps 1hour, 4 hours, 8 hours, 24 hours, 3 days, 7 days, 14 days, 21 days, 30 days, 2 months, 3 months, 6months, > 60 months? or similar. Do a count for each one as that would be useful for a dashboard...")`\
| stats count, dc(user) AS userCount, max(period) AS maxPeriod, avg(period) AS avgPeriod, median(period) AS medianPeriod, \
    avg(total_run_time) AS avg_total_run_time, max(total_run_time) AS max_total_run_time, median(total_run_time) AS median_total_run_time, avg(lispy_efficiency) AS avg_lispy_efficiency, max(lispy_efficiency) AS max_lispy_efficiency, min(lispy_efficiency) AS min_lispy_efficiency, median(lispy_efficiency) AS median_lispy_efficiency by indexes \
| fillnull max_lispy_efficiency, min_lispy_efficiency, median_lispy_efficiency\
| eval maxPeriod=tostring(maxPeriod,"duration"), avgPeriod=tostring(avgPeriod,"duration"), medianPeriod=tostring(medianPeriod,"duration")

[SearchHeadLevel - Search Queries summary non-exact match]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report is an attempt to use the Splunk audit logs to generate summary statistics on what indexes were accessed and the period of time they were accessed over. There is a lot of complexity here as the audit logs make this task very challenging. This version relates to entries where either index names are specified with wildcards or no index is specified, an additional report "SearchHeadLevel - Search Queries summary exact match" also exists to perform this same function where an index=<indexname> is specified. This report requires "SearchHeadLevel - Index access list by user" and "SearchHeadLevel - Macro report". Also note that you need to remove the comment around the lookup within the search...
dispatch.earliest_time = -900s
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Attempt to extract out which indexes are accessed per search query by any search")`\
    index=_audit `searchheadhosts` ("info=granted" "search='search ") OR "info=completed" OR "info=canceled" `comment("Only include search strings based on search ..., that way we don't include REST commands or similar, but don't exlcude the completed/canceled events as they do not have a search field")` search_id!="'SummaryDirector_*" search_id!="'rsa_*"\
    `comment("audit_search_timeperiod")` \
| rex ", search='(?P<search>[\S+\s+]+?)', " \
| rex field=search "^(?P<search>[^|]+)" \
| rex field=search max_match=1 "\`(?!\")(?!')(?P<macro>[^\`]+)\`" \
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)" \
| rex max_match=10 field=macro "(?P<commas2>,)" \
| rex max_match=1 field=macro "(?P<match>[^\(]+\()" \
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")` \
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null())) \
| eval argCount=if(isnull(argCount2),0,argCount2+1) \
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount) \
| rex field=macro "(?P<macro>^[^\( ]+)" \
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")") \
| search `comment("Remove this comment from the line below once the required reports have been run, commented due to preventing AppInspect passing...\
| lookup splunkadmins_macros title AS macroName")`\
| eval macroReplace=if(argCount==0,"`" . macro . "`","`" . macro . "\(.*?\)`") \
| eval search=if(isnotnull(definition),replace(search, macroReplace, mvindex(definition,0)),search) \
| search `comment("This eliminates excessive amounts of data going into the selfjoin command as it has a 100K limit, therefore we eliminate as much as possible before running it. Note that we're ignoring both the macros case and the savedsearches case in the below by looking for index= stanzas in the search query. We also eliminate some legitimate searches that hit zero buckets by doing the searched_buckets>0 clause. We also add searchinfo so we can exclude anything that wasn't run without the search time period (as we will miss the search part of the command)")` \
| eval keep2=if(isnull(search),"true",if(match(search,"(index\s*(=|::)\s*\S*\*)|(index\s+IN\s*\S*\*)"),"true",if(match(search,"index\s*(=|IN|::)"),"false","true"))) \
| where keep2="true" AND if(isnotnull(searched_buckets),if(searched_buckets>0,true(),false()),true()) \
| addinfo \
| where if(isnotnull(exec_time),exec_time>info_min_time,true()) \
| selfjoin overwrite=false search_id \
| search `comment("Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements")` \
| rex mode=sed field=search "s/search \(index=\* OR index=_\*\)\s*[\(]*index=/search index=/" \
| rex mode=sed field=search "s/search index=\s*\S+\s+index=/search index=/" \
| rex field=search "(NOT\s+index\s*(=|::)\s*[^ ]+)|(NOT\s+\([^\)]+\))|(index\s*(=|::)\s*(?P<indexregex>[^ \)\(\r\n]+))" max_match=50 \
| rex field=search "(NOT\s+index\s+IN\s*\([^\)]+)|(index\s+IN\s*\((?P<indexin>[^\)]+))" max_match=50 \
| makemv delim="," indexin \
| eval indexes=mvappend(indexregex,indexin) \
| eval search_lt=if(search_lt=="N/A",strptime(timestamp,"%m-%d-%Y %H:%M:%S.%3N"),search_lt), search_et=if(search_et=="N/A",now()-(365*24*60*60),search_et) \
| eval period=(search_lt-search_et)/60/60 \
| fields - indexin, indexregex \
| search `comment("Now that we have a giant list of indexes, we want to strip any quote characters and lowercase them in case we use a kvstore for lookups or similar.")` \
| search `comment("Run a lookup to find the default indexes and the allowed indexes per user")` \
| lookup splunkadmins_userlist_indexinfo user OUTPUT srchIndexesAllowed, srchIndexesDefault \
| makemv srchIndexesAllowed tokenizer=(\S+)\
| makemv srchIndexesDefault tokenizer=(\S+) \
| search `comment("At this point we have a multi-valued list of indexes and srchIndexesAllowed, mvexpand runs out of memory while trying to expand them so stats works nicely. \
Other multivalue commands return the first result but not all relevant results which we want in this case. \
| inputlookup supports wildcard matching via the where clause but lookup does not, this search was attempted with the map command but that didn't scale at all.\
The current implementation isn't very good either, but it's the best I can find, yet another alternative was to output the results from this search into a lookup, run an | inputlookup on the list of users and then mark the indexes field as a wildcard and run the lookup per-result from the inputlookup from the users list, this is also extremely ugly, so a custom command called streamfilter was created for this one purpose...(but it can be reused)")` \
| streamfilterwildcard pattern=indexes fieldname=indexes srchIndexesAllowed\
| eval indexes=if(isnull(indexes),srchIndexesDefault,indexes)\
| makemv indexes tokenizer=(\S+)\
| search `comment("Commands like multikv result in giant event count numbers compared to scan count, lower the lispy back down to normal to prevent the stats from been broken. lispy efficiency as per Martin Muller's conf presentations")`\
| eval lispy_efficiency = if(event_count>scan_count,scan_count,event_count) / scan_count\
| search `comment("TODO breakdown the counts into time periods, perhaps 1hour, 4 hours, 8 hours, 24 hours, 3 days, 7 days, 14 days, 21 days, 30 days, 2 months, 3 months, 6months, > 60 months? or similar. Do a count for each one as that would be useful for a dashboard...")`\
| stats count, dc(user) AS userCount, max(period) AS maxPeriod, avg(period) AS avgPeriod, median(period) AS medianPeriod, \
    avg(total_run_time) AS avg_total_run_time, max(total_run_time) AS max_total_run_time, median(total_run_time) AS median_total_run_time, avg(lispy_efficiency) AS avg_lispy_efficiency, max(lispy_efficiency) AS max_lispy_efficiency, min(lispy_efficiency) AS min_lispy_efficiency, median(lispy_efficiency) AS median_lispy_efficiency by indexes \
| fillnull max_lispy_efficiency, min_lispy_efficiency, median_lispy_efficiency\
| eval maxPeriod=tostring(maxPeriod,"duration"), avgPeriod=tostring(avgPeriod,"duration"), medianPeriod=tostring(medianPeriod,"duration")

[SearchHeadLevel - Role access list by user]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report outputs a list of users and which role they are in, and what the srchIndexesDefault and srchIndexesAvailable exist for that particular user, this does not list individual index names, the report "SearchHeadLevel - Index access list by user" exists to list index names...
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/authentication/users splunk_server="local" \
| eval comment="This search aims to provide a giant list of users and a list of wildcards (or index names if specified in the Splunk config) for srchIndexesAllowed/srchIndexesDefault" \
| table title roles\
| append [ | makeresults | eval title="splunk-system-user", roles="admin" ]\
| rename title as user \
| mvexpand roles \
| join type=left roles \
    [ rest /services/authorization/roles splunk_server="local" \
    | table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault\
    | rename title as roles] \
| makemv srchIndexesAllowed tokenizer=(\S+) \
| makemv srchIndexesDefault tokenizer=(\S+)\
| makemv imported_srchIndexesAllowed tokenizer=(\S+)\
| makemv imported_srchIndexesDefault tokenizer=(\S+)\
| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed)\
| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault)\
| fillnull srchIndexesDefault, srchIndexesAllowed value="removeme"\
| mvexpand srchIndexesAllowed\
| eval srchIndexesAllowed=if(srchIndexesAllowed=="removeme",null(),srchIndexesAllowed)\
| fields srchIndexesAllowed, srchIndexesDefault, user\
| stats values(*) as * by user\
| mvexpand srchIndexesDefault\
| eval srchIndexesDefault=if(srchIndexesDefault=="removeme",null(),srchIndexesDefault)\
| stats values(*) as * by user

[SearchHeadLevel - Index access list by user]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report outputs a list of indexes available on a per-user basis which is used by another report, "SearchHeadLevel - Search Queries summary non-exact match", requires report "SearchHeadLevel - Index list report"
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/authorization/roles splunk_server="local" \
| eval comment="This search aims to provide a giant list of users and what indexes they have access to (as in a list of index names, not a list of wildcards). Due to mvexpand hitting memory limits in the environment this alternative version runs many subsearches that do not hit the memory limits" \
| table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault \
| rename title as roles \
| makemv srchIndexesAllowed tokenizer=(\S+) \
| makemv srchIndexesDefault tokenizer=(\S+) \
| makemv imported_srchIndexesAllowed tokenizer=(\S+) \
| makemv imported_srchIndexesDefault tokenizer=(\S+) \
| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed) \
| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault) \
| fillnull srchIndexesDefault, srchIndexesAllowed value="requiredformvexpand" \
| mvexpand srchIndexesAllowed \
| eval srchIndexesAllowed=if(srchIndexesAllowed=="requiredformvexpand",null(),srchIndexesAllowed) \
| eval srchIndexesAllowed=lower(srchIndexesAllowed) \
| fields srchIndexesAllowed, srchIndexesDefault, roles \
| map \
    [| inputlookup splunkadmins_indexlist where index="$srchIndexesAllowed$" AND index!="requiredformvexpand"\
    | eval regex="^" . "$srchIndexesAllowed$" . "$" \
    | eval regex=replace(regex,"\*",".*") \
    | eval regex=if(substr(regex,1,3)=="^.*","^[^_].*" . substr(regex,4),regex) \
    | where match(index,regex) \
    | eval srchIndexesAllowed="$srchIndexesAllowed$", srchIndexesDefault="$srchIndexesDefault$", roles="$roles$" \
    | fields index, roles, srchIndexesAllowed, srchIndexesDefault ] maxsearches=5000 \
| stats values(index) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles \
| makemv srchIndexesDefault tokenizer=(\S+) \
| mvexpand srchIndexesDefault \
| append [ | makeresults | eval srchIndexesAllowed="workaround for map errors", srchIndexesDefault="to pass appinspect", roles="N/A" ]\
| map \
    [| inputlookup splunkadmins_indexlist where index="$srchIndexesDefault$" \
    | eval regex="^" . "$srchIndexesDefault$" . "$" \
    | eval regex=replace(lower(regex),"\*",".*") \
    | eval regex=if(substr(regex,1,3)=="^.*","^[^_].*" . substr(regex,4),regex) \
    | where match(index,regex) \
    | eval srchIndexesAllowed="$srchIndexesAllowed$", srchIndexesDefault="$srchIndexesDefault$", roles="$roles$" \
    | fields index, roles, srchIndexesAllowed, srchIndexesDefault ] maxsearches=5000 \
| where srchIndexesAllowed!="workaround for map errors"\
| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(index) AS srchIndexesDefault by roles \
| makemv srchIndexesAllowed tokenizer=(\S+) \
| append \
    [| rest /services/admin/LDAP-groups splunk_server=local \
    | where isnotnull(roles) \
    | mvexpand users \
    | rex field=users "CN=(?P<user>[^,]+)" \
    | stats values(user) AS user by roles ] \
| append \
    [| rest /services/authentication/users splunk_server=local \
    | search type=Splunk \
    | table title, roles \
    | rename title AS user \
    | mvexpand roles ] \
| append \
    [| makeresults \
    | eval user="splunk-system-user", roles="admin" ]\
| eval srchIndexesDefault = if(srchIndexesDefault=="requiredformvexpand",null(),srchIndexesDefault)     \
| eventstats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles \
| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user\
| outputlookup splunkadmins_userlist_indexinfo 

[SearchHeadLevel - Index list report]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report outputs a list of indexes available and an additional "requiredformvexpand" value, which is used by the report "SearchHeadLevel - Index access list by user" 
dispatch.earliest_time = -30d@d
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | eventcount summarize=false index=* OR index=_* \
| fields index \
| dedup index\
| append [ | makeresults | eval index="requiredformvexpand" ]\
| table index\
| outputlookup splunkadmins_indexlist

[SearchHeadLevel - Scheduled Search Efficiency]
action.email.useNSSubject = 1
alert.track = 0
description = Report only? Yes. This report was orginally found on answers or a Splunk conf talk, it lists the scheduled searches, how often they run and how long they have taken to run
dispatch.earliest_time = -30d@d
dispatch.latest_time = now
display.events.fields = ["index","sourcetype","host"]
display.events.list.drilldown = none
display.events.list.wrap = 0
display.events.maxLines = 100
display.events.raw.drilldown = none
display.events.rowNumbers = 1
display.events.table.drilldown = 0
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.statistics.drilldown = none
display.statistics.wrap = 0
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This likely came from a Splunk conf presentation but I cannot remember which one so cannot attribute the original author!")`\
`comment("Determine the length of time a scheduled search takes to run compared to how often it is configured to run, excluding acceleration jobs")`\
index=_internal `searchheadhosts` sourcetype=scheduler source=*scheduler.log (user=*) savedsearch_name!="_ACCELERATE_DM*"\
| stats avg(run_time) as average_runtime_in_sec count(savedsearch_name) as num_times_per_week sum(run_time) as total_runtime_sec by savedsearch_name user app host\
| eval ran_every_x_mins=round(60/(num_times_per_week/168))\
| eval average_runtime_duration=tostring(round(average_runtime_in_sec/60,2), "duration")\
| eval average_runtime_in_sec=round(average_runtime_in_sec, 2)\
| eval efficiency=round(((60/(num_times_per_week/168))/(average_runtime_in_sec/60)), 2)\
| sort efficiency\
| table savedsearch_name, app, average_runtime_duration, num_times_per_week, ran_every_x_mins, efficiency, user, host 

##############
#
# Customise these macros to ensure the SplunkAdmins / Alerts for Splunk Admins
# application works as expected
#
##############
[indexerhosts]
definition = host=*
iseval = 0

[heavyforwarderhosts]
definition = host=*
iseval = 0

[searchheadhosts]
definition = host=*
iseval = 0

#Designed for searches where returning data from other search heads
#would not provide valid results...
[localsearchheadhosts]
definition = host=*
iseval = 0

[splunkenterprisehosts]
definition = host=*
iseval = 0

[deploymentserverhosts]
definition = host=*
iseval = 0

[licensemasterhost]
definition = host=*
iseval = 0

[searchheadsplunkservers]
definition = splunk_server=*
iseval = 0

[splunkindexerhostsvalue]
definition = splunk_server=*
iseval = 0

[splunkadmins_splunkd_source]
definition = source=*splunkd.log
iseval = 0

[splunkadmins_splunkuf_source]
definition = source=*splunkd.log
iseval = 0

[splunkadmins_mongo_source]
definition = source=*mongod.log
iseval = 0

[splunkadmins_license_usage_source]
definition = source=*license_usage.log
iseval = 0

[splunkadmins_clustermaster_oshost]
definition = host=changeme
iseval = 0

#Only used in a few searches, customise this if you have the cluster master as a search
#peer, if not you may wish to leave this to local and run the ClusterMasterLevel searches on
#the cluster master server...
[splunkadmins_clustermaster_host]
definition = splunk_server=local

##############
#
# Utility functions
#
##############
[comment(1)]
args = text
definition = ""
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of ((host=X _time>start _time<end) OR (host=Y _time>start _time<end)
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_list(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for when this particular Splunk server was shutdown, plus any contingency time as requested")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| eval message=coalesce(message,event_message)\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime by host\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search="host=" . host . " _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of keywords (hostnames without the host=) / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of ((X _time>start _time<end) OR (Y _time>start _time<end)
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_keyword(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for when this particular Splunk server was shutdown, plus any contingency time as requested")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| eval message=coalesce(message,event_message)\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime by host\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search=host . " _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of (_time>start _time<end) this allows entire indexer cluster restarts to be filtered out.
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_time(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for the time when any indexer was shutdown")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| eval message=coalesce(message,event_message)\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search=" _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0


##############
#
# Per-alert macros that can be customised for
# filtering unncessary data from alerts where required
#
##############

[splunkadmins_acc_datamodels]
definition = ""
iseval = 0

[splunkadmins_runscript]
definition = ""
iseval = 0

[splunkadmins_timeskew]
definition = ""
iseval = 0

[splunkadmins_changedprops]
definition = ""
iseval = 0

[splunkadmins_changedprops_count]
definition = 3
iseval = 0

[splunkadmins_btoolvalidation_ds]
definition = `comment("Splunk for stream doesn't include a config file which causes errors, however it appears to work without it...")` NOT "/opt/splunk/etc/deployment-apps/Splunk_TA_stream*"
iseval = 0

[splunkadmins_bandwidth]
definition = ""
iseval = 0

[splunkadmins_toosmall_checkcrc]
definition = ""
iseval = 0

[splunkadmins_forwarderdown]
definition = ""
iseval = 0

[splunkadmins_heavylogging]
definition = ""
iseval = 0

[splunkadmins_exceeding_filedescriptor]
definition = ""
iseval = 0

[splunkadmins_sending_data]
definition = ""
iseval = 0

[splunkadmins_sending_data_nonhf_count]
definition = 0
iseval = 0

[splunkadmins_sending_data_hf_count]
definition = 5
iseval = 0

[splunkadmins_unusual_duplication]
definition = ""
iseval = 0

[splunkadmins_unusual_duplication_count]
definition = 10
iseval = 0

[splunkadmins_crcsalt_initcrc]
definition = ""
iseval = 0

[splunkadmins_uf_timeshifting]
definition = ""
iseval = 0

[splunkadmins_future_dated]
definition = ""
iseval = 0

[splunkadmins_failuretoparse_timestamp]
definition = ""
iseval = 0

[splunkadmins_failuretoparse_timestamp_count]
definition = 0
iseval = 0

[splunkadmins_failuretoparse_timestamp_binperiod]
definition = 1m
iseval = 0

[splunkadmins_failuretoparse_timestamp2]
definition = ""
iseval = 0

[splunkadmins_indexconfig_warn]
definition = ""
iseval = 0

[splunkadmins_indexerqueue_fillperc_nonindexqueue]
definition = 50
iseval = 0

[splunkadmins_indexerqueue_fillperc_indexqueue]
definition = 90
iseval = 0

[splunkadmins_indexer_replication_queue_count]
definition = 15
iseval = 0

[splunkadmins_uneven_indexed_perc]
definition = 25
iseval = 0

[splunkadmins_weekly_brokenevents]
definition = ""
iseval = 0

[splunkadmins_weekly_truncated]
definition = ""
iseval = 0

[splunkadmins_weekly_truncated_count]
definition = 0
iseval = 0


[splunkadmins_valid_timestamp_invalidparsed]
definition = ""
iseval = 0

[splunkadmins_longrunning_searches]
definition = `comment("Exclude various standard/expected searches")` savedsearch_name!="Generate Meta Woot! every 15 mins" savedsearch_name!="Generate NMON*"
iseval = 0

[splunkadmins_realtime_scheduledsearches]
definition = ""
iseval = 0

[splunkadmins_scheduledsearches_cannot_run]
definition = ""
iseval = 0

[splunkadmins_scheduledsearches_without_earliestlatest]
definition = NOT (eai:acl.app=splunk_app_aws author=nobody)
iseval = 0

#Ignore Splunk apps which will trigger this
[splunkadmins_scheduledsearches_without_index]
definition = eai:acl.app!="splunk_archiver" eai:acl.app!="splunk_app_windows_infrastructure" eai:acl.app!="splunk_app_aws" eai:acl.app!="nmon"
iseval = 0

[splunkadmins_scriptfailures]
definition = ""
iseval = 0

[splunkadmins_users_violating_searchquota]
definition = ""
iseval = 0

[splunkadmins_users_exceeding_diskquota]
definition = ""
iseval = 0

[splunkadmins_execprocessor]
definition = ""
iseval = 0

[splunkadmins_timeformat_change]
definition = ""
iseval = 0

[splunkadmins_loginattempts]
definition = ""
iseval = 0

[splunkadmins_insufficient_permissions]
definition = ""
iseval = 0

[splunkadmins_tcpoutput_paused]
definition = ""
iseval = 0

[splunkadmins_streamerrors]
definition = ""
iseval = 0

[splunkadmins_unable_distribute_to_peer]
definition = ""
iseval = 0

[splunkadmins_dashboards_allindexes]
definition = NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app)
iseval = 0

[splunkadmins_scheduled_incorrectsharing]
definition = ""
iseval = 0

[splunkadmins_realtime_dashboard]
definition = NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app)
iseval = 0

[splunkadmins_olddata]
definition = ""
iseval = 0

[splunkadmins_olddata_lookback]
definition = -7d

[splunkadmins_olddata_earliest]
definition = -2600d

[splunkadmins_olddata_latest]
definition = -60d

[splunkadmins_forwarders_nottalking_ds]
definition = ""
iseval = 0

#Ignore enterprise security related sendalert errors, they are often false alarms here, also filter the data a bit further...
[splunkadmins_sendmodalert_errors]
definition = `comment("We look for the sendalert commands to provide context around the errors where possible. Since notable/risks fail more often they are removed from this particular alert")` NOT action=notable NOT action=risk NOT (" - INFO]" OR "Results Link" OR "Alert Name:")
iseval = 0

[splunkadmins_bucketrolling_count]
definition = 20
iseval = 0

[splunkadmins_readop_expectingack]
definition = ""
iseval = 0

[splunkadmins_repfailures]
definition = ""
iseval = 0

[splunkadmins_lowdisk]
definition = ""
iseval = 0

[splunkadmins_lowdisk_perc]
definition = 10
iseval = 0

[splunkadmins_lowdisk_mb]
definition = 90000
iseval = 0

[splunkadmins_kvstore_terminated]
definition = ""
iseval = 0

[splunkadmins_fileintegritycheck]
definition = ""
iseval = 0

[splunkadmins_multiline_linemerge]
definition = ""
iseval = 0

[splunkadmins_warninifile]
definition = ""
iseval = 0

[splunkadmins_toomany_sametimestamp]
definition = ""
iseval = 0

[splunkadmins_colddata_percused]
definition = 80
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_colddata]
definition = `comment("Some internal indexes roll based on size by default such as introspection")` index!=_introspection index!=defaultdb
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_bucketfrozen]
definition = `comment("Some internal indexes roll based on size by default such as introspection")` bkt!="*_introspection*" bkt!="*defaultdb*"
iseval = 0

[splunkadmins_permissions]
definition = ""
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_warmdbcount]
definition = `comment("We probably don't care about the warm limits for the internal indexes...")` index!=_introspection index!=defaultdb
iseval = 0

[splunkadmins_warmdbcount_perc]
definition = 80
iseval = 0

[splunkadmins_clustermaster_failurecount]
definition = 1
iseval = 0

#My environment appears to have random SSL interconnectivity issues with mongo which are harmless/never cause an issue
[splunkadmins_mongodb_errors]
definition = NOT "SSL: error"
iseval = 0

[splunkadmins_mongodb_errors2]
definition = ""
iseval = 0

#Many of these applications contain macros which have embedded macros, attempting to expand them proved to be ... complicated so ignoring them!
[splunkadmins_scheduledsearches_without_index_macro]
definition = NOT ((eai:acl.app="splunk_app_windows_infrastructure" OR eai:acl.app="splunk_app_aws" OR eai:acl.app="splunk_app_for_nix" OR eai:acl.app="app-docker" OR eai:acl.app="nmon") AND (eai:acl.sharing=app OR eai:acl.sharing=global))
iseval = 0

[splunkadmins_privilegedowners]
definition = ""
iseval = 0

#Not sure why but this "Success" message appears in my instance...
[splunkadmins_searchfailures]
definition = message!="Success"
iseval = 0

[splunkadmins_captain_switchover]
definition = ""
iseval = 0

[splunkadmins_resource_starvation]
definition = ""
iseval = 0

[splunkadmins_s2sfilereceiver]
definition = ""
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were having a transfer of captain 
#Allowing a macro name to be passed in allows this function to be used for different search head clusters 
#This macro returns in the form of (_time>start _time<end) this allows entire indexer cluster restarts to be filtered out.
[splunkadmins_transfer_captain_times(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for the time when a search head captain transfer occurred")` index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` "Got Transfer captaincy" | eval message=coalesce(message,event_message) | stats min(_time) AS logTime by message, host | stats min(logTime) AS minTime, max(logTime) AS maxTime | eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$ | eval search=" _time>" . minTime . " _time<" .maxTime | fields search | format | rex mode=sed field=search "s/\"//g"
iseval = 0

[splunkadmins_replicationfactor]
definition = 2
iseval = 0

[whataccessdoihave]
definition = rest /services/authentication/users splunk_server=local\
| search `comment("REST query is limited to the current search head this is running on. If users have the dispatch REST to indexers capability then ise the 'What Access Do I Have' version' for more detail")`\
    [| rest /services/authentication/current-context/context splunk_server=local\
    | head 1 \
    | fields username \
    | rename username AS title] \
| table title roles | rename title as user | mvexpand roles\
| join type=left roles \
    [rest /services/authorization/roles splunk_server=local\
    | table title srchIndexesAllowed srchIndexesDefault imported_srchIndexesAllowed imported_srchIndexesDefault | rename title as roles]\
| fillnull value="" srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault\
| eval srchIndexesAllowed = srchIndexesAllowed . " " . imported_srchIndexesAllowed, srchIndexesDefault = srchIndexesDefault . " " . imported_srchIndexesDefault\
| makemv srchIndexesAllowed tokenizer=(\S+) | makemv srchIndexesDefault tokenizer=(\S+)\
| eval indexes= [ | eventcount summarize=false index=* index=_* | stats values(index) AS indexes | eval theindexes="\"" . mvjoin(indexes, " ") . "\"" | return $theindexes ]\
| makemv indexes\
| stats values(roles) AS roles, values(indexes) AS indexes, values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user

[splunkadmins_restmacro]
definition = splunk_server=local
iseval = 0

#Number of metric logs printed per minute, defaults to 30 seconds but can be changed by the user...
#if you log metrics every minute change this to 1
[splunkadmins_metrics_permin]
definition = 2
iseval = 0

#Substitute `<macro name>` within the audit.log files with the audit definition based on a lookup file
[splunkadmins_audit_logs_macro_sub]
definition = search `comment("Set all values to null() in case this macro is called again within the same search. Subsitute a macro used inside a search with the definition found in the lookup file")`\
| eval definition=null(), commas=null(), commas2=null(), argCount2=null(), argCount=null(), match=null()\
| rex field=search max_match=1 "\`(?!\")(?!')(?P<macro>[^\`]+)\`" \
| search `comment("You can have multiple macro definitions with either 0 or more arguments so we have to count them...")` \
| rex max_match=10 field=macro "([^\"]+\")|([^']+')\s*(?P<commas>,)" \
| rex max_match=10 field=macro "(?P<commas2>,)" \
| rex max_match=1 field=macro "(?P<match>[^\(]+\()" \
| search `comment("Two count methods are used as if we have macro(arg1) that has no commas, but macro(arg1,arg2) will work as expected...")` \
| eval argCount2=if(match(macro,"([^\"]+\")|([^']+')") AND isnull(commas),-1,if(isnotnull(commas2),mvcount(commas2),null())) \
| eval argCount=if(isnull(argCount2),0,argCount2+1) \
| eval argCount=if(argCount==0,if(isnotnull(match),1,0),argCount) \
| rex field=macro "(?P<macro>^[^\( ]+)" \
| eval macroName=if(argCount==0,macro,macro . "(" . argCount . ")") \
| lookup splunkadmins_macros title AS macroName, app AS app_name, splunk_server \
| eval app_name2="global"\
| lookup splunkadmins_macros title AS macroName, app AS app_name2, splunk_server OUTPUTNEW definition\
| lookup splunkadmins_macros title AS macroName, splunk_server OUTPUTNEW definition\
| eval macroReplace=if((argCount == 0),(("`" . macro) . "`"),(("`" . macro) . "\\(.*?\\)`")), search=if(isnotnull(definition),replace(search,macroReplace,mvindex(definition,0)),search)
iseval = 0

#Note this macro requires TA-webtools
[splunkadmins_remote_macros(3)]
args = url,user,pass
definition = | curl method=get uri="$url$/servicesNS/-/-/configs/conf-macros?count=-1&output_mode=json" user=$user$ pass=$pass$\
| spath input=curl_message path="entry{}.name" output=title\
| spath input=curl_message path="entry{}.acl.app" output=app\
| spath input=curl_message path="entry{}.content.definition" output=definition\
| spath input=curl_message path="entry{}.acl.sharing" output=sharing\
| fields - curl_* \
| fields title, app, definition, sharing \
| eval data=mvzip(mvzip(mvzip(title, 'app', "%%%%"),definition,"%%%%"),sharing,"%%%%")\
| fields data \
| mvexpand data \
| makemv data delim="%%%%" \
| eval title=mvindex(data,0),app=mvindex(data,1), definition=mvindex(data,2), sharing=mvindex(data,3)\
| search sharing!=user\
| fields - data
iseval = 0

#Not currently in use by searches but attempts to pull the roles from a remote Splunk server
[splunkadmins_remote_roles(3)]
args = url,user,pass
definition = | curl method=get uri="$url$/services/authentication/users?output_mode=json&count=0&f=roles" user="$user$" pass="$pass$"\
| rex field=curl_message max_match=10000 "{\"name\":\"(?P<user>[^\"]+)\".*?\"roles\":\[(?P<roles>[^\]]+)" \
| fields - curl_* \
| eval data=mvzip(user,roles,"%%%%") \
| mvexpand data \
| table data \
| makemv data delim="%%%%" \
| eval user=mvindex(data,0), roles=mvindex(data,1)\
| fields - data\
| eval roles=replace(roles,"\"","")\
| makemv roles delim=","
iseval = 0

#Macro to determine search head cluster name, potentially using a case statement or similar
[search_head_cluster]
definition = "default"
iseval = 0

#Macro to determine which indexer cluster name, potentially using a case statement or similar
[indexer_cluster_name(1)]
args = indexer
definition = "default"
iseval = 0

[search_type_from_sid(1)]
args = search_id
definition = eval from=null(), username=null(), searchname2=null(), searchname=null()\
| rex field=$search_id$ "(_?subsearch)*_?(?P<from>[^_]+)((_(?P<base64username>[^_]+))|(__(?P<username>[^_]+)))((__(?P<app>[^_]+)__(?P<searchname2>[^_]+))|(_(?P<base64appname>[^_]+)__(?P<searchname>[^_]+)))"\
| rex field=$search_id$ "^_?(?P<from>SummaryDirector)"\
| search `comment("Pattern appears to vary but remote_<hostname>_ is consistent along with the optional _subsearch, the _from can be <username>__ownername__appname__RMD for dashboards as one pattern, it can also be unixepoch (ad-hoc), or scheduler__username__appname (scheduled search), or  username__owner__(something)__dashboardview, among others. RMD values can be translated via audit.log, scheduler.log or remote_searches.log (if savedsearch_name is there)!")`\
| fillnull from value="adhoc"\
| eval searchname=coalesce(searchname,searchname2)\
| eval type=case(from=="scheduler","scheduled",from=="SummaryDirector","acceleration",isnotnull(searchname),"dashboard",1=1,"ad-hoc")
iseval = 0

[base64decode(1)]
args = afield
definition = eval $afield$=null() | search `comment("As per https://docs.splunk.com/Documentation/Splunk/latest/Report/Createandeditreports usernames/apps can be base64 encrypted, remove the eval when ready to use this...decrypt (splunkbase) can be used to decrypt with (remove the backslashes): eval $afield$=$afield$ . \"===\" | decrypt f=$afield$ atob emit('$afield$')")`
iseval = 0

[dashboard_depends_filter1]
definition = ""
iseval = 0

[dashboard_depends_filter2]
definition = search `comment("potentially a where clause to only filter when a certain number of tokens exist...")`
iseval = 0

[dashboard_depends_filter3]
definition = search `comment("potentially a where clause to only filter when a certain number of tokens were matched or similar...")`
iseval = 0

[splunkadmins_wineventlog_index]
definition = wineventlog
iseval = 0

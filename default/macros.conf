##############
#
# Customise these macros to ensure the SplunkAdmins / Alerts for Splunk Admins
# application works as expected
#
##############
[indexerhosts]
definition = host=*
iseval = 0

[heavyforwarderhosts]
definition = host=*
iseval = 0

[searchheadhosts]
definition = host=*
iseval = 0

#Designed for searches where returning data from other search heads
#would not provide valid results...
[localsearchheadhosts]
definition = host=*
iseval = 0

[splunkenterprisehosts]
definition = host=*
iseval = 0

[deploymentserverhosts]
definition = host=*
iseval = 0

[licensemasterhost]
definition = host=*
iseval = 0

[searchheadsplunkservers]
definition = splunk_server=*
iseval = 0

[splunkindexerhostsvalue]
definition = splunk_server=*
iseval = 0

[splunkadmins_splunkd_source]
definition = source=*splunkd.log
iseval = 0

[splunkadmins_splunkuf_source]
definition = source=*splunkd.log
iseval = 0

[splunkadmins_mongo_source]
definition = source=*mongod.log
iseval = 0

[splunkadmins_license_usage_source]
definition = source=*license_usage.log
iseval = 0

[splunkadmins_clustermaster_oshost]
definition = host=changeme
iseval = 0

#Only used in a few searches, customise this if you have the cluster master as a search
#peer, if not you may wish to leave this to local and run the ClusterMasterLevel searches on
#the cluster master server...
[splunkadmins_clustermaster_host]
definition = splunk_server=local

##############
#
# Utility functions
#
##############
[comment(1)]
args = text
definition = ""
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of ((host=X _time>start _time<end) OR (host=Y _time>start _time<end)
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_list(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for when this particular Splunk server was shutdown, plus any contingency time as requested")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime by host\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search="host=" . host . " _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of keywords (hostnames without the host=) / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of ((X _time>start _time<end) OR (Y _time>start _time<end)
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_keyword(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for when this particular Splunk server was shutdown, plus any contingency time as requested")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime by host\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search=host . " _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were restarting, for example if search heads were restarting we probably don't care about delayed scheduled searches at this point in time
#Allowing a macro name to be passed in allows this function to be used for search heads or indexers or anything else
#Furthermore allowing contingency time allows some time for the server to recover from the restart if required...
#This macro returns in the form of (_time>start _time<end) this allows entire indexer cluster restarts to be filtered out.
#
#The query is checking various shutdown messages as different types of server have different messages signalling the start of the shutdown process
#simplifying this in the past has resulted in missing at least 1 type of shutdown or the start of the shutdown process...
[splunkadmins_shutdown_time(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for the time when any indexer was shutdown")`\
index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` ("ShutdownHandler" "Shutting down") OR "Shutdown complete in" OR "Received shutdown signal." OR "master has instructed peer to restart" OR "Performing early shutdown tasks"\
| stats min(_time) AS logTime by message, host\
| stats min(logTime) AS minTime, max(logTime) AS maxTime\
| eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$\
| eval search=" _time>" . minTime . " _time<" .maxTime\
| fields search\
| format\
| rex mode=sed field=search "s/\"//g"
iseval = 0


##############
#
# Per-alert macros that can be customised for
# filtering unncessary data from alerts where required
#
##############

[splunkadmins_acc_datamodels]
definition = ""
iseval = 0

[splunkadmins_runscript]
definition = ""
iseval = 0

[splunkadmins_timeskew]
definition = ""
iseval = 0

[splunkadmins_changedprops]
definition = ""
iseval = 0

[splunkadmins_changedprops_count]
definition = 3
iseval = 0

[splunkadmins_btoolvalidation_ds]
definition = `comment("Splunk for stream doesn't include a config file which causes errors, however it appears to work without it...")` NOT "/opt/splunk/etc/deployment-apps/Splunk_TA_stream*"
iseval = 0

[splunkadmins_bandwidth]
definition = ""
iseval = 0

[splunkadmins_toosmall_checkcrc]
definition = ""
iseval = 0

[splunkadmins_forwarderdown]
definition = ""
iseval = 0

[splunkadmins_heavylogging]
definition = ""
iseval = 0

[splunkadmins_exceeding_filedescriptor]
definition = ""
iseval = 0

[splunkadmins_sending_data]
definition = ""
iseval = 0

[splunkadmins_sending_data_nonhf_count]
definition = 0
iseval = 0

[splunkadmins_sending_data_hf_count]
definition = 5
iseval = 0

[splunkadmins_unusual_duplication]
definition = ""
iseval = 0

[splunkadmins_unusual_duplication_count]
definition = 10
iseval = 0

[splunkadmins_crcsalt_initcrc]
definition = ""
iseval = 0

[splunkadmins_uf_timeshifting]
definition = ""
iseval = 0

[splunkadmins_future_dated]
definition = ""
iseval = 0

[splunkadmins_failuretoparse_timestamp]
definition = ""
iseval = 0

[splunkadmins_failuretoparse_timestamp_count]
definition = 0
iseval = 0

[splunkadmins_failuretoparse_timestamp_binperiod]
definition = 1m
iseval = 0

[splunkadmins_failuretoparse_timestamp2]
definition = ""
iseval = 0

[splunkadmins_indexconfig_warn]
definition = ""
iseval = 0

[splunkadmins_indexerqueue_fillperc_nonindexqueue]
definition = 50
iseval = 0

[splunkadmins_indexerqueue_fillperc_indexqueue]
definition = 90
iseval = 0

[splunkadmins_indexer_replication_queue_count]
definition = 15
iseval = 0

[splunkadmins_uneven_indexed_perc]
definition = 25
iseval = 0

[splunkadmins_weekly_brokenevents]
definition = ""
iseval = 0

[splunkadmins_weekly_truncated]
definition = ""
iseval = 0

[splunkadmins_weekly_truncated_count]
definition = 0
iseval = 0


[splunkadmins_valid_timestamp_invalidparsed]
definition = ""
iseval = 0

[splunkadmins_longrunning_searches]
definition = `comment("Exclude various standard/expected searches")` savedsearch_name!="Generate Meta Woot! every 15 mins" savedsearch_name!="Generate NMON*"
iseval = 0

[splunkadmins_realtime_scheduledsearches]
definition = ""
iseval = 0

[splunkadmins_scheduledsearches_cannot_run]
definition = ""
iseval = 0

[splunkadmins_scheduledsearches_without_earliestlatest]
definition = NOT (eai:acl.app=splunk_app_aws author=nobody)
iseval = 0

#Ignore Splunk apps which will trigger this
[splunkadmins_scheduledsearches_without_index]
definition = eai:acl.app!="splunk_archiver" eai:acl.app!="splunk_app_windows_infrastructure" eai:acl.app!="splunk_app_aws" eai:acl.app!="nmon"
iseval = 0

[splunkadmins_scriptfailures]
definition = ""
iseval = 0

[splunkadmins_users_violating_searchquota]
definition = ""
iseval = 0

[splunkadmins_users_exceeding_diskquota]
definition = ""
iseval = 0

[splunkadmins_execprocessor]
definition = ""
iseval = 0

[splunkadmins_timeformat_change]
definition = ""
iseval = 0

[splunkadmins_loginattempts]
definition = ""
iseval = 0

[splunkadmins_insufficient_permissions]
definition = ""
iseval = 0

[splunkadmins_tcpoutput_paused]
definition = ""
iseval = 0

[splunkadmins_streamerrors]
definition = ""
iseval = 0

[splunkadmins_unable_distribute_to_peer]
definition = ""
iseval = 0

[splunkadmins_dashboards_allindexes]
definition = NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app)
iseval = 0

[splunkadmins_scheduled_incorrectsharing]
definition = ""
iseval = 0

[splunkadmins_realtime_dashboard]
definition = NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app)
iseval = 0

[splunkadmins_olddata]
definition = ""
iseval = 0

[splunkadmins_olddata_lookback]
definition = -7d

[splunkadmins_olddata_earliest]
definition = -2600d

[splunkadmins_olddata_latest]
definition = -60d

[splunkadmins_forwarders_nottalking_ds]
definition = ""
iseval = 0

#Ignore enterprise security related sendalert errors, they are often false alarms here, also filter the data a bit further...
[splunkadmins_sendmodalert_errors]
definition = `comment("We look for the sendalert commands to provide context around the errors where possible. Since notable/risks fail more often they are removed from this particular alert")` NOT action=notable NOT action=risk NOT (" - INFO]" OR "Results Link" OR "Alert Name:")
iseval = 0

[splunkadmins_bucketrolling_count]
definition = 20
iseval = 0

[splunkadmins_readop_expectingack]
definition = ""
iseval = 0

[splunkadmins_repfailures]
definition = ""
iseval = 0

[splunkadmins_lowdisk]
definition = ""
iseval = 0

[splunkadmins_lowdisk_perc]
definition = 10
iseval = 0

[splunkadmins_lowdisk_mb]
definition = 90000
iseval = 0

[splunkadmins_kvstore_terminated]
definition = ""
iseval = 0

[splunkadmins_fileintegritycheck]
definition = ""
iseval = 0

[splunkadmins_multiline_linemerge]
definition = ""
iseval = 0

[splunkadmins_warninifile]
definition = ""
iseval = 0

[splunkadmins_toomany_sametimestamp]
definition = ""
iseval = 0

[splunkadmins_colddata_percused]
definition = 80
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_colddata]
definition = `comment("Some internal indexes roll based on size by default such as introspection")` index!=_introspection index!=defaultdb
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_bucketfrozen]
definition = `comment("Some internal indexes roll based on size by default such as introspection")` bkt!="*_introspection*" bkt!="*defaultdb*"
iseval = 0

[splunkadmins_permissions]
definition = ""
iseval = 0

#Ignore internal indexes introspection & main
[splunkadmins_warmdbcount]
definition = `comment("We probably don't care about the warm limits for the internal indexes...")` index!=_introspection index!=defaultdb
iseval = 0

[splunkadmins_warmdbcount_perc]
definition = 80
iseval = 0

[splunkadmins_clustermaster_failurecount]
definition = 1
iseval = 0

#My environment appears to have random SSL interconnectivity issues with mongo which are harmless/never cause an issue
[splunkadmins_mongodb_errors]
definition = NOT "SSL: error"
iseval = 0

[splunkadmins_mongodb_errors2]
definition = ""
iseval = 0

#Many of these applications contain macros which have embedded macros, attempting to expand them proved to be ... complicated so ignoring them!
[splunkadmins_scheduledsearches_without_index_macro]
definition = NOT ((eai:acl.app="splunk_app_windows_infrastructure" OR eai:acl.app="splunk_app_aws" OR eai:acl.app="splunk_app_for_nix" OR eai:acl.app="app-docker" OR eai:acl.app="nmon") AND (eai:acl.sharing=app OR eai:acl.sharing=global))
iseval = 0

[splunkadmins_privilegedowners]
definition = ""
iseval = 0

#Not sure why but this "Success" message appears in my instance...
[splunkadmins_searchfailures]
definition = message!="Success"
iseval = 0

[splunkadmins_captain_switchover]
definition = ""
iseval = 0

[splunkadmins_resource_starvation]
definition = ""
iseval = 0

[splunkadmins_s2sfilereceiver]
definition = ""
iseval = 0

#
#Dynamically generate a Splunk SPL statement to filter out a list of hosts / time periods where the particular hosts
#were having a transfer of captain 
#Allowing a macro name to be passed in allows this function to be used for different search head clusters 
#This macro returns in the form of (_time>start _time<end) this allows entire indexer cluster restarts to be filtered out.
[splunkadmins_transfer_captain_times(3)]
args = macroName, minTimeContingency, maxTimeContingency
definition = search `comment("Send an exclusion list in terms of a search result for the time when a search head captain transfer occurred")` index=_internal (`$macroName$`) sourcetype=splunkd `splunkadmins_splunkd_source` "Got Transfer captaincy" | stats min(_time) AS logTime by message, host | stats min(logTime) AS minTime, max(logTime) AS maxTime | eval minTime=minTime - $minTimeContingency$, maxTime=maxTime + $maxTimeContingency$ | eval search=" _time>" . minTime . " _time<" .maxTime | fields search | format | rex mode=sed field=search "s/\"//g"
iseval = 0

[splunkadmins_replicationfactor]
definition = 2
iseval = 0

[whataccessdoihave]
definition = rest /services/authentication/users splunk_server=local\
| search `comment("REST query is limited to the current search head this is running on. If users have the dispatch REST to indexers capability then ise the 'What Access Do I Have' version' for more detail")`\
    [| rest /services/authentication/current-context/context splunk_server=local\
    | head 1 \
    | fields username \
    | rename username AS title] \
| table title roles | rename title as user | mvexpand roles\
| join type=left roles \
    [rest /services/authorization/roles splunk_server=local\
    | table title srchIndexesAllowed srchIndexesDefault imported_srchIndexesAllowed imported_srchIndexesDefault | rename title as roles]\
| fillnull value="" srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault\
| eval srchIndexesAllowed = srchIndexesAllowed . " " . imported_srchIndexesAllowed, srchIndexesDefault = srchIndexesDefault . " " . imported_srchIndexesDefault\
| makemv srchIndexesAllowed tokenizer=(\S+) | makemv srchIndexesDefault tokenizer=(\S+)\
| eval indexes= [ | eventcount summarize=false index=* index=_* | stats values(index) AS indexes | eval theindexes="\"" . mvjoin(indexes, " ") . "\"" | return $theindexes ]\
| makemv indexes\
| stats values(roles) AS roles, values(indexes) AS indexes, values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user

[splunkadmins_restmacro]
definition = splunk_server=local
iseval = 0

#Number of metric logs printed per minute, defaults to 30 seconds but can be changed by the user...
#if you log metrics every minute change this to 1
[splunkadmins_metrics_permin]
definition = 2
iseval = 0

